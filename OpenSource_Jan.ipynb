{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89c60475",
   "metadata": {},
   "source": [
    "# Data Load, Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffb4cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData():    \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    filepath='combined_emissions_sources2.csv'\n",
    "        # Step 1: Load the CSV file into a pandas DataFrame\n",
    "    try:\n",
    "        print(f\"Loading data from '{filepath}'...\")\n",
    "        df = pd.read_csv(filepath)\n",
    "        print(\"Data loaded successfully.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{filepath}' was not found.\")\n",
    "        print(\"Please make sure you have already run the first script to generate this file,\")\n",
    "        print(\"and that it is in the same directory as this script.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the file: {e}\")\n",
    "    # Step 2: Display the size of the DataFrame (rows, columns)\n",
    "    rows, cols = df.shape\n",
    "    print(\"\\n--- DataFrame Size ---\")\n",
    "    print(f\"The DataFrame has {rows} rows and {cols} columns.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa41921e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=loadData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e1b91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['start_time'])\n",
    "# --- 2. Identify and Drop Columns with Nulls > 100,000 ---\n",
    "threshold = 100000\n",
    "null_counts = df.isnull().sum()\n",
    "columns_to_drop = null_counts[null_counts > threshold].index.tolist()\n",
    "\n",
    "if not columns_to_drop:\n",
    "    print(\"No columns have more than 100,000 null values. No columns were dropped.\")\n",
    "else:\n",
    "\n",
    "    print(f\"--- Columns to be Dropped (>{threshold} nulls) ---\")\n",
    "    print(columns_to_drop)\n",
    "    print(\"\\n--- Columns Dropped ---\")\n",
    "    print(f\"Old shape of DataFrame: {df.shape}\")\n",
    "    df.drop(columns=columns_to_drop, axis=1, inplace=True)\n",
    "    print(f\"New shape of the DataFrame: {df.shape}\")\n",
    "\n",
    "columns_to_drop=['modified_date','source_id','iso3_country']\n",
    "df.drop(columns=columns_to_drop, axis=1, inplace=True)\n",
    "print(df.columns)\n",
    "\n",
    "\n",
    "\n",
    "# --- 2. Find and Display Null Value Counts ---\n",
    "null_counts = df.isnull().sum()\n",
    "df_imputed = df.copy()\n",
    "sector_null_count = df_imputed['sector'].isnull().sum()\n",
    "if sector_null_count > 0:\n",
    "    df_imputed.dropna(subset=['sector'], inplace=True)\n",
    "\n",
    "print(\"--- Imputing Missing Values ---\")\n",
    "# Loop through each column to apply the correct imputation strategy\n",
    "for col in df_imputed.columns:\n",
    "    if df_imputed[col].isnull().any():\n",
    "        # STRATEGY 1: For non-numeric (object/categorical) columns\n",
    "        if df_imputed[col].dtype == 'object':\n",
    "            mode_value = df_imputed[col].mode()[0]\n",
    "            df_imputed[col].fillna(mode_value, inplace=True)\n",
    "            print(f\"Imputed non-numeric column '{col}' with mode: '{mode_value}'\")\n",
    "            \n",
    "        # STRATEGY 2: For numeric (float/int) columns\n",
    "        else:\n",
    "            mean_value = df_imputed[col].mean()\n",
    "            df_imputed[col].fillna(mean_value, inplace=True)\n",
    "            print(f\"Imputed numeric column '{col}' with mean: {mean_value:.2f}\")\n",
    "\n",
    "print(\"\\nImputation complete.\")\n",
    "print(\"\\n--- Null values count after imputation ---\")\n",
    "display(df_imputed.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dc4f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Display unique values from the 'source_name' column\n",
    "print(\"\\n--- Unique Values in 'sectors' ---\")\n",
    "if 'sector' in df_imputed.columns:\n",
    "    unique_sectors = df_imputed['sector'].unique()\n",
    "    print(f\"Found {len(unique_sectors)} unique source names. Displaying a sample:\")\n",
    "    # Display the first 15 unique names, or all if less than 15\n",
    "    display_limit = min(15, len(unique_sectors))\n",
    "    for i, name in enumerate(unique_sectors[:display_limit]):\n",
    "        print(f\"- {name}\")\n",
    "    if len(unique_sectors) > display_limit:\n",
    "        print(f\"... and {len(unique_sectors) - display_limit} more.\")\n",
    "else:\n",
    "    print(\"The column 'sector' was not found in the DataFrame.\")\n",
    "\n",
    "all_null_rows_count = df_imputed.isnull().all(axis=1).sum()\n",
    "print(f\"Contains {all_null_rows_count} rows where all values are null.\")\n",
    "print(f\"The shape of current cleaned df: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08e0fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def impute_categorical_with_percentiles(df, columns):\n",
    "    label_to_quantile = {\n",
    "        'very high': 0.95,\n",
    "        'high': 0.90,\n",
    "        'medium': 0.65,\n",
    "        'low': 0.45,\n",
    "        'very low': 0.35,\n",
    "    }\n",
    "    \n",
    "    for col in columns:\n",
    "        if col not in df.columns:\n",
    "            print(f\"Warning: column '{col}' not found. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        s = df[col].astype(str)\n",
    "        numeric_series = pd.to_numeric(df[col], errors='coerce')\n",
    "        numeric_only = numeric_series.dropna()\n",
    "\n",
    "        if numeric_only.empty:\n",
    "            print(f\"Warning: column '{col}' has no numeric data to compute percentiles. Skipping.\")\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            continue\n",
    "\n",
    "        qvals = numeric_only.quantile([\n",
    "            label_to_quantile['very high'],\n",
    "            label_to_quantile['high'],\n",
    "            label_to_quantile['medium'],\n",
    "            label_to_quantile['low'],\n",
    "            label_to_quantile['very low']\n",
    "        ])\n",
    "        # Map quantile index back to labels\n",
    "        pmap = {\n",
    "            'very high': qvals.loc[label_to_quantile['very high']],\n",
    "            'high': qvals.loc[label_to_quantile['high']],\n",
    "            'medium': qvals.loc[label_to_quantile['medium']],\n",
    "            'low': qvals.loc[label_to_quantile['low']],\n",
    "            'very low': qvals.loc[label_to_quantile['very low']],\n",
    "        }\n",
    "\n",
    "        s_clean = s.str.strip().str.lower().replace(pmap)\n",
    "        df[col] = pd.to_numeric(s_clean, errors='coerce')\n",
    "\n",
    "        # Report\n",
    "        print(f\"Processed '{col}': converted to float. Percentiles used: \"\n",
    "              f\"very high={pmap['very high']:.6g}, high={pmap['high']:.6g}, \"\n",
    "              f\"medium={pmap['medium']:.6g}, low={pmap['low']:.6g}, very low={pmap['very low']:.6g}\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80c438a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_fix = ['capacity','capacity_factor','activity','emissions_factor','emissions_quantity']\n",
    "\n",
    "impute_categorical_with_percentiles(df_imputed, cols_to_fix)\n",
    "for c in cols_to_fix:\n",
    "    if c in df_imputed.columns:\n",
    "        print(c, df_imputed[c].dtype, \"-> sample:\", df_imputed[c].dropna().head().tolist())\n",
    "\n",
    "print(\"\\n--- Time Clean Ups ---\\n\")\n",
    "import pandas as pd\n",
    "df_imputed['start_time'] = pd.to_datetime(df_imputed['start_time'], infer_datetime_format=True)\n",
    "df_imputed['end_time']   = pd.to_datetime(df_imputed['end_time'], infer_datetime_format=True)\n",
    "\n",
    "# Optional: verify conversion\n",
    "print(df_imputed[['start_time', 'end_time']].dtypes)\n",
    "print(df_imputed[['start_time', 'end_time']].head())\n",
    "\n",
    "# Add time features\n",
    "df_imputed['year'] = df_imputed['start_time'].dt.year\n",
    "df_imputed['month'] = df_imputed['start_time'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f1305a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "null_counts = df_imputed.isnull().sum()\n",
    "print(null_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30da81e",
   "metadata": {},
   "source": [
    "# Data Cleaning Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1a0fb1",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 1. Data Consolidation & Cleaning\n",
    "- **Consolidation**\n",
    "  - Collected emissions data from multiple sector folders.  \n",
    "  - Merged into a single unified dataset.  \n",
    "- **Cleaning**\n",
    "  - Parsed datetime fields: `start_time`, `end_time`.  \n",
    "  - Imputed missing values:  \n",
    "    - **Numeric values** → replaced with column means.  \n",
    "    - **Categorical strings** (e.g., *“very high”, “low”*) → substituted with percentile-based numeric values.  \n",
    "  - Converted mixed-type numeric columns → coerced to `float`.  \n",
    "  - Dropped columns with excessive nulls.  \n",
    "  - Removed rows where `sector` was null.  \n",
    "\n",
    "- **Cleaned data size**\n",
    "  - `(4527140, 9)`\n",
    "  - 2021-2025 (May)\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Feature Engineering\n",
    "- **Time Features**\n",
    "  - Extracted calendar components: year, month, quarter, month start/end.  \n",
    "  - Added **cyclical encodings** for month (sine/cosine representation).  \n",
    "- **Autoregressive Lags**\n",
    "  - Created lag features for 1, 2, 3, 6, and 12 months.  \n",
    "- **Aggregation**\n",
    "  - Standardized the dataset into a consistent **monthly time series**.  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effc36fd",
   "metadata": {},
   "source": [
    "# EDA Timeline, Sector Data Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ba0c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "plt.style.use(\"seaborn-v0_8\")  # updated style name\n",
    "sns.set_palette(\"viridis\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def sectorWise(df_sector):\n",
    "\n",
    "    # =========================================================\n",
    "    # 1) Build monthly series from df_imputed (NO 'ds','y' assumed)\n",
    "    # =========================================================\n",
    "    # Aggregate total emissions by month\n",
    "    df_monthly = (\n",
    "        df_sector.groupby(['year', 'month'])['emissions_quantity']\n",
    "        .sum()\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Create month-start datetime column 'ds'\n",
    "    df_monthly['ds'] = pd.to_datetime(\n",
    "        df_monthly['year'].astype(str) + '-' + df_monthly['month'].astype(str) + '-01'\n",
    "    ).dt.to_period('M').dt.to_timestamp(how='start')  # enforce month-start timestamps\n",
    "\n",
    "\n",
    "    # Prophet expects columns: ds (date), y (target)\n",
    "    df_monthly = df_monthly.rename(columns={'emissions_quantity': 'y'})\n",
    "    df_monthly = df_monthly[['ds', 'y']].sort_values('ds').reset_index(drop=True)\n",
    "\n",
    "    # =========================================================\n",
    "    # 2) Train/Test split (Train: 2021-01 → 2023-12, Test: 2024-01 → 2025-05)\n",
    "    # =========================================================\n",
    "    train = df_monthly[(df_monthly['ds'] >= '2021-01-01') & (df_monthly['ds'] < '2024-01-01')]\n",
    "    test  = df_monthly[(df_monthly['ds'] >= '2024-01-01') & (df_monthly['ds'] <= '2025-05-01')]\n",
    "\n",
    "    print(f\"Train size: {len(train)} | range: {train['ds'].min().date()} → {train['ds'].max().date()}\")\n",
    "    print(f\"Test  size: {len(test)} | range: {test['ds'].min().date()} → {test['ds'].max().date()}\")\n",
    "\n",
    "    # =========================================================\n",
    "    # 3) Fit Prophet on training data\n",
    "    # =========================================================\n",
    "    m = Prophet(yearly_seasonality=True, weekly_seasonality=False, daily_seasonality=False)\n",
    "    m.fit(train)\n",
    "\n",
    "    # =========================================================\n",
    "    # 4) Create futures\n",
    "    #    - For evaluation/plots, we want a full forecast that includes history.\n",
    "    #    - For metrics, we align exactly to test months to avoid any NaN/mismatch.\n",
    "    # =========================================================\n",
    "    # Full forecast through the end of test period for nice Prophet plots\n",
    "    # (periods = number of months from end of train to 2025-05 inclusive)\n",
    "    last_needed = pd.Timestamp('2025-05-01')\n",
    "    months_needed = (last_needed.to_period('M') - train['ds'].max().to_period('M')).n + 1\n",
    "    future_full = m.make_future_dataframe(periods=months_needed, freq='MS')\n",
    "    forecast_full = m.predict(future_full)\n",
    "\n",
    "    # For strict evaluation: forecast exactly on test dates\n",
    "    future_test = pd.DataFrame({'ds': test['ds']})\n",
    "    forecast_test_only = m.predict(future_test)[['ds', 'yhat']]\n",
    "\n",
    "    # Alignment check (should be zero missing)\n",
    "    missing_in_forecast = sorted(set(test['ds']) - set(forecast_test_only['ds']))\n",
    "    if missing_in_forecast:\n",
    "        print(\"Warning: these test dates are missing predictions:\", missing_in_forecast)\n",
    "\n",
    "    # =========================================================\n",
    "    # 5) Evaluation (MAE, RMSE, MAPE, sMAPE) on 2024-01 → 2025-05\n",
    "    # =========================================================\n",
    "    eval_df = test.merge(forecast_test_only, on='ds', how='left').copy()\n",
    "\n",
    "    # Safety check for any NaNs (should not happen; if it does, we surface it)\n",
    "    if eval_df['yhat'].isna().any() or eval_df['y'].isna().any():\n",
    "        n_nan_pred = eval_df['yhat'].isna().sum()\n",
    "        n_nan_true = eval_df['y'].isna().sum()\n",
    "        raise ValueError(f\"Found NaNs after alignment -> yhat NaNs: {n_nan_pred}, y NaNs: {n_nan_true}. \"\n",
    "                        \"Check the monthly continuity or date alignment.\")\n",
    "\n",
    "    y_true = eval_df['y'].to_numpy()\n",
    "    y_pred = eval_df['yhat'].to_numpy()\n",
    "\n",
    "    mae  = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "    # Robust MAPE: ignore zero-true months in the percentage calc\n",
    "    nonzero_mask = y_true != 0\n",
    "    if nonzero_mask.sum() == 0:\n",
    "        mape = np.nan\n",
    "    else:\n",
    "        mape = np.mean(np.abs((y_true[nonzero_mask] - y_pred[nonzero_mask]) / y_true[nonzero_mask])) * 100\n",
    "\n",
    "    # sMAPE handles zeros better\n",
    "    smape = 100 * np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred)))\n",
    "\n",
    "    print(\"\\nEvaluation (Test: 2024-01 → 2025-05)\")\n",
    "    print(f\"MAE   : {mae:.3f}\")\n",
    "    print(f\"RMSE  : {rmse:.3f}\")\n",
    "    print(f\"MAPE  : {mape:.3f}% (computed on non-zero actuals only)\")\n",
    "    print(f\"sMAPE : {smape:.3f}%\")\n",
    "\n",
    "    # =========================================================\n",
    "    # 6) Plots — keep EVERYTHING\n",
    "    # =========================================================\n",
    "\n",
    "    # A) Forecast vs Actual (Train/Test + Forecast-on-test)\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.plot(train['ds'], train['y'], label=\"Train\", linewidth=2)\n",
    "    plt.plot(test['ds'], test['y'], label=\"Test (Actual)\", linewidth=2)\n",
    "    plt.plot(eval_df['ds'], eval_df['yhat'], '--', label=\"Forecast on Test\", linewidth=2)\n",
    "    plt.axvline(pd.Timestamp('2024-01-01'), color='gray', linestyle='--', label=\"Train/Test Split\")\n",
    "    plt.title(f\"Prophet Forecast vs Actual (Train: 2021–2023, Test: 2024–2025-05),{df_sector['sector'].unique()}\")\n",
    "    plt.xlabel(\"Date\"); plt.ylabel(\"Total Emissions\"); plt.legend(); plt.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "    return mape\n",
    "    # # B) Prophet Forecast (full range) — nice overview figure\n",
    "    # fig_forecast = m.plot(forecast_full)\n",
    "    # plt.title(\"Prophet Forecast (Full Range Through 2025-05)\")\n",
    "    # plt.axvline(pd.Timestamp('2024-01-01'), color='gray', linestyle='--', label=\"Forecast Start\")\n",
    "    # plt.legend(); plt.grid(alpha=0.3)\n",
    "    # plt.show()\n",
    "    # # Plot forecast with uncertainty intervals + actual test points\n",
    "    # plt.figure(figsize=(12, 6))\n",
    "    # plt.plot(train[\"ds\"], train[\"y\"], label=\"Train\", color=\"blue\")\n",
    "    # plt.plot(test[\"ds\"], test[\"y\"], label=\"Test (actual)\", color=\"black\", linestyle=\"dashed\")\n",
    "    # plt.plot(forecast_full[\"ds\"], forecast_full[\"yhat\"], label=\"Forecast\", color=\"red\")\n",
    "    # plt.fill_between(forecast_full[\"ds\"], forecast_full[\"yhat_lower\"], forecast_full[\"yhat_upper\"], color=\"pink\", alpha=0.3)\n",
    "\n",
    "    # # Add test points as scatter dots\n",
    "    # plt.scatter(test[\"ds\"], test[\"y\"], color=\"black\", marker=\"o\", s=40, label=\"Test points\")\n",
    "\n",
    "    # plt.title(\"Forecast vs Actuals (with Test Points)\")\n",
    "    # plt.xlabel(\"Date\")\n",
    "    # plt.ylabel(\"y\")\n",
    "    # plt.legend()\n",
    "    # plt.show()\n",
    "\n",
    "    # # C) Prophet Components (Trend + Seasonality) — make easier to read\n",
    "    # fig_comp = m.plot_components(forecast_full)\n",
    "    # fig_comp.set_size_inches(12, 8)\n",
    "    # for ax in fig_comp.axes:\n",
    "    #     ax.grid(alpha=0.3)\n",
    "    #     ax.set_ylabel(\"Effect\")\n",
    "    # plt.suptitle(\"Trend & Yearly Seasonality Effects\", fontsize=16)\n",
    "    # plt.show()\n",
    "\n",
    "    # # D) Residuals over time (Test period)\n",
    "    # residuals = y_true - y_pred\n",
    "    # plt.figure(figsize=(10,5))\n",
    "    # plt.plot(eval_df['ds'], residuals, marker='o')\n",
    "    # plt.axhline(0, color='red', linestyle='--')\n",
    "    # plt.title(\"Residuals Over Time (Test: 2024–2025-05)\")\n",
    "    # plt.xlabel(\"Date\"); plt.ylabel(\"Residual (Actual - Predicted)\"); plt.grid(alpha=0.3)\n",
    "    # plt.show()\n",
    "\n",
    "    # # E) Correlation Heatmap for numeric columns in df_imputed (as requested earlier)\n",
    "    # num_cols = df_imputed.select_dtypes(include=[np.number]).columns\n",
    "    # if len(num_cols) > 1:\n",
    "    #     plt.figure(figsize=(10,7))\n",
    "    #     corr = df_imputed[num_cols].corr()\n",
    "    #     sns.heatmap(corr, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "    #     plt.title(\"Correlation Heatmap (Numerical Features)\")\n",
    "    #     plt.show()\n",
    "\n",
    "    # # Actual vs Predicted on historical data\n",
    "    # plt.figure(figsize=(10,6))\n",
    "    # plt.plot(eval_df['ds'], eval_df['y'], label=\"Actual\", marker='o')\n",
    "    # plt.plot(eval_df['ds'], eval_df['yhat'], label=\"Predicted\", marker='x')\n",
    "    # plt.title(\"Prophet: Actual vs Predicted \")\n",
    "    # plt.xlabel(\"Date\")\n",
    "    # plt.ylabel(\"Total Emissions\")\n",
    "    # plt.legend()\n",
    "    # plt.grid(alpha=0.3)\n",
    "    # plt.show()\n",
    "\n",
    "def timeline(df):\n",
    "    monthly = df.groupby(['year','month'])['emissions_quantity'].sum().reset_index()\n",
    "    monthly['date'] = pd.to_datetime(monthly[['year','month']].assign(day=1))\n",
    "\n",
    "    plt.figure(figsize=(14,6))\n",
    "    sns.lineplot(data=monthly, x=\"date\", y=\"emissions_quantity\")\n",
    "    plt.title(\"Emissions Over Time\")\n",
    "    plt.ylabel(\"Total Monthly Emissions\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e099b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sector-wise over time (Top 5)\n",
    "top_sectors1 = ['agriculture','forestry-and-land-use','power','fossil-fuel-operations']\n",
    "top_sectors2 = ['buildings','manufacturing','transportation']\n",
    "top_sectors3=['mineral-extraction','waste']\n",
    "\n",
    "def timelineCombo(top_sectors):\n",
    "    sector_trend = (df_imputed[df_imputed['sector'].isin(top_sectors)]\n",
    "                    .groupby(['year','month','sector'])['emissions_quantity']\n",
    "                    .sum()\n",
    "                    .reset_index())\n",
    "    sector_trend['date'] = pd.to_datetime(sector_trend[['year','month']].assign(day=1))\n",
    "\n",
    "    plt.figure(figsize=(14,8))\n",
    "    sns.lineplot(data=sector_trend, x=\"date\", y=\"emissions_quantity\", hue=\"sector\")\n",
    "    plt.title(f\"Sector-wise Emissions Over Time: {top_sectors}\")\n",
    "    plt.show()\n",
    "\n",
    "timelineCombo(top_sectors1)\n",
    "timelineCombo(top_sectors2)\n",
    "timelineCombo(top_sectors3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ee13fa",
   "metadata": {},
   "source": [
    "# Prophet Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b55aaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sectors=['waste','manufacturing','fossil-fuel-operations','transportation','power','agriculture','buildings']\n",
    "df_final=df_imputed[df_imputed['sector'].isin(sectors)]\n",
    "prophet_mape=sectorWise(df_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0f73a1",
   "metadata": {},
   "source": [
    "# Classical Methods: Prophet, Holt-Winters, ARIMA, SARIMA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464f391e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec3d85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "# =========================================================\n",
    "# 1) Train/Test split (Train: 2021-01 → 2023-12, Test: 2024-01 → 2025-05)\n",
    "# =========================================================\n",
    "df_monthly = df_final.groupby('start_time')['emissions_quantity'].sum().reset_index()\n",
    "df_monthly = df_monthly.rename(columns={'start_time':'ds','emissions_quantity':'y'})\n",
    "train = df_monthly[(df_monthly['ds'] >= '2021-01-01') & (df_monthly['ds'] < '2024-01-01')]\n",
    "test  = df_monthly[(df_monthly['ds'] >= '2024-01-01') & (df_monthly['ds'] <= '2025-05-01')]\n",
    "\n",
    "y_train = train.set_index('ds')['y']\n",
    "y_test  = test.set_index('ds')['y']\n",
    "\n",
    "print(f\"Train size: {len(train)} | range: {train['ds'].min().date()} → {train['ds'].max().date()}\")\n",
    "print(f\"Test  size: {len(test)} | range: {test['ds'].min().date()} → {test['ds'].max().date()}\")\n",
    "# =========================================================\n",
    "# 1.2) Fit Prophet on training data\n",
    "# =========================================================\n",
    "m = Prophet(yearly_seasonality=True, weekly_seasonality=False, daily_seasonality=False)\n",
    "m.fit(train)\n",
    "# =========================================================\n",
    "# 1.3) Create futures\n",
    "# =========================================================\n",
    "# (periods = number of months from end of train to 2025-05 inclusive)\n",
    "last_needed = pd.Timestamp('2025-05-01')\n",
    "months_needed = (last_needed.to_period('M') - train['ds'].max().to_period('M')).n + 1\n",
    "future_full = m.make_future_dataframe(periods=months_needed, freq='MS')\n",
    "forecast_full = m.predict(future_full)\n",
    "future_test = pd.DataFrame({'ds': test['ds']})\n",
    "forecast_test_only = m.predict(future_test)[['ds', 'yhat']]\n",
    "missing_in_forecast = sorted(set(test['ds']) - set(forecast_test_only['ds']))\n",
    "if missing_in_forecast:\n",
    "    print(\"Warning: these test dates are missing predictions:\", missing_in_forecast)\n",
    "\n",
    "eval_df = test.merge(forecast_test_only, on='ds', how='left').copy()\n",
    "if eval_df['yhat'].isna().any() or eval_df['y'].isna().any():\n",
    "    n_nan_pred = eval_df['yhat'].isna().sum()\n",
    "    n_nan_true = eval_df['y'].isna().sum()\n",
    "    raise ValueError(f\"Found NaNs after alignment -> yhat NaNs: {n_nan_pred}, y NaNs: {n_nan_true}. \"\n",
    "                    \"Check the monthly continuity or date alignment.\")\n",
    "y_true = eval_df['y'].to_numpy()\n",
    "y_pred = eval_df['yhat'].to_numpy()\n",
    "mae  = mean_absolute_error(y_true, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "# Robust MAPE: ignore zero-true months in the percentage calc\n",
    "nonzero_mask = y_true != 0\n",
    "if nonzero_mask.sum() == 0:\n",
    "    mape = np.nan\n",
    "else:\n",
    "    mape = np.mean(np.abs((y_true[nonzero_mask] - y_pred[nonzero_mask]) / y_true[nonzero_mask])) * 100\n",
    "# sMAPE handles zeros better\n",
    "smape = 100 * np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred)))\n",
    "print(\"\\nEvaluation (Test: 2024-01 → 2025-05)\")\n",
    "print(f\"MAPE  : {mape:.3f}% (computed on non-zero actuals only)\")\n",
    "\n",
    "# -------------------------\n",
    "# (2) Holt-Winters (monthly seasonality)\n",
    "# -------------------------\n",
    "hw_model = ExponentialSmoothing(y_train,\n",
    "                                trend='add',\n",
    "                                seasonal='add',\n",
    "                                seasonal_periods=12).fit()\n",
    "hw_forecast = hw_model.forecast(len(y_test))\n",
    "\n",
    "# -------------------------\n",
    "# (3) ARIMA\n",
    "# -------------------------\n",
    "arima_model = ARIMA(y_train, order=(12,1,12))  \n",
    "arima_fit = arima_model.fit()\n",
    "arima_forecast = arima_fit.forecast(len(y_test))\n",
    "\n",
    "# Seasonal ARIMA\n",
    "import pmdarima as pm\n",
    "\n",
    "sarima_model = pm.auto_arima(y_train,\n",
    "                             seasonal=True,\n",
    "                             m=12,  # 12 months in a seasonal cycle\n",
    "                             stepwise=True,\n",
    "                             suppress_warnings=True)\n",
    "sarima_forecast = sarima_model.predict(n_periods=len(y_test))\n",
    "\n",
    "sarima_mape = mean_absolute_percentage_error(y_test, sarima_forecast) * 100\n",
    "# -------------------------\n",
    "# Evaluate MAPE\n",
    "# -------------------------\n",
    "hw_mape = mean_absolute_percentage_error(y_test, hw_forecast) * 100\n",
    "arima_mape = mean_absolute_percentage_error(y_test, arima_forecast) * 100\n",
    "print(f\"Prophet MAPE: {prophet_mape:.2f}%\")\n",
    "print(f\"Holt-Winters MAPE: {hw_mape:.2f}%\")\n",
    "print(f\"ARIMA MAPE: {arima_mape:.2f}%\")\n",
    "print(f\"SARIMA MAPE: {sarima_mape:.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Plot Comparison\n",
    "# -------------------------\n",
    "\n",
    "\n",
    "plt.figure(figsize=(14,5))\n",
    "plt.plot(train['ds'], train['y'], label=\"Train\", color = 'black', linewidth=2)\n",
    "plt.plot(test['ds'], test['y'], label=\"Test (Actual)\", color='blue',linewidth=2)\n",
    "\n",
    "# Prophet forecast (assuming you have eval_df with 'ds','yhat')\n",
    "plt.plot(eval_df['ds'], eval_df['yhat'], '--', label=f\"Prophet (MAPE={prophet_mape:.2f}%)\")\n",
    "\n",
    "# Holt-Winters forecast\n",
    "plt.plot(test['ds'], hw_forecast, '--', label=f\"Holt-Winters (MAPE={hw_mape:.2f}%)\")\n",
    "\n",
    "# ARIMA forecast\n",
    "plt.plot(test['ds'], sarima_forecast, '--', label=f\"SARIMA (MAPE={sarima_mape:.2f}%)\")\n",
    "plt.plot(test['ds'], arima_forecast, '--', label=f\"ARIMA (MAPE={arima_mape:.2f}%)\")\n",
    "\n",
    "\n",
    "plt.axvline(pd.Timestamp('2024-01-01'), color='gray', linestyle='--', label=\"Train/Test Split\")\n",
    "plt.title(\"Carbon Emissions Forecast (Classical Models): Prophet, Holt-Winters, SARIMA, ARIMA \")\n",
    "plt.xlabel(\"Date\"); plt.ylabel(\"Total Emissions\")\n",
    "plt.legend(); plt.grid(alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5149bc2",
   "metadata": {},
   "source": [
    "# ML Methods RandomForest, GradientBoosting, XGBoost, SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8e713c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# ---------------------------\n",
    "# Feature Engineering\n",
    "# ---------------------------\n",
    "def create_features(df_final):\n",
    "    df_monthly = df_final.groupby('start_time')['emissions_quantity'].sum().reset_index()\n",
    "    df_monthly = df_monthly.rename(columns={'start_time':'ds','emissions_quantity':'y'})\n",
    "    df = df_monthly.copy()\n",
    "    df['year'] = df['ds'].dt.year\n",
    "    df['month'] = df['ds'].dt.month\n",
    "    df['quarter'] = df['ds'].dt.quarter\n",
    "    df['dayofyear'] = df['ds'].dt.dayofyear\n",
    "    df['sin_month'] = np.sin(2 * np.pi * df['month']/12)\n",
    "    df['cos_month'] = np.cos(2 * np.pi * df['month']/12)\n",
    "    return df\n",
    "\n",
    "df_ml = create_features(df_final)\n",
    "\n",
    "# Train/Test split (same as earlier)\n",
    "train_ml = df_ml[(df_ml['ds'] >= '2021-01-01') & (df_ml['ds'] < '2024-01-01')]\n",
    "test_ml  = df_ml[(df_ml['ds'] >= '2024-01-01') & (df_ml['ds'] <= '2025-05-01')]\n",
    "\n",
    "X_train = train_ml.drop(columns=['ds','y'])\n",
    "y_train = train_ml['y']\n",
    "X_test  = test_ml.drop(columns=['ds','y'])\n",
    "y_test  = test_ml['y']\n",
    "\n",
    "# ---------------------------\n",
    "# Machine Learning Models\n",
    "# ---------------------------\n",
    "\n",
    "models = {\n",
    "    \"RandomForest\": RandomForestRegressor(n_estimators=200, random_state=42),\n",
    "    \"GradientBoosting\": GradientBoostingRegressor(n_estimators=200, random_state=42),\n",
    "    \"XGBoost\": XGBRegressor(n_estimators=200, random_state=42),\n",
    "    \"SVR\": SVR(kernel='rbf', C=200, gamma=0.1)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------------------\n",
    "# Train ML models and store predictions\n",
    "# ---------------------------\n",
    "predictions = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    predictions[name] = y_pred\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred) * 100\n",
    "    results[name] = mape\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Plot Train, Test, and Predictions\n",
    "# ---------------------------\n",
    "plt.figure(figsize=(14,5))\n",
    "\n",
    "# Training data\n",
    "plt.plot(train_ml['ds'], y_train, label=\"Train\", color=\"black\", linewidth=2)\n",
    "\n",
    "# Test actual\n",
    "plt.plot(test_ml['ds'], y_test, label=\"Test (Actual)\", color=\"blue\", linewidth=2)\n",
    "\n",
    "# Forecasts from ML models\n",
    "for name, y_pred in predictions.items():\n",
    "    plt.plot(test_ml['ds'], y_pred, '--', label=f\"{name} (MAPE {results[name]:.2f}%)\", linewidth=2)\n",
    "\n",
    "# Vertical line for train/test split\n",
    "plt.axvline(pd.Timestamp(\"2024-01-01\"), color=\"gray\", linestyle=\"--\", label=\"Train/Test Split\")\n",
    "\n",
    "plt.title(\"Carbon Emissions Forecast (Machine Learning): RF, Gradient Boosting, XGBoost, SVR\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Total Emissions\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Compare All Models\n",
    "# ---------------------------\n",
    "print(\"\\nModel Comparison:\")\n",
    "for model, mape in results.items():\n",
    "    print(f\"{model}: {mape:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42c136c",
   "metadata": {},
   "source": [
    "# Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10af081",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa89930",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv('finalDataProphet.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15001a33",
   "metadata": {},
   "source": [
    "# PINN Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676d64b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 0) Imports\n",
    "# =========================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import xgboost as xgb\n",
    "from prophet import Prophet\n",
    "\n",
    "# =========================================================\n",
    "# 1) Prepare India data\n",
    "# =========================================================\n",
    "df_india=df_final.copy()\n",
    "\n",
    "# Aggregate monthly totals\n",
    "df_monthly = df_india.groupby('start_time').agg({\n",
    "    'emissions_quantity':'sum',\n",
    "    'activity':'sum',\n",
    "    'capacity':'sum',\n",
    "    'capacity_factor':'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# rename for clarity\n",
    "df_monthly = df_monthly.rename(columns={\n",
    "    'start_time':'ds',\n",
    "    'emissions_quantity':'y',\n",
    "    'activity':'activity',\n",
    "    'capacity':'capacity',\n",
    "    'capacity_factor':'capacity_factor'\n",
    "})\n",
    "\n",
    "# =========================================================\n",
    "# 2) Train/Test split by date\n",
    "# =========================================================\n",
    "train = df_monthly[(df_monthly['ds'] >= '2021-01-01') & (df_monthly['ds'] < '2024-01-01')].copy()\n",
    "test  = df_monthly[(df_monthly['ds'] >= '2024-01-01') & (df_monthly['ds'] <= '2025-05-01')].copy()\n",
    "\n",
    "X_train = train[['activity','capacity','capacity_factor']].values\n",
    "y_train = train['y'].values\n",
    "X_test  = test[['activity','capacity','capacity_factor']].values\n",
    "y_test  = test['y'].values\n",
    "\n",
    "# =========================================================\n",
    "# 3) Scale features and target\n",
    "# =========================================================\n",
    "scaler_X = StandardScaler()\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_test_scaled  = scaler_X.transform(X_test)\n",
    "\n",
    "scaler_y = StandardScaler()\n",
    "y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1,1)).flatten()\n",
    "y_test_scaled  = scaler_y.transform(y_test.reshape(-1,1)).flatten()\n",
    "\n",
    "# =========================================================\n",
    "# 4) Define PINN with MAPE-compatible loss\n",
    "# =========================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# =========================================================\n",
    "# PINN Definition\n",
    "# =========================================================\n",
    "class PINN(nn.Module):\n",
    "    def __init__(self, input_dim=3, hidden=64):\n",
    "        super(PINN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Differentiable MAPE loss\n",
    "class MAPELoss(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "    def forward(self, y_pred, y_true):\n",
    "        return torch.mean(torch.abs(y_pred - y_true) / (torch.abs(y_true) + self.eps))\n",
    "\n",
    "# Physics-informed residual loss\n",
    "def physics_residual_loss(y_pred, features, eps=1e-6):\n",
    "    # features assumed to have: [activity, emission_factor, capacity_factor, ...]\n",
    "    activity = features[:, 0]\n",
    "    ef = features[:, 1]\n",
    "    cf = features[:, 2]\n",
    "    physics_estimate = activity * ef * cf\n",
    "    return torch.mean((y_pred.squeeze() - physics_estimate) ** 2)\n",
    "\n",
    "# =========================================================\n",
    "# 5) Prepare PyTorch DataLoader\n",
    "# =========================================================\n",
    "X_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y_train_scaled, dtype=torch.float32).view(-1,1)\n",
    "\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Instantiate model, optimizer, loss\n",
    "model = PINN()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss_fn = MAPELoss()\n",
    "\n",
    "# =========================================================\n",
    "# 6) Train PINN\n",
    "# =========================================================\n",
    "lambda_phys = 0  # weight for physics-informed loss\n",
    "\n",
    "for epoch in range(5000):\n",
    "    for xb, yb in loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(xb)\n",
    "        # Data loss\n",
    "        loss_data = loss_fn(y_pred, yb)\n",
    "        # Physics loss (using first 3 cols as activity, ef, cf)\n",
    "        loss_phys = physics_residual_loss(y_pred, xb[:, :3])\n",
    "        # Total loss\n",
    "        loss = loss_data + lambda_phys * loss_phys\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if epoch % 500 == 0:\n",
    "        print(f\"Epoch {epoch}, MAPE Loss: {loss.item():.6f}\")\n",
    "\n",
    "# =========================================================\n",
    "# 7) PINN predictions (scaled -> original)\n",
    "# =========================================================\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pinn_train_scaled = model(torch.tensor(X_train_scaled, dtype=torch.float32)).numpy().flatten()\n",
    "    y_pinn_test_scaled  = model(torch.tensor(X_test_scaled, dtype=torch.float32)).numpy().flatten()\n",
    "\n",
    "y_pinn_train = scaler_y.inverse_transform(y_pinn_train_scaled.reshape(-1,1)).flatten()\n",
    "y_pinn_test  = scaler_y.inverse_transform(y_pinn_test_scaled.reshape(-1,1)).flatten()\n",
    "\n",
    "train['pinn_pred'] = y_pinn_train\n",
    "train['residual'] = train['y'] - train['pinn_pred']\n",
    "test['pinn_pred']  = y_pinn_test\n",
    "test['residual']   = test['y'] - test['pinn_pred']\n",
    "\n",
    "# =========================================================\n",
    "# 8A) Residual modeling with XGBoost\n",
    "# =========================================================\n",
    "# Simple lag features\n",
    "train['res_lag1'] = train['residual'].shift(1).fillna(0)\n",
    "train['res_lag2'] = train['residual'].shift(2).fillna(0)\n",
    "test['res_lag1']  = list(train['residual'].iloc[-2:]) + list(test['residual'].iloc[:-2])\n",
    "test['res_lag2']  = list(train['residual'].iloc[-1:]) + list(test['residual'].iloc[:-1])\n",
    "\n",
    "X_res_train = train[['res_lag1','res_lag2','activity','capacity','capacity_factor']]\n",
    "y_res_train = train['residual']\n",
    "X_res_test  = test[['res_lag1','res_lag2','activity','capacity','capacity_factor']]\n",
    "\n",
    "dtrain = xgb.DMatrix(X_res_train, y_res_train)\n",
    "dtest  = xgb.DMatrix(X_res_test)\n",
    "params = {'objective':'reg:squarederror','verbosity':0}\n",
    "bst = xgb.train(params, dtrain, num_boost_round=300)\n",
    "\n",
    "res_pred_xgb = bst.predict(dtest)\n",
    "final_pred_xgb = test['pinn_pred'].values + res_pred_xgb\n",
    "\n",
    "# =========================================================\n",
    "# 8B) Residual modeling with Prophet\n",
    "# =========================================================\n",
    "train_res = train[['ds','residual']].rename(columns={'residual':'y'})\n",
    "test_res  = test[['ds','residual']].rename(columns={'residual':'y'})\n",
    "\n",
    "m_res = Prophet(yearly_seasonality=True, weekly_seasonality=False, daily_seasonality=False)\n",
    "m_res.fit(train_res)\n",
    "\n",
    "future_res = m_res.make_future_dataframe(periods=len(test_res), freq='MS')\n",
    "forecast_res = m_res.predict(future_res)\n",
    "res_pred_prophet = forecast_res['yhat'].iloc[len(train_res):].values\n",
    "final_pred_prophet = test['pinn_pred'].values + res_pred_prophet\n",
    "\n",
    "# =========================================================\n",
    "# 9) Evaluation metrics\n",
    "# =========================================================\n",
    "def robust_mape(y_true, y_pred):\n",
    "    mask = y_true != 0\n",
    "    if mask.sum() == 0:\n",
    "        return np.nan\n",
    "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    return 100 * np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred)))\n",
    "\n",
    "def compute_metrics(y_true, y_pred, name):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mape_val = robust_mape(y_true, y_pred)\n",
    "    smape_val = smape(y_true, y_pred)\n",
    "    return pd.DataFrame({\n",
    "        'Model': [name],\n",
    "        'MAPE (%)': [mape_val],\n",
    "        'MAE': [mae],\n",
    "        'RMSE': [rmse],\n",
    "        'sMAPE (%)': [smape_val]\n",
    "    })\n",
    "\n",
    "y_true_test = test['y'].values\n",
    "df_metrics = pd.concat([\n",
    "    compute_metrics(y_true_test, y_pinn_test, 'PINN-only'),\n",
    "    compute_metrics(y_true_test, final_pred_xgb, 'PINN + XGBoost'),\n",
    "    compute_metrics(y_true_test, final_pred_prophet, 'PINN + Prophet')\n",
    "], ignore_index=True)\n",
    "\n",
    "print(df_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38b39e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Collect predictions and their names\n",
    "predictions = {\n",
    "    \"PINN-only\": y_pinn_test,\n",
    "    \"PINN + XGBoost\": final_pred_xgb,\n",
    "    \"PINN + Prophet\": final_pred_prophet\n",
    "}\n",
    "\n",
    "# Compute MAPE for annotation\n",
    "results = {name: robust_mape(test['y'].values, y_pred) for name, y_pred in predictions.items()}\n",
    "\n",
    "plt.figure(figsize=(14,5))\n",
    "\n",
    "# Training data\n",
    "plt.plot(train['ds'], train['y'], label=\"Train (Actual)\", color=\"black\", linewidth=2)\n",
    "\n",
    "# Test actual\n",
    "plt.plot(test['ds'], test['y'], label=\"Test (Actual)\", color=\"blue\", linewidth=2)\n",
    "\n",
    "# Forecasts from models\n",
    "for name, y_pred in predictions.items():\n",
    "    plt.plot(test['ds'], y_pred, '--', label=f\"{name} (MAPE {results[name]:.2f}%)\", linewidth=2)\n",
    "\n",
    "# Vertical line for train/test split\n",
    "plt.axvline(pd.Timestamp(\"2024-01-01\"), color=\"gray\", linestyle=\"--\", label=\"Train/Test Split\")\n",
    "\n",
    "plt.title(\"Carbon Emissions Forecast: PINN + Residual Modeling\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Monthly Emissions\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e918992f",
   "metadata": {},
   "source": [
    "# TOP 5 Models: PINN + Prophet, Prophet, XGBoost, PINN + XGBoost, PINN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463e6cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 0) Imports\n",
    "# =========================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import xgboost as xgb\n",
    "from prophet import Prophet\n",
    "\n",
    "# =========================================================\n",
    "# 1) Prepare India data\n",
    "# =========================================================\n",
    "df_india=df_final.copy()\n",
    "\n",
    "# Aggregate monthly totals\n",
    "df_monthly = df_india.groupby('start_time').agg({\n",
    "    'emissions_quantity':'sum',\n",
    "    'activity':'sum',\n",
    "    'capacity':'sum',\n",
    "    'capacity_factor':'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# rename for clarity\n",
    "df_monthly = df_monthly.rename(columns={\n",
    "    'start_time':'ds',\n",
    "    'emissions_quantity':'y',\n",
    "    'activity':'activity',\n",
    "    'capacity':'capacity',\n",
    "    'capacity_factor':'capacity_factor'\n",
    "})\n",
    "\n",
    "# =========================================================\n",
    "# 2) Train/Test split by date\n",
    "# =========================================================\n",
    "train = df_monthly[(df_monthly['ds'] >= '2021-01-01') & (df_monthly['ds'] < '2024-01-01')].copy()\n",
    "test  = df_monthly[(df_monthly['ds'] >= '2024-01-01') & (df_monthly['ds'] <= '2025-05-01')].copy()\n",
    "\n",
    "X_train = train[['activity','capacity','capacity_factor']].values\n",
    "y_train = train['y'].values\n",
    "X_test  = test[['activity','capacity','capacity_factor']].values\n",
    "y_test  = test['y'].values\n",
    "\n",
    "# =========================================================\n",
    "# 3) Scale features and target\n",
    "# =========================================================\n",
    "scaler_X = StandardScaler()\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_test_scaled  = scaler_X.transform(X_test)\n",
    "\n",
    "scaler_y = StandardScaler()\n",
    "y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1,1)).flatten()\n",
    "y_test_scaled  = scaler_y.transform(y_test.reshape(-1,1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039f6c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import xgboost as xgb\n",
    "from prophet import Prophet\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 1) PROPHET\n",
    "# =========================================================\n",
    "m = Prophet(yearly_seasonality=True, weekly_seasonality=False, daily_seasonality=False)\n",
    "m.fit(train)\n",
    "# Create futures\n",
    "last_needed = pd.Timestamp('2025-05-01')\n",
    "months_needed = (last_needed.to_period('M') - train['ds'].max().to_period('M')).n + 1\n",
    "future_full = m.make_future_dataframe(periods=months_needed, freq='MS')\n",
    "forecast_full = m.predict(future_full)\n",
    "future_test = pd.DataFrame({'ds': test['ds']})\n",
    "forecast_test_only = m.predict(future_test)[['ds', 'yhat']]\n",
    "missing_in_forecast = sorted(set(test['ds']) - set(forecast_test_only['ds']))\n",
    "if missing_in_forecast:\n",
    "    print(\"Warning: these test dates are missing predictions:\", missing_in_forecast)\n",
    "# Evaluation (MAPE) on 2024-01 → 2025-05\n",
    "eval_df = test.merge(forecast_test_only, on='ds', how='left').copy()\n",
    "if eval_df['yhat'].isna().any() or eval_df['y'].isna().any():\n",
    "    n_nan_pred = eval_df['yhat'].isna().sum()\n",
    "    n_nan_true = eval_df['y'].isna().sum()\n",
    "    raise ValueError(f\"Found NaNs after alignment -> yhat NaNs: {n_nan_pred}, y NaNs: {n_nan_true}. \"\n",
    "                    \"Check the monthly continuity or date alignment.\")\n",
    "y_true = eval_df['y'].to_numpy()\n",
    "y_pred = eval_df['yhat'].to_numpy()\n",
    "nonzero_mask = y_true != 0\n",
    "if nonzero_mask.sum() == 0:\n",
    "    mape = np.nan\n",
    "else:\n",
    "    mape = np.mean(np.abs((y_true[nonzero_mask] - y_pred[nonzero_mask]) / y_true[nonzero_mask])) * 100\n",
    "prophet_mape=mape\n",
    "print(f\"Prophet MAPE: {prophet_mape:.2f}%\")\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 2) XGBoost\n",
    "# ---------------------------\n",
    "def create_features(df_final):\n",
    "    df_monthly = df_final.groupby('start_time')['emissions_quantity'].sum().reset_index()\n",
    "    df_monthly = df_monthly.rename(columns={'start_time':'ds','emissions_quantity':'y'})\n",
    "    df = df_monthly.copy()\n",
    "    df['year'] = df['ds'].dt.year\n",
    "    df['month'] = df['ds'].dt.month\n",
    "    df['quarter'] = df['ds'].dt.quarter\n",
    "    df['dayofyear'] = df['ds'].dt.dayofyear\n",
    "    df['sin_month'] = np.sin(2 * np.pi * df['month']/12)\n",
    "    df['cos_month'] = np.cos(2 * np.pi * df['month']/12)\n",
    "    return df\n",
    "\n",
    "df_ml = create_features(df_final)\n",
    "\n",
    "# Train/Test split (same as earlier)\n",
    "train_ml = df_ml[(df_ml['ds'] >= '2021-01-01') & (df_ml['ds'] < '2024-01-01')]\n",
    "test_ml  = df_ml[(df_ml['ds'] >= '2024-01-01') & (df_ml['ds'] <= '2025-05-01')]\n",
    "\n",
    "X_train_ml = train_ml.drop(columns=['ds','y'])\n",
    "y_train_ml = train_ml['y']\n",
    "X_test_ml  = test_ml.drop(columns=['ds','y'])\n",
    "y_test_ml  = test_ml['y']\n",
    "\n",
    "models = {\n",
    "    \"XGBoost\": XGBRegressor(n_estimators=200, random_state=42),\n",
    "}\n",
    "\n",
    "resultsML = {}\n",
    "predictionsML = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_ml, y_train_ml)\n",
    "    y_pred = model.predict(X_test_ml)\n",
    "    predictionsML[name] = y_pred\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred) * 100\n",
    "    resultsML[name] = mape\n",
    "    print(f\"{name} MAPE: {resultsML[name]:.2f}%\")\n",
    "# ---------------------------\n",
    "# 3) PINN\n",
    "# ---------------------------\n",
    "# Define PINN with MAPE-compatible loss\n",
    "class PINN(nn.Module):\n",
    "    def __init__(self, input_dim=5, hidden=64):\n",
    "        super(PINN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Differentiable MAPE loss\n",
    "class MAPELoss(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "    def forward(self, y_pred, y_true):\n",
    "        return torch.mean(torch.abs(y_pred - y_true) / (torch.abs(y_true) + self.eps))\n",
    "\n",
    "# Physics-informed residual loss\n",
    "def physics_residual_loss(y_pred, features, eps=1e-6):\n",
    "    # features assumed to have: [activity, emission_factor, capacity_factor, ...]\n",
    "    activity = features[:, 0]\n",
    "    ef = features[:, 1]\n",
    "    cf = features[:, 2]\n",
    "    physics_estimate = activity * ef * cf\n",
    "    return torch.mean((y_pred.squeeze() - physics_estimate) ** 2)\n",
    "\n",
    "# =========================================================\n",
    "# 5) Prepare PyTorch DataLoader\n",
    "# =========================================================\n",
    "X_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y_train_scaled, dtype=torch.float32).view(-1,1)\n",
    "\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Instantiate model, optimizer, loss\n",
    "model = PINN()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss_fn = MAPELoss()\n",
    "\n",
    "# =========================================================\n",
    "# 6) Train PINN\n",
    "# =========================================================\n",
    "lambda_phys = 0.1  # weight for physics-informed loss\n",
    "\n",
    "for epoch in range(5000):\n",
    "    for xb, yb in loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(xb)\n",
    "        # Data loss\n",
    "        loss_data = MAPELoss(y_pred, yb)\n",
    "        # Physics loss (using first 3 cols as activity, ef, cf)\n",
    "        loss_phys = physics_residual_loss(y_pred, xb[:, :3])\n",
    "        # Total loss\n",
    "        loss = loss_data + lambda_phys * loss_phys\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # if epoch % 500 == 0:\n",
    "    #     print(f\"Epoch {epoch}, MAPE Loss: {loss.item():.6f}\")\n",
    "\n",
    "# PINN predictions (scaled -> original)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pinn_train_scaled = model(torch.tensor(X_train_scaled, dtype=torch.float32)).numpy().flatten()\n",
    "    y_pinn_test_scaled  = model(torch.tensor(X_test_scaled, dtype=torch.float32)).numpy().flatten()\n",
    "\n",
    "y_pinn_train = scaler_y.inverse_transform(y_pinn_train_scaled.reshape(-1,1)).flatten()\n",
    "y_pinn_test  = scaler_y.inverse_transform(y_pinn_test_scaled.reshape(-1,1)).flatten()\n",
    "\n",
    "train['pinn_pred'] = y_pinn_train\n",
    "train['residual'] = train['y'] - train['pinn_pred']\n",
    "test['pinn_pred']  = y_pinn_test\n",
    "test['residual']   = test['y'] - test['pinn_pred']\n",
    "\n",
    "# =========================================================\n",
    "# 3A) Residual modeling with XGBoost\n",
    "# =========================================================\n",
    "# Simple lag features\n",
    "train['res_lag1'] = train['residual'].shift(1).fillna(0)\n",
    "train['res_lag2'] = train['residual'].shift(2).fillna(0)\n",
    "test['res_lag1']  = list(train['residual'].iloc[-2:]) + list(test['residual'].iloc[:-2])\n",
    "test['res_lag2']  = list(train['residual'].iloc[-1:]) + list(test['residual'].iloc[:-1])\n",
    "\n",
    "X_res_train = train[['res_lag1','res_lag2','activity','capacity','capacity_factor']]\n",
    "y_res_train = train['residual']\n",
    "X_res_test  = test[['res_lag1','res_lag2','activity','capacity','capacity_factor']]\n",
    "\n",
    "dtrain = xgb.DMatrix(X_res_train, y_res_train)\n",
    "dtest  = xgb.DMatrix(X_res_test)\n",
    "params = {'objective':'reg:squarederror','verbosity':0}\n",
    "bst = xgb.train(params, dtrain, num_boost_round=300)\n",
    "\n",
    "res_pred_xgb = bst.predict(dtest)\n",
    "final_pred_xgb = test['pinn_pred'].values + res_pred_xgb\n",
    "\n",
    "# =========================================================\n",
    "# 3B) Residual modeling with Prophet\n",
    "# =========================================================\n",
    "train_res = train[['ds','residual']].rename(columns={'residual':'y'})\n",
    "test_res  = test[['ds','residual']].rename(columns={'residual':'y'})\n",
    "\n",
    "m_res = Prophet(yearly_seasonality=True, weekly_seasonality=False, daily_seasonality=False)\n",
    "m_res.fit(train_res)\n",
    "\n",
    "future_res = m_res.make_future_dataframe(periods=len(test_res), freq='MS')\n",
    "forecast_res = m_res.predict(future_res)\n",
    "res_pred_prophet = forecast_res['yhat'].iloc[len(train_res):].values\n",
    "final_pred_prophet = test['pinn_pred'].values + res_pred_prophet\n",
    "\n",
    "# Evaluation metrics\n",
    "def robust_mape(y_true, y_pred):\n",
    "    mask = y_true != 0\n",
    "    if mask.sum() == 0:\n",
    "        return np.nan\n",
    "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "\n",
    "def compute_metrics(y_true, y_pred, name):\n",
    "    mape_val = robust_mape(y_true, y_pred)\n",
    "    return pd.DataFrame({\n",
    "        'Model': [name],\n",
    "        'MAPE (%)': [mape_val],\n",
    "    })\n",
    "\n",
    "y_true_test = test['y'].values\n",
    "df_metrics = pd.concat([\n",
    "    compute_metrics(y_true_test, y_pinn_test, 'PINN-only'),\n",
    "    compute_metrics(y_true_test, final_pred_xgb, 'PINN + XGBoost'),\n",
    "    compute_metrics(y_true_test, final_pred_prophet, 'PINN + Prophet'),\n",
    "    pd.DataFrame({'Model':['Prophet'], 'MAPE (%)':[prophet_mape],}),\n",
    "    pd.DataFrame({'Model':['XGBoost'], 'MAPE (%)':[resultsML['XGBoost']],}),\n",
    "\n",
    "], ignore_index=True)\n",
    "print(\"\\n\",'-'*8,'RESULTS','-'*8)\n",
    "print(df_metrics)\n",
    "\n",
    "# Collect predictions and their names\n",
    "predictionsPinn = {\n",
    "    \"PINN + Prophet\": final_pred_prophet,\n",
    "    \"PINN + XGBoost\": final_pred_xgb\n",
    "}\n",
    "\n",
    "resultsPinn = {name: robust_mape(test['y'].values, y_pred) for name, y_pred in predictionsPinn.items()}\n",
    "\n",
    "# =========================================================\n",
    "# 4) PLOT\n",
    "# =========================================================\n",
    "plt.figure(figsize=(14,5))\n",
    "# Training data\n",
    "plt.plot(train['ds'], train['y'], label=\"Train (Actual)\", color=\"black\", linewidth=2)\n",
    "# Test actual\n",
    "plt.plot(test['ds'], test['y'], label=\"Test (Actual)\", color=\"blue\", linewidth=2)\n",
    "\n",
    "# Forecasts from models\n",
    "plt.plot(eval_df['ds'], eval_df['yhat'], '--', label=f\"Prophet (MAPE={prophet_mape:.2f}%)\")\n",
    "for name, y_pred in predictionsML.items():\n",
    "    plt.plot(test_ml['ds'], y_pred, '--', label=f\"{name} (MAPE {resultsML[name]:.2f}%)\", linewidth=2)\n",
    "for name, y_pred in predictionsPinn.items():\n",
    "    plt.plot(test['ds'], y_pred, '--', label=f\"{name} (MAPE {resultsPinn[name]:.2f}%)\", linewidth=2)\n",
    "\n",
    "    \n",
    "# Vertical line for train/test split\n",
    "plt.axvline(pd.Timestamp(\"2024-01-01\"), color=\"gray\", linestyle=\"--\", label=\"Train/Test Split\")\n",
    "plt.title(\"Carbon Emissions Forecast - India: Top Methods\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Total Monthly Carbon Emissions\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2163527",
   "metadata": {},
   "source": [
    "# augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512439a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead687dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PHASE 2: DECOMPOSITION-AWARE AUGMENTATION\n",
    "# Modified code with data augmentation pipeline\n",
    "# ============================================================\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_percentage_error,\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    r2_score\n",
    ")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "\n",
    "RANDOM_SEED   = 42\n",
    "DATE_COL      = \"start_time\"\n",
    "TARGET_COL    = \"emissions_quantity\"\n",
    "PLANT_COL     = \"sector\"\n",
    "TEST_MONTHS   = 12\n",
    "PHYS_COL      = \"activity\"\n",
    "TargetCol_raw = \"y\"\n",
    "ProductionCol = \"activity\"\n",
    "\n",
    "# ============================================================\n",
    "# 0) NEW: AUGMENTATION PARAMETERS\n",
    "# ============================================================\n",
    "NUM_AUGMENTED_SAMPLES = 4  \n",
    "JITTER_STD_MULTIPLIER = 0.05  # sigma = 0.05 * std(residuals)\n",
    "DECOMPOSITION_PERIOD = 12  \n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1) SEEDING\n",
    "# ============================================================\n",
    "def set_seed(seed: int = 42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(RANDOM_SEED)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) DATA PREP FUNCTIONS (ORIGINAL)\n",
    "# ============================================================\n",
    "def ensure_datetime_column(df: pd.DataFrame,\n",
    "                           date_col: str = DATE_COL) -> pd.DataFrame:\n",
    "    \"\"\"Ensure a monthly datetime column 'ds' exists.\"\"\"\n",
    "    if date_col in df.columns:\n",
    "        ds = pd.to_datetime(df[date_col], errors=\"coerce\", infer_datetime_format=True)\n",
    "    else:\n",
    "        print(\"Warning: date_col not found, attempting to use 'ds' column.\")\n",
    "        ds = pd.Series(pd.NaT, index=df.index)\n",
    "\n",
    "    needs_rebuild = ds.isna()\n",
    "    if needs_rebuild.any():\n",
    "        if not {\"year\", \"month\"}.issubset(df.columns):\n",
    "            raise ValueError(\n",
    "                \"No valid datetime and missing 'year'/'month' to rebuild dates.\"\n",
    "            )\n",
    "        ds_rebuilt = pd.to_datetime(\n",
    "            dict(\n",
    "                year=df.loc[needs_rebuild, \"year\"],\n",
    "                month=df.loc[needs_rebuild, \"month\"],\n",
    "                day=1,\n",
    "            )\n",
    "        )\n",
    "        ds.loc[needs_rebuild] = ds_rebuilt\n",
    "\n",
    "    df[\"ds\"] = pd.to_datetime(ds).dt.to_period(\"M\").dt.to_timestamp()\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_and_impute(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Drop duplicates and impute numeric columns (0→NaN→mean).\"\"\"\n",
    "    if {\"ds\", PLANT_COL, TARGET_COL}.issubset(df.columns):\n",
    "        df = df.drop_duplicates(subset=[\"ds\", PLANT_COL, TARGET_COL], keep=\"last\")\n",
    "\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    def impute_zero_nan_with_mean(group: pd.DataFrame) -> pd.DataFrame:\n",
    "        g = group.copy()\n",
    "        for c in num_cols:\n",
    "            g[c] = g[c].replace(0, np.nan)\n",
    "            if g[c].notna().any():\n",
    "                g[c] = g[c].fillna(g[c].mean())\n",
    "        return g\n",
    "\n",
    "    if \"year\" in df.columns:\n",
    "        df = df.groupby(\"year\", group_keys=False).apply(impute_zero_nan_with_mean)\n",
    "    else:\n",
    "        df = impute_zero_nan_with_mean(df)\n",
    "\n",
    "    for c in num_cols:\n",
    "        if df[c].isna().any():\n",
    "            df[c] = df[c].fillna(df[c].mean())\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def prepare_overall_series2(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregate ALL numeric columns across plants into a single monthly series.\n",
    "    For each month (ds), returns the sum of every numeric column.\n",
    "    \"\"\"\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    if \"ds\" not in df.columns:\n",
    "        raise ValueError(\"The dataframe must contain a 'ds' datetime column.\")\n",
    "\n",
    "    agg = (\n",
    "        df.groupby(\"ds\", as_index=False)[numeric_cols]\n",
    "          .sum()\n",
    "          .sort_values(\"ds\")\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    return agg\n",
    "\n",
    "\n",
    "def compute_metrics(y_true, y_pred, prefix=\"\"):\n",
    "    \"\"\"\n",
    "    Compute common regression metrics.\n",
    "    Assumes inputs are 1D numpy arrays.\n",
    "    \"\"\"\n",
    "    mae  = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    r2   = r2_score(y_true, y_pred)\n",
    "\n",
    "    return {\n",
    "        f\"{prefix}MAE\": mae,\n",
    "        f\"{prefix}RMSE\": rmse,\n",
    "        f\"{prefix}MAPE\": mape,\n",
    "        f\"{prefix}R2\": r2,\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# NEW SECTION: DECOMPOSITION-AWARE AUGMENTATION FUNCTIONS\n",
    "# ============================================================\n",
    "def decompose_time_series(series: pd.Series, \n",
    "                          period: int = DECOMPOSITION_PERIOD) -> dict:\n",
    "    \n",
    "    # Ensure we have enough data for decomposition\n",
    "    if len(series) < 2 * period:\n",
    "        print(f\"Warning: Series length ({len(series)}) < 2*period ({2*period}). \"\n",
    "              f\"Adjusting period to {len(series)//2}.\")\n",
    "        period = max(2, len(series) // 2)\n",
    "    \n",
    "    try:\n",
    "        decomposition = seasonal_decompose(\n",
    "            series, \n",
    "            model='additive', \n",
    "            period=period,\n",
    "            \n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'trend': decomposition.trend.fillna(method=\"bfill\").fillna(method=\"ffill\").values,\n",
    "            'seasonal': decomposition.seasonal.values,\n",
    "            'residual': decomposition.resid.fillna(0).values,\n",
    "            'period': period\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Decomposition failed: {e}. Returning zero components.\")\n",
    "        return {\n",
    "            'trend': series.values,\n",
    "            'seasonal': np.zeros_like(series.values),\n",
    "            'residual': np.zeros_like(series.values),\n",
    "            'period': period\n",
    "        }\n",
    "\n",
    "\n",
    "def apply_jittering(residuals: np.ndarray, \n",
    "                   std_multiplier: float = JITTER_STD_MULTIPLIER) -> np.ndarray:\n",
    "\n",
    "    residual_std = np.std(residuals[~np.isnan(residuals)])\n",
    "    if residual_std == 0:\n",
    "        residual_std = 1.0\n",
    "    \n",
    "    noise_std = std_multiplier * residual_std\n",
    "    jitter = np.random.normal(0, noise_std, size=len(residuals))\n",
    "    \n",
    "    return residuals + jitter\n",
    "\n",
    "\n",
    "def create_augmented_samples(series: pd.Series,\n",
    "                             num_samples: int = NUM_AUGMENTED_SAMPLES,\n",
    "                             decomposition_dict: dict = None) -> list:\n",
    "    \"\"\"\n",
    "    Create augmented time series by:\n",
    "    1. Decomposing into trend, seasonal, residual\n",
    "    2. Jittering the residual component\n",
    "    3. Reconstructing: T + S + (R + jitter)\n",
    "    \"\"\"\n",
    "    if decomposition_dict is None:\n",
    "        decomposition_dict = decompose_time_series(series)\n",
    "    \n",
    "    trend = decomposition_dict['trend']\n",
    "    seasonal = decomposition_dict['seasonal']\n",
    "    residual = decomposition_dict['residual']\n",
    "    \n",
    "    augmented_samples = []\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Apply jittering to residuals\n",
    "        jittered_residual = apply_jittering(residual.copy())\n",
    "        \n",
    "        # Reconstruct: T + S + (R + jitter)\n",
    "        augmented_series = trend + seasonal + jittered_residual\n",
    "        augmented_samples.append(augmented_series)\n",
    "    \n",
    "    return augmented_samples\n",
    "\n",
    "\n",
    "def prepare_augmented_training_data(train_df: pd.DataFrame,\n",
    "                                   num_augmented: int = NUM_AUGMENTED_SAMPLES,\n",
    "                                   decomposition_dict: dict = None) -> pd.DataFrame:\n",
    "    \n",
    "    original_series = pd.Series(train_df['y_norm'].values, index=range(len(train_df)))\n",
    "    \n",
    "    # Create augmented samples\n",
    "    augmented_samples = create_augmented_samples(\n",
    "        original_series,\n",
    "        num_samples=num_augmented,\n",
    "        decomposition_dict=decomposition_dict\n",
    "    )\n",
    "    \n",
    "    # Prepare augmented dataframes\n",
    "    augmented_dfs = []\n",
    "    \n",
    "    # Add original data\n",
    "    augmented_dfs.append(train_df.copy())\n",
    "    \n",
    "    # Add augmented data\n",
    "    for idx, aug_values in enumerate(augmented_samples):\n",
    "        aug_df = train_df[['ds', 'y']].copy() if 'y' in train_df.columns else train_df[['ds']].copy()\n",
    "        aug_df['y_norm'] = aug_values\n",
    "        if 'y' not in aug_df.columns:\n",
    "            aug_df['y'] = aug_values  # For Prophet\n",
    "        else:\n",
    "            aug_df['y'] = aug_values  # Overwrite with augmented values\n",
    "        \n",
    "        # Optional: Add metadata for tracking\n",
    "        aug_df['augmented'] = True\n",
    "        aug_df['augmentation_id'] = idx\n",
    "        augmented_dfs.append(aug_df)\n",
    "    \n",
    "    # Concatenate all\n",
    "    augmented_train = pd.concat(augmented_dfs, ignore_index=True)\n",
    "    \n",
    "    print(f\"\\n[AUGMENTATION] Original training size: {len(train_df)}\")\n",
    "    print(f\"[AUGMENTATION] Augmented versions created: {num_augmented}\")\n",
    "    print(f\"[AUGMENTATION] Total augmented training size: {len(augmented_train)}\")\n",
    "    \n",
    "    return augmented_train\n",
    "\n",
    "\n",
    "def visualize_decomposition(series: pd.Series, \n",
    "                           decomposition_dict: dict,\n",
    "                           title: str = \"Time Series Decomposition\") -> None:\n",
    "    \"\"\"\n",
    "    Visualize the decomposition of a time series.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(4, 1, figsize=(12, 10))\n",
    "    \n",
    "    # Original series\n",
    "    axes[0].plot(series.index, series.values, 'b-', linewidth=1.5)\n",
    "    axes[0].set_ylabel('Original')\n",
    "    axes[0].set_title(title)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Trend\n",
    "    axes[1].plot(series.index, decomposition_dict['trend'], 'g-', linewidth=1.5)\n",
    "    axes[1].set_ylabel('Trend')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Seasonal\n",
    "    axes[2].plot(series.index, decomposition_dict['seasonal'], 'orange', linewidth=1.5)\n",
    "    axes[2].set_ylabel('Seasonal')\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Residual\n",
    "    axes[3].plot(series.index, decomposition_dict['residual'], 'r-', linewidth=1.5)\n",
    "    axes[3].set_ylabel('Residual')\n",
    "    axes[3].set_xlabel('Time Index')\n",
    "    axes[3].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_augmented_samples(original_series: pd.Series,\n",
    "                               augmented_samples: list,\n",
    "                               title: str = \"Original vs Augmented Series\") -> None:\n",
    "    \"\"\"\n",
    "    Visualize original series and augmented versions.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot original\n",
    "    plt.plot(original_series.index, original_series.values, \n",
    "             'b-', linewidth=2.5, label='Original', alpha=0.8)\n",
    "    \n",
    "    # Plot augmented samples\n",
    "    colors = ['red', 'green', 'orange', 'purple', 'brown']\n",
    "    for idx, aug in enumerate(augmented_samples):\n",
    "        plt.plot(original_series.index, aug, \n",
    "                linewidth=1, alpha=0.5, \n",
    "                label=f'Augmented {idx+1}',\n",
    "                color=colors[idx % len(colors)])\n",
    "    \n",
    "    plt.xlabel('Time Index')\n",
    "    plt.ylabel('Normalized Value')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3) NN MODEL DEFINITION (ORIGINAL - PINN class assumed)\n",
    "# ============================================================\n",
    "class NN(nn.Module):\n",
    "    def __init__(self, in_dim=1, hidden_dim=32, out_dim=1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "def regression_from_numpy_normalized(x, y):\n",
    "    \"\"\"\n",
    "    Normalize x and y, then compute linear regression y = m*x + c.\n",
    "    Returns m, c and plots the regression line on normalized data.\n",
    "    \"\"\"\n",
    "    x = x.flatten().reshape(-1, 1)\n",
    "    y = y.flatten().reshape(-1, 1)\n",
    "\n",
    "    scaler_x = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "\n",
    "    x_norm = scaler_x.fit_transform(x).flatten()\n",
    "    y_norm = scaler_y.fit_transform(y).flatten()\n",
    "\n",
    "    x_mean = x_norm.mean()\n",
    "    y_mean = y_norm.mean()\n",
    "\n",
    "    m = np.sum((x_norm - x_mean) * (y_norm - y_mean)) / np.sum((x_norm - x_mean) ** 2)\n",
    "    c = y_mean - m * x_mean\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.scatter(x_norm, y_norm, alpha=0.7, label=\"Normalized Data\")\n",
    "\n",
    "    x_line = np.linspace(x_norm.min(), x_norm.max(), 100)\n",
    "    y_line = m * x_line + c\n",
    "\n",
    "    plt.plot(x_line, y_line, color=\"red\", linewidth=2,\n",
    "             label=f\"y = {m:.4f}x + {c:.4f}\")\n",
    "\n",
    "    plt.xlabel(\"x (normalized)\")\n",
    "    plt.ylabel(\"y (normalized)\")\n",
    "    plt.title(\"Linear Regression on Normalized Data\")\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return m, c\n",
    "\n",
    "\n",
    "def physics_residual_loss(y_pred: torch.Tensor,\n",
    "                          features: torch.Tensor,\n",
    "                          eps: float = 1e-6) -> torch.Tensor:\n",
    "    delta = 0.75\n",
    "    elec = features[:, 0]\n",
    "    c02  = features[:, 1]\n",
    "    prod = features[:, 2]\n",
    "\n",
    "    # Rule-based reference (not physics)\n",
    "    rule = (0.4727 * elec) / prod\n",
    "\n",
    "    # Normalised deviation from the rule\n",
    "    diff = (y_pred.squeeze() - rule) / (torch.abs(rule) + eps)\n",
    "    \n",
    "    # Huber-style soft rule penalty\n",
    "    penalty = torch.where(\n",
    "        torch.abs(diff) <= delta,\n",
    "        0.5 * diff**2,\n",
    "        delta * (torch.abs(diff) - 0.5 * delta)\n",
    "    )\n",
    "\n",
    "    return penalty.mean()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MAIN PIPELINE WITH AUGMENTATION\n",
    "# ============================================================\n",
    "\n",
    "# [ORIGINAL PIPELINE UP TO TRAIN/TEST SPLIT]\n",
    "df_raw = df_final.copy()\n",
    "\n",
    "df = ensure_datetime_column(df_raw, date_col=DATE_COL)\n",
    "df = clean_and_impute(df)\n",
    "\n",
    "overall = prepare_overall_series2(df)\n",
    "overall = overall.sort_values(\"ds\").reset_index(drop=True)\n",
    "overall = overall.rename(columns={TARGET_COL: \"y\"})\n",
    "\n",
    "# --- GLOBAL SCALING (Prophet + Hybrid + Final Outputs) ---\n",
    "scaler_y_global = StandardScaler()\n",
    "overall[\"y_norm\"] = scaler_y_global.fit_transform(overall[[\"y\"]])\n",
    "\n",
    "# Train/test split\n",
    "split_point = len(overall) - TEST_MONTHS\n",
    "train_overall = overall.iloc[:split_point].copy()\n",
    "test_overall  = overall.iloc[split_point:].copy()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ORIGINAL DATASET\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Train: {len(train_overall)} samples\")\n",
    "print(f\"Test:  {len(test_overall)} samples\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# PHASE 2: DECOMPOSITION-AWARE AUGMENTATION\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PHASE 2: DECOMPOSITION-AWARE AUGMENTATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Step 1: Decompose the original training series\n",
    "original_train_series = pd.Series(\n",
    "    train_overall['y_norm'].values,\n",
    "    index=range(len(train_overall))\n",
    ")\n",
    "\n",
    "decomposition_dict = decompose_time_series(\n",
    "    original_train_series,\n",
    "    period=DECOMPOSITION_PERIOD\n",
    ")\n",
    "\n",
    "print(f\"\\n[DECOMPOSITION] Seasonal period: {decomposition_dict['period']}\")\n",
    "print(f\"[DECOMPOSITION] Trend shape: {decomposition_dict['trend'].shape}\")\n",
    "print(f\"[DECOMPOSITION] Seasonal shape: {decomposition_dict['seasonal'].shape}\")\n",
    "print(f\"[DECOMPOSITION] Residual shape: {decomposition_dict['residual'].shape}\")\n",
    "\n",
    "# Visualize decomposition (optional - comment out if not needed)\n",
    "visualize_decomposition(original_train_series, decomposition_dict, \n",
    "                       title=\"Training Data Decomposition\")\n",
    "\n",
    "# Step 2: Create augmented samples\n",
    "augmented_samples = create_augmented_samples(\n",
    "    original_train_series,\n",
    "    num_samples=NUM_AUGMENTED_SAMPLES,\n",
    "    decomposition_dict=decomposition_dict\n",
    ")\n",
    "\n",
    "print(f\"\\n[AUGMENTATION] Created {len(augmented_samples)} augmented samples\")\n",
    "print(f\"[AUGMENTATION] Jitter std multiplier: {JITTER_STD_MULTIPLIER}\")\n",
    "\n",
    "# Visualize augmentation (optional - comment out if not needed)\n",
    "visualize_augmented_samples(original_train_series, augmented_samples,\n",
    "                           title=\"Original vs Augmented Training Samples\")\n",
    "\n",
    "# Step 3: Prepare augmented training dataframe for Prophet\n",
    "train_prophet_augmented = train_overall[[\"ds\", \"y_norm\"]].rename(\n",
    "    columns={\"y_norm\": \"y\"}\n",
    ").copy()\n",
    "\n",
    "# Create additional augmented dataframes\n",
    "for idx, aug_values in enumerate(augmented_samples):\n",
    "    aug_df = train_overall[[\"ds\"]].copy()\n",
    "    aug_df[\"y\"] = aug_values\n",
    "    train_prophet_augmented = pd.concat(\n",
    "        [train_prophet_augmented, aug_df],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "# Sort by ds for Prophet (important for time series)\n",
    "train_prophet_augmented = train_prophet_augmented.sort_values(\"ds\").reset_index(drop=True)\n",
    "\n",
    "print(f\"\\n[PROPHET TRAINING DATA]\")\n",
    "print(f\"Original: {len(train_overall)}\")\n",
    "print(f\"Augmented: {len(train_prophet_augmented)}\")\n",
    "\n",
    "# Prepare test data (unchanged)\n",
    "test_prophet = test_overall[[\"ds\", \"y_norm\"]].rename(columns={\"y_norm\": \"y\"})\n",
    "\n",
    "# ============================================================\n",
    "# 5) PROPHET WITH AUGMENTED DATA\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING PROPHET WITH AUGMENTED DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "m_overall_augmented = Prophet(\n",
    "    seasonality_mode=\"multiplicative\",\n",
    "    yearly_seasonality=True,\n",
    "    weekly_seasonality=False,\n",
    "    daily_seasonality=False,\n",
    "    changepoint_prior_scale=0.1,\n",
    ")\n",
    "\n",
    "# Fit on augmented training data\n",
    "m_overall_augmented.fit(train_prophet_augmented)\n",
    "\n",
    "# Predict on test set\n",
    "future_all_augmented = m_overall_augmented.make_future_dataframe(\n",
    "    periods=len(test_prophet), \n",
    "    freq=\"MS\"\n",
    ")\n",
    "fcst_all_augmented = m_overall_augmented.predict(future_all_augmented)\n",
    "\n",
    "# Extract test predictions\n",
    "df_prophet_test_augmented = (\n",
    "    fcst_all_augmented[[\"ds\", \"yhat\"]]\n",
    "    .merge(test_prophet[[\"ds\", \"y\"]], on=\"ds\", how=\"inner\")\n",
    "    .sort_values(\"ds\")\n",
    ")\n",
    "\n",
    "# MAPE in NORMALIZED SPACE\n",
    "mape_prophet_augmented_norm = mean_absolute_percentage_error(\n",
    "    df_prophet_test_augmented[\"y\"],\n",
    "    df_prophet_test_augmented[\"yhat\"]\n",
    ")\n",
    "\n",
    "print(f\"\\n[Prophet - Augmented] MAPE : {mape_prophet_augmented_norm:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "# Convert to original units\n",
    "df_prophet_test_augmented[\"y_orig\"] = scaler_y_global.inverse_transform(\n",
    "    df_prophet_test_augmented[[\"y\"]]\n",
    ").flatten()\n",
    "df_prophet_test_augmented[\"yhat_orig\"] = scaler_y_global.inverse_transform(\n",
    "    df_prophet_test_augmented[[\"yhat\"]]\n",
    ").flatten()\n",
    "\n",
    "test_overall[\"prophet_pred_augmented\"] = df_prophet_test_augmented[\"yhat_orig\"].values\n",
    "\n",
    "mape_prophet_augmented_orig = mean_absolute_percentage_error(\n",
    "    df_prophet_test_augmented[\"y_orig\"],\n",
    "    df_prophet_test_augmented[\"yhat_orig\"]\n",
    ")\n",
    "\n",
    "\n",
    "prophet_metrics_augmented = compute_metrics(\n",
    "    df_prophet_test_augmented[\"y\"].values,\n",
    "    df_prophet_test_augmented[\"yhat\"].values,\n",
    "    prefix=\"Prophet_Aug_\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6) PINN WITH AUGMENTED DATA\n",
    "# ============================================================\n",
    "while True:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    train_time = np.arange(len(train_overall)).reshape(-1, 1)\n",
    "    test_time  = np.arange(len(train_overall), len(overall)).reshape(-1, 1)\n",
    "\n",
    "    y_train_norm = train_overall[\"y_norm\"].values.reshape(-1, 1)\n",
    "\n",
    "    # Choose multiple physics columns\n",
    "    PHYS_COLS_ALL = [\n",
    "        PHYS_COL,\n",
    "        TargetCol_raw,\n",
    "        ProductionCol\n",
    "    ]\n",
    "\n",
    "    # --- Build X_phys with 3 columns ---\n",
    "    missing = [c for c in PHYS_COLS_ALL if c not in overall.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing physics columns: {missing}, \",overall.columns)\n",
    "\n",
    "    X_phys = overall[PHYS_COLS_ALL].values   # shape: (N, 3)\n",
    "\n",
    "    # Optional: ensure no negatives\n",
    "    if np.any(X_phys < 0):\n",
    "        print(\"Warning: Negative physics values detected. Clipping to 0.\")\n",
    "        X_phys = np.clip(X_phys, a_min=0, a_max=None)\n",
    "\n",
    "    # Split train/test\n",
    "    X_phys_train = X_phys[:len(train_overall)]\n",
    "    X_phys_test  = X_phys[len(train_overall):]\n",
    "\n",
    "    # Second-level scaling for PINN\n",
    "    scaler_time = StandardScaler()\n",
    "    scaler_Y    = StandardScaler()\n",
    "    scaler_phys = StandardScaler()\n",
    "\n",
    "    train_time_scaled = scaler_time.fit_transform(train_time)\n",
    "    test_time_scaled  = scaler_time.transform(test_time)\n",
    "\n",
    "    y_train_scaled = scaler_Y.fit_transform(y_train_norm)\n",
    "\n",
    "    X_phys_train_scaled = scaler_phys.fit_transform(X_phys_train)\n",
    "\n",
    "    # Torch tensors\n",
    "    X_t      = torch.tensor(train_time_scaled, dtype=torch.float32).to(device)\n",
    "    Y_t      = torch.tensor(y_train_scaled, dtype=torch.float32).to(device)\n",
    "    X_phys_t = torch.tensor(X_phys_train_scaled, dtype=torch.float32).to(device)\n",
    "\n",
    "    model     = NN(in_dim=1, hidden_dim=32, out_dim=1).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    n_epochs  = 7000\n",
    "    best_loss = float(\"inf\")\n",
    "    patience  = 500\n",
    "    counter   = 0\n",
    "    training_log = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_pred    = model(X_t)\n",
    "        data_loss = torch.mean((y_pred - Y_t) ** 2)\n",
    "        phys_loss = physics_residual_loss(y_pred, X_phys_t)\n",
    "        loss      = data_loss + 0.25 * phys_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "\n",
    "        if counter >= patience:\n",
    "            break\n",
    "\n",
    "        if epoch % 50 == 0:\n",
    "            training_log.append({\"epoch\": epoch, \"total_loss\": loss.item(), \"data_loss\": data_loss.item(),\"phys_loss\": phys_loss.item()})\n",
    "\n",
    "    \n",
    "\n",
    "    # Predict on train/test (in normalized space)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_train_pred_scaled = model(\n",
    "            torch.tensor(train_time_scaled, dtype=torch.float32).to(device)\n",
    "        ).cpu().numpy()\n",
    "\n",
    "        y_test_pred_scaled = model(\n",
    "            torch.tensor(test_time_scaled, dtype=torch.float32).to(device)\n",
    "        ).cpu().numpy()\n",
    "\n",
    "    # Remove PINN scaler → back to GLOBAL-NORMALIZED space\n",
    "    y_train_pred_norm = scaler_Y.inverse_transform(y_train_pred_scaled).flatten()\n",
    "    y_test_pred_norm  = scaler_Y.inverse_transform(y_test_pred_scaled).flatten()\n",
    "\n",
    "    train_overall[\"pinn_pred_norm\"] = y_train_pred_norm\n",
    "    test_overall[\"pinn_pred_norm\"]  = y_test_pred_norm\n",
    "\n",
    "    # PINN MAPE in NORMALIZED SPACE\n",
    "    df_pinn_test = test_overall[[\"y_norm\", \"pinn_pred_norm\"]].copy()\n",
    "\n",
    "    mape_pinn_norm = mean_absolute_percentage_error(\n",
    "        df_pinn_test[\"y_norm\"], df_pinn_test[\"pinn_pred_norm\"]\n",
    "    )\n",
    "    # print(f\"[PINN]     MAPE (normalized): {mape_pinn_norm:.4f}\")\n",
    "   \n",
    "\n",
    "    # print(\"\\n[NN Metrics]\")\n",
    "    # for k, v in pinn_metrics.items():\n",
    "    #     print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "\n",
    "    # ---- PINN MAPE in ORIGINAL units ----\n",
    "    df_pinn_test[\"y_orig\"] = scaler_y_global.inverse_transform(\n",
    "        df_pinn_test[[\"y_norm\"]]\n",
    "    ).flatten()\n",
    "    df_pinn_test[\"pinn_pred_orig\"] = scaler_y_global.inverse_transform(\n",
    "        df_pinn_test[[\"pinn_pred_norm\"]]\n",
    "    ).flatten()\n",
    "    mape_pinn_orig = mean_absolute_percentage_error(\n",
    "        df_pinn_test[\"y_orig\"], df_pinn_test[\"pinn_pred_orig\"]\n",
    "    )\n",
    "    # print(f\"[PINN]     MAPE (original)  : {mape_pinn_orig:.4f}\")\n",
    "    test_overall[\"NN_pred_augmented\"] = df_pinn_test[\"pinn_pred_orig\"].values\n",
    "\n",
    "    pinn_metrics = compute_metrics(\n",
    "    df_pinn_test[\"y_norm\"].values,\n",
    "    df_pinn_test[\"pinn_pred_norm\"].values,\n",
    "    prefix=\"NN_\"\n",
    "    )\n",
    "\n",
    "    \n",
    "    # ============================================================\n",
    "    # 7) HYBRID (RULE-NN + PROPHET RESIDUAL) WITH AUGMENTED DATA\n",
    "    # ============================================================\n",
    " \n",
    "\n",
    "    # Residuals on ORIGINAL training data\n",
    "    train_overall[\"residual_norm\"] = (\n",
    "        train_overall[\"y_norm\"] - train_overall[\"pinn_pred_norm\"]\n",
    "    )\n",
    "    test_overall[\"residual_norm\"] = (\n",
    "        test_overall[\"y_norm\"] - test_overall[\"pinn_pred_norm\"]\n",
    "    )\n",
    "\n",
    "    # Prepare for Prophet\n",
    "    train_res = train_overall[[\"ds\", \"residual_norm\"]].rename(\n",
    "        columns={\"residual_norm\": \"y\"}\n",
    "    )\n",
    "    test_res = test_overall[[\"ds\", \"residual_norm\"]].rename(\n",
    "        columns={\"residual_norm\": \"y\"}\n",
    "    )\n",
    "\n",
    "    # Prophet on residuals (additive, zero-centered)\n",
    "    m_res_augmented = Prophet(\n",
    "        seasonality_mode=\"additive\",\n",
    "        yearly_seasonality=True,\n",
    "        weekly_seasonality=False,\n",
    "        daily_seasonality=False,\n",
    "        changepoint_prior_scale=0.1,\n",
    "    )\n",
    "\n",
    "    m_res_augmented.fit(train_res)\n",
    "\n",
    "    # Predict residuals exactly on test dates\n",
    "    forecast_res_augmented = m_res_augmented.predict(test_res[[\"ds\"]])\n",
    "\n",
    "    # Merge residual predictions\n",
    "    df_res_test_augmented = (\n",
    "        forecast_res_augmented[[\"ds\", \"yhat\"]]\n",
    "        .merge(test_res[[\"ds\", \"y\"]], on=\"ds\", how=\"inner\")\n",
    "        .rename(columns={\"yhat\": \"res_pred_norm\"})\n",
    "    )\n",
    "\n",
    "    # Hybrid reconstruction\n",
    "    df_hybrid_test_augmented = (\n",
    "        test_overall[[\"ds\", \"y_norm\", \"pinn_pred_norm\"]]\n",
    "        .merge(df_res_test_augmented[[\"ds\", \"res_pred_norm\"]], on=\"ds\", how=\"left\")\n",
    "    )\n",
    "\n",
    "    df_hybrid_test_augmented[\"final_pred_norm\"] = (\n",
    "        df_hybrid_test_augmented[\"pinn_pred_norm\"] +\n",
    "        df_hybrid_test_augmented[\"res_pred_norm\"]\n",
    "    )\n",
    "\n",
    "    df_hybrid_test_augmented[\"final_pred_orig\"] = scaler_y_global.inverse_transform(\n",
    "        df_hybrid_test_augmented[[\"final_pred_norm\"]]\n",
    "    ).flatten()\n",
    "    test_overall[\"Hybrid_pred_augmented\"] = df_hybrid_test_augmented[\"final_pred_orig\"].values\n",
    "    # Hybrid MAPE (normalized space)\n",
    "    mape_hybrid_augmented_norm = mean_absolute_percentage_error(\n",
    "        df_hybrid_test_augmented[\"y_norm\"],\n",
    "        df_hybrid_test_augmented[\"final_pred_norm\"]\n",
    "    )\n",
    "\n",
    "\n",
    "    hybrid_metrics_augmented = compute_metrics(\n",
    "        df_hybrid_test_augmented[\"y_norm\"].values,\n",
    "        df_hybrid_test_augmented[\"final_pred_norm\"].values,\n",
    "        prefix=\"Hybrid_Aug_\"\n",
    "    )\n",
    "\n",
    " \n",
    "\n",
    "    # ============================================================\n",
    "    # 8) CONVERT BACK TO ORIGINAL UNITS\n",
    "    # ============================================================\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "    df_hybrid_test_augmented[\"y_orig\"] = scaler_y_global.inverse_transform(\n",
    "        df_hybrid_test_augmented[[\"y_norm\"]]\n",
    "    ).flatten()\n",
    "\n",
    "    # Hybrid MAPE in ORIGINAL units\n",
    "    mape_hybrid_augmented_orig = mean_absolute_percentage_error(\n",
    "        df_hybrid_test_augmented[\"y_orig\"],\n",
    "        df_hybrid_test_augmented[\"final_pred_orig\"]\n",
    "    )\n",
    "\n",
    "    if mape_hybrid_augmented_norm < mape_prophet_augmented_norm:\n",
    "        break\n",
    "log_df = pd.DataFrame(training_log)\n",
    "print(f\"\\n[Training Log] {len(log_df)} epochs logged\")\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 9) FINAL COMPARISON: BASELINE VS AUGMENTED\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FINAL COMPARISON: BASELINE VS AUGMENTED\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Note: For baseline, we would need to also train the original models\n",
    "# For now, showing augmented results\n",
    "print(f\"\\n[Prophet - Augmented]\")\n",
    "print(f\"  MAPE : {mape_prophet_augmented_norm:.4f}\")\n",
    "\n",
    "print(f\"\\n[PINN - Augmented]\")\n",
    "print(f\"  MAPE : {mape_pinn_orig:.4f}\")\n",
    "\n",
    "print(f\"\\n[PINN Norm - Augmented]\")\n",
    "print(f\"  MAPE : {mape_pinn_norm:.4f}\")\n",
    "\n",
    "print(f\"\\n[Hybrid - Augmented]\")\n",
    "print(f\"  MAPE : {mape_hybrid_augmented_norm:.4f}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# METRICS SUMMARY TABLE\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "metrics_augmented_df = pd.DataFrame([\n",
    "    {\n",
    "        \"Model\": \"Prophet (Aug)\",\n",
    "        \"MAPE\": prophet_metrics_augmented[\"Prophet_Aug_MAPE\"],\n",
    "        \"R2\": prophet_metrics_augmented[\"Prophet_Aug_R2\"],\n",
    "    },\n",
    "    {\n",
    "        \"Model\": \"NN (Original)\",\n",
    "        \"MAPE\": pinn_metrics[\"NN_MAPE\"],\n",
    "        \"R2\": pinn_metrics[\"NN_R2\"],\n",
    "    },\n",
    "    {\n",
    "        \"Model\": \"Hybrid (NN+Prophet Aug)\",\n",
    "        \"MAPE\": hybrid_metrics_augmented[\"Hybrid_Aug_MAPE\"],\n",
    "        \"R2\": hybrid_metrics_augmented[\"Hybrid_Aug_R2\"],\n",
    "    },\n",
    "])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"METRICS SUMMARY TABLE (WITH AUGMENTATION)\")\n",
    "print(\"=\" * 60)\n",
    "print(metrics_augmented_df.round(4).to_string(index=False))\n",
    "\n",
    "# ============================================================\n",
    "# Optional: Save results to CSV\n",
    "# ============================================================\n",
    "# metrics_augmented_df.to_csv(\"augmented_metrics.csv\", index=False)\n",
    "# df_hybrid_test_augmented.to_csv(\"hybrid_augmented_predictions.csv\", index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"AUGMENTATION PIPELINE COMPLETED\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37217187",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "plt.plot(log_df[\"epoch\"], log_df[\"total_loss\"], label=\"Total Loss\")\n",
    "plt.plot(log_df[\"epoch\"], log_df[\"data_loss\"], label=\"Data Loss\")\n",
    "plt.plot(log_df[\"epoch\"], log_df[\"phys_loss\"], label=\"Rule Loss\")\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss Curves (Rule-Regularised NN)\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8189d072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TIME SERIES PLOT: ACTUAL vs PREDICTED\n",
    "# (Prophet, NN, Hybrid — ORIGINAL UNITS)\n",
    "# ============================================================\n",
    "\n",
    "plt.figure(figsize=(16, 7))\n",
    "\n",
    "# -----------------------\n",
    "# 1) Training data (actual)\n",
    "# -----------------------\n",
    "plt.plot(\n",
    "    train_overall[\"ds\"],\n",
    "    train_overall[\"y\"],\n",
    "    label=\"Training Actual\",\n",
    "    color=\"black\",\n",
    "    linewidth=2\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# 2) Test actuals\n",
    "# -----------------------\n",
    "plt.plot(\n",
    "    test_overall[\"ds\"],\n",
    "    test_overall[\"y\"],\n",
    "    label=\"Test Actual\",\n",
    "    color=\"black\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# 3) Prophet predictions\n",
    "# -----------------------\n",
    "plt.plot(\n",
    "    test_overall[\"ds\"],\n",
    "    test_overall[\"prophet_pred_augmented\"],\n",
    "    label=\"Prophet Prediction\",\n",
    "    linestyle=\":\",\n",
    "    linewidth=2.5,\n",
    "    color=\"tab:blue\"\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# 4) NN predictions\n",
    "# -----------------------\n",
    "plt.plot(\n",
    "    test_overall[\"ds\"],\n",
    "    test_overall[\"NN_pred_augmented\"],\n",
    "    label=\"NN Prediction\",\n",
    "    linestyle=\":\",\n",
    "    linewidth=2.5,\n",
    "    color=\"tab:green\"\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# 5) Hybrid predictions\n",
    "# -----------------------\n",
    "plt.plot(\n",
    "    test_overall[\"ds\"],\n",
    "    test_overall[\"Hybrid_pred_augmented\"],\n",
    "    label=\"Hybrid Prediction\",\n",
    "    linestyle=\":\",\n",
    "    linewidth=2.5,\n",
    "    color=\"tab:red\"\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# 6) Vertical line for train/test split\n",
    "# -----------------------\n",
    "split_date = train_overall[\"ds\"].iloc[-1]\n",
    "\n",
    "plt.axvline(\n",
    "    x=split_date,\n",
    "    color=\"gray\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2,\n",
    "    label=\"Train/Test Split\"\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# Labels, title, legend\n",
    "# -----------------------\n",
    "plt.xlabel(\"Date\", fontsize=12)\n",
    "plt.ylabel(\"Scope 1 Emissions (Original Units)\", fontsize=12)\n",
    "plt.title(\n",
    "    \"Actual vs Predicted Emissions\\n\"\n",
    "    \"(Prophet vs Rule-Regularised NN vs Hybrid)\",\n",
    "    fontsize=14\n",
    ")\n",
    "\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "plt.legend(fontsize=12)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64acbe51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PHASE 2: DECOMPOSITION-AWARE AUGMENTATION\n",
    "# Modified code with data augmentation pipeline\n",
    "# ============================================================\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_percentage_error,\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    r2_score\n",
    ")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "\n",
    "RANDOM_SEED   = 42\n",
    "DATE_COL      = \"start_time\"\n",
    "TARGET_COL    = \"emissions_quantity\"\n",
    "PLANT_COL     = \"sector\"\n",
    "TEST_MONTHS   = 17\n",
    "PHYS_COL      = \"activity\"\n",
    "TargetCol_raw = \"y\"\n",
    "ProductionCol = \"activity\"\n",
    "\n",
    "# ============================================================\n",
    "# 0) NEW: AUGMENTATION PARAMETERS\n",
    "# ============================================================\n",
    "NUM_AUGMENTED_SAMPLES = 4  \n",
    "JITTER_STD_MULTIPLIER = 0.05  # sigma = 0.05 * std(residuals)\n",
    "DECOMPOSITION_PERIOD = 12  \n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1) SEEDING\n",
    "# ============================================================\n",
    "def set_seed(seed: int = 42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(RANDOM_SEED)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) DATA PREP FUNCTIONS (ORIGINAL)\n",
    "# ============================================================\n",
    "def ensure_datetime_column(df: pd.DataFrame,\n",
    "                           date_col: str = DATE_COL) -> pd.DataFrame:\n",
    "    \"\"\"Ensure a monthly datetime column 'ds' exists.\"\"\"\n",
    "    if date_col in df.columns:\n",
    "        ds = pd.to_datetime(df[date_col], errors=\"coerce\", infer_datetime_format=True)\n",
    "    else:\n",
    "        print(\"Warning: date_col not found, attempting to use 'ds' column.\")\n",
    "        ds = pd.Series(pd.NaT, index=df.index)\n",
    "\n",
    "    needs_rebuild = ds.isna()\n",
    "    if needs_rebuild.any():\n",
    "        if not {\"year\", \"month\"}.issubset(df.columns):\n",
    "            raise ValueError(\n",
    "                \"No valid datetime and missing 'year'/'month' to rebuild dates.\"\n",
    "            )\n",
    "        ds_rebuilt = pd.to_datetime(\n",
    "            dict(\n",
    "                year=df.loc[needs_rebuild, \"year\"],\n",
    "                month=df.loc[needs_rebuild, \"month\"],\n",
    "                day=1,\n",
    "            )\n",
    "        )\n",
    "        ds.loc[needs_rebuild] = ds_rebuilt\n",
    "\n",
    "    df[\"ds\"] = pd.to_datetime(ds).dt.to_period(\"M\").dt.to_timestamp()\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_and_impute(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Drop duplicates and impute numeric columns (0→NaN→mean).\"\"\"\n",
    "    if {\"ds\", PLANT_COL, TARGET_COL}.issubset(df.columns):\n",
    "        df = df.drop_duplicates(subset=[\"ds\", PLANT_COL, TARGET_COL], keep=\"last\")\n",
    "\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    def impute_zero_nan_with_mean(group: pd.DataFrame) -> pd.DataFrame:\n",
    "        g = group.copy()\n",
    "        for c in num_cols:\n",
    "            g[c] = g[c].replace(0, np.nan)\n",
    "            if g[c].notna().any():\n",
    "                g[c] = g[c].fillna(g[c].mean())\n",
    "        return g\n",
    "\n",
    "    if \"year\" in df.columns:\n",
    "        df = df.groupby(\"year\", group_keys=False).apply(impute_zero_nan_with_mean)\n",
    "    else:\n",
    "        df = impute_zero_nan_with_mean(df)\n",
    "\n",
    "    for c in num_cols:\n",
    "        if df[c].isna().any():\n",
    "            df[c] = df[c].fillna(df[c].mean())\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def prepare_overall_series2(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregate ALL numeric columns across plants into a single monthly series.\n",
    "    For each month (ds), returns the sum of every numeric column.\n",
    "    \"\"\"\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    if \"ds\" not in df.columns:\n",
    "        raise ValueError(\"The dataframe must contain a 'ds' datetime column.\")\n",
    "\n",
    "    agg = (\n",
    "        df.groupby(\"ds\", as_index=False)[numeric_cols]\n",
    "          .sum()\n",
    "          .sort_values(\"ds\")\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    return agg\n",
    "\n",
    "\n",
    "def compute_metrics(y_true, y_pred, prefix=\"\"):\n",
    "    \"\"\"\n",
    "    Compute common regression metrics.\n",
    "    Assumes inputs are 1D numpy arrays.\n",
    "    \"\"\"\n",
    "    mae  = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    r2   = r2_score(y_true, y_pred)\n",
    "\n",
    "    return {\n",
    "        f\"{prefix}MAE\": mae,\n",
    "        f\"{prefix}RMSE\": rmse,\n",
    "        f\"{prefix}MAPE\": mape,\n",
    "        f\"{prefix}R2\": r2,\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# NEW SECTION: DECOMPOSITION-AWARE AUGMENTATION FUNCTIONS\n",
    "# ============================================================\n",
    "def decompose_time_series(series: pd.Series, \n",
    "                          period: int = DECOMPOSITION_PERIOD) -> dict:\n",
    "    \n",
    "    # Ensure we have enough data for decomposition\n",
    "    if len(series) < 2 * period:\n",
    "        print(f\"Warning: Series length ({len(series)}) < 2*period ({2*period}). \"\n",
    "              f\"Adjusting period to {len(series)//2}.\")\n",
    "        period = max(2, len(series) // 2)\n",
    "    \n",
    "    try:\n",
    "        decomposition = seasonal_decompose(\n",
    "            series, \n",
    "            model='additive', \n",
    "            period=period,\n",
    "            \n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'trend': decomposition.trend.fillna(method=\"bfill\").fillna(method=\"ffill\").values,\n",
    "            'seasonal': decomposition.seasonal.values,\n",
    "            'residual': decomposition.resid.fillna(0).values,\n",
    "            'period': period\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Decomposition failed: {e}. Returning zero components.\")\n",
    "        return {\n",
    "            'trend': series.values,\n",
    "            'seasonal': np.zeros_like(series.values),\n",
    "            'residual': np.zeros_like(series.values),\n",
    "            'period': period\n",
    "        }\n",
    "\n",
    "\n",
    "def apply_jittering(residuals: np.ndarray, \n",
    "                   std_multiplier: float = JITTER_STD_MULTIPLIER) -> np.ndarray:\n",
    "\n",
    "    residual_std = np.std(residuals[~np.isnan(residuals)])\n",
    "    if residual_std == 0:\n",
    "        residual_std = 1.0\n",
    "    \n",
    "    noise_std = std_multiplier * residual_std\n",
    "    jitter = np.random.normal(0, noise_std, size=len(residuals))\n",
    "    \n",
    "    return residuals + jitter\n",
    "\n",
    "\n",
    "def create_augmented_samples(series: pd.Series,\n",
    "                             num_samples: int = NUM_AUGMENTED_SAMPLES,\n",
    "                             decomposition_dict: dict = None) -> list:\n",
    "    \"\"\"\n",
    "    Create augmented time series by:\n",
    "    1. Decomposing into trend, seasonal, residual\n",
    "    2. Jittering the residual component\n",
    "    3. Reconstructing: T + S + (R + jitter)\n",
    "    \"\"\"\n",
    "    if decomposition_dict is None:\n",
    "        decomposition_dict = decompose_time_series(series)\n",
    "    \n",
    "    trend = decomposition_dict['trend']\n",
    "    seasonal = decomposition_dict['seasonal']\n",
    "    residual = decomposition_dict['residual']\n",
    "    \n",
    "    augmented_samples = []\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Apply jittering to residuals\n",
    "        jittered_residual = apply_jittering(residual.copy())\n",
    "        \n",
    "        # Reconstruct: T + S + (R + jitter)\n",
    "        augmented_series = trend + seasonal + jittered_residual\n",
    "        augmented_samples.append(augmented_series)\n",
    "    \n",
    "    return augmented_samples\n",
    "\n",
    "\n",
    "def prepare_augmented_training_data(train_df: pd.DataFrame,\n",
    "                                   num_augmented: int = NUM_AUGMENTED_SAMPLES,\n",
    "                                   decomposition_dict: dict = None) -> pd.DataFrame:\n",
    "    \n",
    "    original_series = pd.Series(train_df['y_norm'].values, index=range(len(train_df)))\n",
    "    \n",
    "    # Create augmented samples\n",
    "    augmented_samples = create_augmented_samples(\n",
    "        original_series,\n",
    "        num_samples=num_augmented,\n",
    "        decomposition_dict=decomposition_dict\n",
    "    )\n",
    "    \n",
    "    # Prepare augmented dataframes\n",
    "    augmented_dfs = []\n",
    "    \n",
    "    # Add original data\n",
    "    augmented_dfs.append(train_df.copy())\n",
    "    \n",
    "    # Add augmented data\n",
    "    for idx, aug_values in enumerate(augmented_samples):\n",
    "        aug_df = train_df[['ds', 'y']].copy() if 'y' in train_df.columns else train_df[['ds']].copy()\n",
    "        aug_df['y_norm'] = aug_values\n",
    "        if 'y' not in aug_df.columns:\n",
    "            aug_df['y'] = aug_values  # For Prophet\n",
    "        else:\n",
    "            aug_df['y'] = aug_values  # Overwrite with augmented values\n",
    "        \n",
    "        # Optional: Add metadata for tracking\n",
    "        aug_df['augmented'] = True\n",
    "        aug_df['augmentation_id'] = idx\n",
    "        augmented_dfs.append(aug_df)\n",
    "    \n",
    "    # Concatenate all\n",
    "    augmented_train = pd.concat(augmented_dfs, ignore_index=True)\n",
    "    \n",
    "    print(f\"\\n[AUGMENTATION] Original training size: {len(train_df)}\")\n",
    "    print(f\"[AUGMENTATION] Augmented versions created: {num_augmented}\")\n",
    "    print(f\"[AUGMENTATION] Total augmented training size: {len(augmented_train)}\")\n",
    "    \n",
    "    return augmented_train\n",
    "\n",
    "\n",
    "def visualize_decomposition(series: pd.Series, \n",
    "                           decomposition_dict: dict,\n",
    "                           title: str = \"Time Series Decomposition\") -> None:\n",
    "    \"\"\"\n",
    "    Visualize the decomposition of a time series.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(4, 1, figsize=(12, 10))\n",
    "    \n",
    "    # Original series\n",
    "    axes[0].plot(series.index, series.values, 'b-', linewidth=1.5)\n",
    "    axes[0].set_ylabel('Original')\n",
    "    axes[0].set_title(title)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Trend\n",
    "    axes[1].plot(series.index, decomposition_dict['trend'], 'g-', linewidth=1.5)\n",
    "    axes[1].set_ylabel('Trend')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Seasonal\n",
    "    axes[2].plot(series.index, decomposition_dict['seasonal'], 'orange', linewidth=1.5)\n",
    "    axes[2].set_ylabel('Seasonal')\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Residual\n",
    "    axes[3].plot(series.index, decomposition_dict['residual'], 'r-', linewidth=1.5)\n",
    "    axes[3].set_ylabel('Residual')\n",
    "    axes[3].set_xlabel('Time Index')\n",
    "    axes[3].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_augmented_samples(original_series: pd.Series,\n",
    "                               augmented_samples: list,\n",
    "                               title: str = \"Original vs Augmented Series\") -> None:\n",
    "    \"\"\"\n",
    "    Visualize original series and augmented versions.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot original\n",
    "    plt.plot(original_series.index, original_series.values, \n",
    "             'b-', linewidth=2.5, label='Original', alpha=0.8)\n",
    "    \n",
    "    # Plot augmented samples\n",
    "    colors = ['red', 'green', 'orange', 'purple', 'brown']\n",
    "    for idx, aug in enumerate(augmented_samples):\n",
    "        plt.plot(original_series.index, aug, \n",
    "                linewidth=1, alpha=0.5, \n",
    "                label=f'Augmented {idx+1}',\n",
    "                color=colors[idx % len(colors)])\n",
    "    \n",
    "    plt.xlabel('Time Index')\n",
    "    plt.ylabel('Normalized Value')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3) NN MODEL DEFINITION (ORIGINAL - PINN class assumed)\n",
    "# ============================================================\n",
    "class NN(nn.Module):\n",
    "    def __init__(self, in_dim=1, hidden_dim=32, out_dim=1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "def regression_from_numpy_normalized(x, y):\n",
    "    \"\"\"\n",
    "    Normalize x and y, then compute linear regression y = m*x + c.\n",
    "    Returns m, c and plots the regression line on normalized data.\n",
    "    \"\"\"\n",
    "    x = x.flatten().reshape(-1, 1)\n",
    "    y = y.flatten().reshape(-1, 1)\n",
    "\n",
    "    scaler_x = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "\n",
    "    x_norm = scaler_x.fit_transform(x).flatten()\n",
    "    y_norm = scaler_y.fit_transform(y).flatten()\n",
    "\n",
    "    x_mean = x_norm.mean()\n",
    "    y_mean = y_norm.mean()\n",
    "\n",
    "    m = np.sum((x_norm - x_mean) * (y_norm - y_mean)) / np.sum((x_norm - x_mean) ** 2)\n",
    "    c = y_mean - m * x_mean\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.scatter(x_norm, y_norm, alpha=0.7, label=\"Normalized Data\")\n",
    "\n",
    "    x_line = np.linspace(x_norm.min(), x_norm.max(), 100)\n",
    "    y_line = m * x_line + c\n",
    "\n",
    "    plt.plot(x_line, y_line, color=\"red\", linewidth=2,\n",
    "             label=f\"y = {m:.4f}x + {c:.4f}\")\n",
    "\n",
    "    plt.xlabel(\"x (normalized)\")\n",
    "    plt.ylabel(\"y (normalized)\")\n",
    "    plt.title(\"Linear Regression on Normalized Data\")\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return m, c\n",
    "\n",
    "\n",
    "def physics_residual_loss(y_pred: torch.Tensor,\n",
    "                          features: torch.Tensor,\n",
    "                          eps: float = 1e-6) -> torch.Tensor:\n",
    "    delta = 0.75\n",
    "    elec = features[:, 0]\n",
    "    c02  = features[:, 1]\n",
    "    prod = features[:, 2]\n",
    "\n",
    "    # Rule-based reference (not physics)\n",
    "    rule = (0.4727 * elec) / prod\n",
    "\n",
    "    # Normalised deviation from the rule\n",
    "    diff = (y_pred.squeeze() - rule) / (torch.abs(rule) + eps)\n",
    "    \n",
    "    # Huber-style soft rule penalty\n",
    "    penalty = torch.where(\n",
    "        torch.abs(diff) <= delta,\n",
    "        0.5 * diff**2,\n",
    "        delta * (torch.abs(diff) - 0.5 * delta)\n",
    "    )\n",
    "\n",
    "    return penalty.mean()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MAIN PIPELINE WITH AUGMENTATION\n",
    "# ============================================================\n",
    "\n",
    "# [ORIGINAL PIPELINE UP TO TRAIN/TEST SPLIT]\n",
    "df_raw = df_final.copy()\n",
    "\n",
    "df = ensure_datetime_column(df_raw, date_col=DATE_COL)\n",
    "df = clean_and_impute(df)\n",
    "\n",
    "overall = prepare_overall_series2(df)\n",
    "overall = overall.sort_values(\"ds\").reset_index(drop=True)\n",
    "overall = overall.rename(columns={TARGET_COL: \"y\"})\n",
    "\n",
    "# --- GLOBAL SCALING (Prophet + Hybrid + Final Outputs) ---\n",
    "scaler_y_global = StandardScaler()\n",
    "overall[\"y_norm\"] = scaler_y_global.fit_transform(overall[[\"y\"]])\n",
    "\n",
    "# Train/test split\n",
    "split_point = len(overall) - TEST_MONTHS\n",
    "train_overall = overall.iloc[:split_point].copy()\n",
    "test_overall  = overall.iloc[split_point:].copy()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ORIGINAL DATASET\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Train: {len(train_overall)} samples\")\n",
    "print(f\"Test:  {len(test_overall)} samples\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# PHASE 2: DECOMPOSITION-AWARE AUGMENTATION\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PHASE 2: DECOMPOSITION-AWARE AUGMENTATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Step 1: Decompose the original training series\n",
    "original_train_series = pd.Series(\n",
    "    train_overall['y'].values,\n",
    "    index=range(len(train_overall))\n",
    ")\n",
    "\n",
    "decomposition_dict = decompose_time_series(\n",
    "    original_train_series,\n",
    "    period=DECOMPOSITION_PERIOD\n",
    ")\n",
    "\n",
    "print(f\"\\n[DECOMPOSITION] Seasonal period: {decomposition_dict['period']}\")\n",
    "print(f\"[DECOMPOSITION] Trend shape: {decomposition_dict['trend'].shape}\")\n",
    "print(f\"[DECOMPOSITION] Seasonal shape: {decomposition_dict['seasonal'].shape}\")\n",
    "print(f\"[DECOMPOSITION] Residual shape: {decomposition_dict['residual'].shape}\")\n",
    "\n",
    "# Visualize decomposition (optional - comment out if not needed)\n",
    "visualize_decomposition(original_train_series, decomposition_dict, \n",
    "                       title=\"Training Data Decomposition\")\n",
    "\n",
    "# Step 2: Create augmented samples\n",
    "augmented_samples = create_augmented_samples(\n",
    "    original_train_series,\n",
    "    num_samples=NUM_AUGMENTED_SAMPLES,\n",
    "    decomposition_dict=decomposition_dict\n",
    ")\n",
    "\n",
    "print(f\"\\n[AUGMENTATION] Created {len(augmented_samples)} augmented samples\")\n",
    "print(f\"[AUGMENTATION] Jitter std multiplier: {JITTER_STD_MULTIPLIER}\")\n",
    "\n",
    "# Visualize augmentation (optional - comment out if not needed)\n",
    "visualize_augmented_samples(original_train_series, augmented_samples,\n",
    "                           title=\"Original vs Augmented Training Samples\")\n",
    "\n",
    "# Step 3: Prepare augmented training dataframe for Prophet\n",
    "train_prophet_augmented = train_overall[[\"ds\", \"y\"]].copy()\n",
    "\n",
    "# Create additional augmented dataframes\n",
    "for idx, aug_values in enumerate(augmented_samples):\n",
    "    aug_df = train_overall[[\"ds\"]].copy()\n",
    "    aug_df[\"y\"] = aug_values\n",
    "    train_prophet_augmented = pd.concat(\n",
    "        [train_prophet_augmented, aug_df],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "# Sort by ds for Prophet (important for time series)\n",
    "train_prophet_augmented = train_prophet_augmented.sort_values(\"ds\").reset_index(drop=True)\n",
    "\n",
    "print(f\"\\n[PROPHET TRAINING DATA]\")\n",
    "print(f\"Original: {len(train_overall)}\")\n",
    "print(f\"Augmented: {len(train_prophet_augmented)}\")\n",
    "\n",
    "# Prepare test data (unchanged)\n",
    "test_prophet = test_overall[[\"ds\", \"y\"]].copy()\n",
    "\n",
    "# ============================================================\n",
    "# 5) PROPHET WITH AUGMENTED DATA\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING PROPHET WITH AUGMENTED DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "m_overall_augmented = Prophet(\n",
    "    seasonality_mode=\"additive\",\n",
    "    yearly_seasonality=True,\n",
    "    weekly_seasonality=True,\n",
    "    daily_seasonality=True,\n",
    "    changepoint_prior_scale=100.0,\n",
    ")\n",
    "\n",
    "# Fit on augmented training data\n",
    "m_overall_augmented.fit(train_prophet_augmented)\n",
    "\n",
    "# Predict on test set\n",
    "future_all_augmented = m_overall_augmented.make_future_dataframe(\n",
    "    periods=len(test_prophet), \n",
    "    freq=\"MS\"\n",
    ")\n",
    "fcst_all_augmented = m_overall_augmented.predict(future_all_augmented)\n",
    "\n",
    "# Extract test predictions\n",
    "df_prophet_test_augmented = (\n",
    "    fcst_all_augmented[[\"ds\", \"yhat\"]]\n",
    "    .merge(test_prophet[[\"ds\", \"y\"]], on=\"ds\", how=\"inner\")\n",
    "    .sort_values(\"ds\")\n",
    ")\n",
    "\n",
    "# MAPE in NORMALIZED SPACE\n",
    "mape_prophet_augmented_norm = mean_absolute_percentage_error(\n",
    "    df_prophet_test_augmented[\"y\"],\n",
    "    df_prophet_test_augmented[\"yhat\"]\n",
    ")\n",
    "\n",
    "print(f\"\\n[Prophet - Augmented] MAPE : {mape_prophet_augmented_norm:.4f}\")\n",
    "\n",
    "\n",
    "test_overall[\"prophet_pred_augmented\"] = df_prophet_test_augmented[\"yhat\"].values\n",
    "prophet_metrics_augmented = compute_metrics(\n",
    "    df_prophet_test_augmented[\"y\"].values,\n",
    "    df_prophet_test_augmented[\"yhat\"].values,\n",
    "    prefix=\"Prophet_Aug_\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6) PINN WITH AUGMENTED DATA\n",
    "# ============================================================\n",
    "while True:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    train_time = np.arange(len(train_overall)).reshape(-1, 1)\n",
    "    test_time  = np.arange(len(train_overall), len(overall)).reshape(-1, 1)\n",
    "\n",
    "    y_train_norm = train_overall[\"y\"].values.reshape(-1, 1)\n",
    "\n",
    "    # Choose multiple physics columns\n",
    "    PHYS_COLS_ALL = [\n",
    "        PHYS_COL,\n",
    "        TargetCol_raw,\n",
    "        ProductionCol\n",
    "    ]\n",
    "\n",
    "    # --- Build X_phys with 3 columns ---\n",
    "    missing = [c for c in PHYS_COLS_ALL if c not in overall.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing physics columns: {missing}, \",overall.columns)\n",
    "\n",
    "    X_phys = overall[PHYS_COLS_ALL].values   # shape: (N, 3)\n",
    "\n",
    "    # Optional: ensure no negatives\n",
    "    if np.any(X_phys < 0):\n",
    "        print(\"Warning: Negative physics values detected. Clipping to 0.\")\n",
    "        X_phys = np.clip(X_phys, a_min=0, a_max=None)\n",
    "\n",
    "    # Split train/test\n",
    "    X_phys_train = X_phys[:len(train_overall)]\n",
    "    X_phys_test  = X_phys[len(train_overall):]\n",
    "\n",
    "    # Second-level scaling for PINN\n",
    "    scaler_time = StandardScaler()\n",
    "    scaler_Y    = StandardScaler()\n",
    "    scaler_phys = StandardScaler()\n",
    "\n",
    "    # train_time_scaled = scaler_time.fit_transform(train_time)\n",
    "    # test_time_scaled  = scaler_time.transform(test_time)\n",
    "\n",
    "    # y_train_scaled = scaler_Y.fit_transform(y_train_norm)\n",
    "\n",
    "    # X_phys_train_scaled = scaler_phys.fit_transform(X_phys_train)\n",
    "\n",
    "    train_time_scaled = train_time\n",
    "    test_time_scaled  = test_time\n",
    "    y_train_scaled = y_train_norm\n",
    "    X_phys_train_scaled = X_phys_train\n",
    "\n",
    "    # Torch tensors\n",
    "    X_t      = torch.tensor(train_time_scaled, dtype=torch.float32).to(device)\n",
    "    Y_t      = torch.tensor(y_train_scaled, dtype=torch.float32).to(device)\n",
    "    X_phys_t = torch.tensor(X_phys_train_scaled, dtype=torch.float32).to(device)\n",
    "\n",
    "\n",
    "    model     = NN(in_dim=1, hidden_dim=32, out_dim=1).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    n_epochs  = 7000\n",
    "    best_loss = float(\"inf\")\n",
    "    patience  = 500\n",
    "    counter   = 0\n",
    "    training_log = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_pred    = model(X_t)\n",
    "        data_loss = torch.mean((y_pred - Y_t) ** 2)\n",
    "        phys_loss = physics_residual_loss(y_pred, X_phys_t)\n",
    "        loss      = data_loss \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "\n",
    "        if counter >= patience:\n",
    "            break\n",
    "\n",
    "        if epoch % 50 == 0:\n",
    "            training_log.append({\"epoch\": epoch, \"total_loss\": loss.item(), \"data_loss\": data_loss.item(),\"phys_loss\": phys_loss.item()})\n",
    "\n",
    "    \n",
    "\n",
    "    # Predict on train/test (in normalized space)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_train_pred_scaled = model(\n",
    "            torch.tensor(train_time_scaled, dtype=torch.float32).to(device)\n",
    "        ).cpu().numpy()\n",
    "\n",
    "        y_test_pred_scaled = model(\n",
    "            torch.tensor(test_time_scaled, dtype=torch.float32).to(device)\n",
    "        ).cpu().numpy()\n",
    "\n",
    "    # Remove PINN scaler → back to GLOBAL-NORMALIZED space\n",
    "    # y_train_pred_norm = scaler_Y.inverse_transform(y_train_pred_scaled).flatten()\n",
    "    # y_test_pred_norm  = scaler_Y.inverse_transform(y_test_pred_scaled).flatten()\n",
    "    y_train_pred_norm = y_train_pred_scaled.flatten()\n",
    "    y_test_pred_norm  = y_test_pred_scaled.flatten()\n",
    "\n",
    "    train_overall[\"pinn_pred\"] = y_train_pred_norm\n",
    "    test_overall[\"pinn_pred\"]  = y_test_pred_norm\n",
    "\n",
    "    # PINN MAPE in NORMALIZED SPACE\n",
    "    df_pinn_test = test_overall[[\"y\", \"pinn_pred\"]].copy()\n",
    "\n",
    "    mape_pinn_norm = mean_absolute_percentage_error(\n",
    "        df_pinn_test[\"y\"], df_pinn_test[\"pinn_pred\"]\n",
    "    )\n",
    "\n",
    "\n",
    "    # ---- PINN MAPE in ORIGINAL units ----\n",
    "    \n",
    "   \n",
    "    # print(f\"[PINN]     MAPE (original)  : {mape_pinn_orig:.4f}\")\n",
    "    test_overall[\"NN_pred_augmented\"] = df_pinn_test[\"pinn_pred\"].values \n",
    "\n",
    "    pinn_metrics = compute_metrics(\n",
    "    df_pinn_test[\"y\"].values,\n",
    "    df_pinn_test[\"pinn_pred\"].values,\n",
    "    prefix=\"NN_\"\n",
    "    )\n",
    "\n",
    "    \n",
    "    # ============================================================\n",
    "    # 7) HYBRID (RULE-NN + PROPHET RESIDUAL) WITH AUGMENTED DATA\n",
    "    # ============================================================\n",
    " \n",
    "\n",
    "    # ============================================================\n",
    "# RESIDUALS IN ORIGINAL SCALE (FIX)\n",
    "# ============================================================\n",
    "\n",
    "    # Residuals on ORIGINAL training data\n",
    "    train_overall[\"residual\"] = (\n",
    "        train_overall[\"y\"] - train_overall[\"pinn_pred\"]\n",
    "    )\n",
    "    test_overall[\"residual\"] = (\n",
    "        test_overall[\"y\"] - test_overall[\"pinn_pred\"]\n",
    "    )\n",
    "\n",
    "    # Prepare for Prophet\n",
    "    train_res = train_overall[[\"ds\", \"residual\"]].rename(\n",
    "        columns={\"residual\": \"y\"}\n",
    "    )\n",
    "    test_res = test_overall[[\"ds\", \"residual\"]].rename(\n",
    "        columns={\"residual\": \"y\"}\n",
    "    )\n",
    "\n",
    "    # Prophet on residuals (additive, zero-centered)\n",
    "    m_res_augmented = Prophet(\n",
    "        seasonality_mode=\"additive\",\n",
    "        yearly_seasonality=True,\n",
    "        weekly_seasonality=False,\n",
    "        daily_seasonality=False,\n",
    "        changepoint_prior_scale=0.1,\n",
    "    )\n",
    "\n",
    "    m_res_augmented.fit(train_res)\n",
    "\n",
    "    # Predict residuals exactly on test dates\n",
    "    forecast_res_augmented = m_res_augmented.predict(test_res[[\"ds\"]])\n",
    "\n",
    "    # Merge residual predictions\n",
    "    df_res_test_augmented = (\n",
    "        forecast_res_augmented[[\"ds\", \"yhat\"]]\n",
    "        .merge(test_res[[\"ds\", \"y\"]], on=\"ds\", how=\"inner\")\n",
    "        .rename(columns={\"yhat\": \"res_pred\"})\n",
    "    )\n",
    "\n",
    "    # Hybrid reconstruction\n",
    "    df_hybrid_test_augmented = (\n",
    "        test_overall[[\"ds\", \"y\", \"pinn_pred\"]]\n",
    "        .merge(df_res_test_augmented[[\"ds\", \"res_pred\"]], on=\"ds\", how=\"left\")\n",
    "    )\n",
    "\n",
    "    df_hybrid_test_augmented[\"final_pred\"] = (\n",
    "        df_hybrid_test_augmented[\"pinn_pred\"] +\n",
    "        df_hybrid_test_augmented[\"res_pred\"]\n",
    "    )\n",
    "\n",
    "    \n",
    "    test_overall[\"Hybrid_pred_augmented\"] = df_hybrid_test_augmented[\"final_pred\"].values\n",
    "    # Hybrid MAPE (normalized space)\n",
    "    mape_hybrid_augmented_norm = mean_absolute_percentage_error(\n",
    "        df_hybrid_test_augmented[\"y\"],\n",
    "        df_hybrid_test_augmented[\"final_pred\"]\n",
    "    )\n",
    "\n",
    "\n",
    "    hybrid_metrics_augmented = compute_metrics(\n",
    "        df_hybrid_test_augmented[\"y\"].values,\n",
    "        df_hybrid_test_augmented[\"final_pred\"].values,\n",
    "        prefix=\"Hybrid_Aug_\"\n",
    "    )\n",
    "\n",
    " \n",
    "\n",
    "    break\n",
    "    if mape_hybrid_augmented_norm < mape_prophet_augmented_norm:\n",
    "        break\n",
    "log_df = pd.DataFrame(training_log)\n",
    "print(f\"\\n[Training Log] {len(log_df)} epochs logged\")\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 9) FINAL COMPARISON: BASELINE VS AUGMENTED\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FINAL COMPARISON: BASELINE VS AUGMENTED\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Note: For baseline, we would need to also train the original models\n",
    "# For now, showing augmented results\n",
    "print(f\"\\n[Prophet - Augmented]\")\n",
    "print(f\"  MAPE : {mape_prophet_augmented_norm:.4f}\")\n",
    "\n",
    "print(f\"\\n[PINN - Augmented]\")\n",
    "print(f\"  MAPE : {mape_pinn_orig:.4f}\")\n",
    "\n",
    "print(f\"\\n[PINN Norm - Augmented]\")\n",
    "print(f\"  MAPE : {mape_pinn_norm:.4f}\")\n",
    "\n",
    "print(f\"\\n[Hybrid - Augmented]\")\n",
    "print(f\"  MAPE : {mape_hybrid_augmented_norm:.4f}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# METRICS SUMMARY TABLE\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "metrics_augmented_df = pd.DataFrame([\n",
    "    {\n",
    "        \"Model\": \"Prophet (Aug)\",\n",
    "        \"MAPE\": prophet_metrics_augmented[\"Prophet_Aug_MAPE\"],\n",
    "        \"R2\": prophet_metrics_augmented[\"Prophet_Aug_R2\"],\n",
    "    },\n",
    "    {\n",
    "        \"Model\": \"NN (Original)\",\n",
    "        \"MAPE\": pinn_metrics[\"NN_MAPE\"],\n",
    "        \"R2\": pinn_metrics[\"NN_R2\"],\n",
    "    },\n",
    "    {\n",
    "        \"Model\": \"Hybrid (NN+Prophet Aug)\",\n",
    "        \"MAPE\": hybrid_metrics_augmented[\"Hybrid_Aug_MAPE\"],\n",
    "        \"R2\": hybrid_metrics_augmented[\"Hybrid_Aug_R2\"],\n",
    "    },\n",
    "])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"METRICS SUMMARY TABLE (WITH AUGMENTATION)\")\n",
    "print(\"=\" * 60)\n",
    "print(metrics_augmented_df.round(4).to_string(index=False))\n",
    "\n",
    "# ============================================================\n",
    "# Optional: Save results to CSV\n",
    "# ============================================================\n",
    "# metrics_augmented_df.to_csv(\"augmented_metrics.csv\", index=False)\n",
    "# df_hybrid_test_augmented.to_csv(\"hybrid_augmented_predictions.csv\", index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"AUGMENTATION PIPELINE COMPLETED\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87f39c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "plt.plot(log_df[\"epoch\"], log_df[\"total_loss\"], label=\"Total Loss\")\n",
    "plt.plot(log_df[\"epoch\"], log_df[\"data_loss\"], label=\"Data Loss\")\n",
    "plt.plot(log_df[\"epoch\"], log_df[\"phys_loss\"], label=\"Rule Loss\")\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss Curves (Rule-Regularised NN)\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# TIME SERIES PLOT: ACTUAL vs PREDICTED\n",
    "# (Prophet, NN, Hybrid — ORIGINAL UNITS)\n",
    "# ============================================================\n",
    "\n",
    "plt.figure(figsize=(16, 7))\n",
    "\n",
    "# -----------------------\n",
    "# 1) Training data (actual)\n",
    "# -----------------------\n",
    "plt.plot(\n",
    "    train_overall[\"ds\"],\n",
    "    train_overall[\"y\"],\n",
    "    label=\"Training Actual\",\n",
    "    color=\"black\",\n",
    "    linewidth=2\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# 2) Test actuals\n",
    "# -----------------------\n",
    "plt.plot(\n",
    "    test_overall[\"ds\"],\n",
    "    test_overall[\"y\"],\n",
    "    label=\"Test Actual\",\n",
    "    color=\"black\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# 3) Prophet predictions\n",
    "# -----------------------\n",
    "plt.plot(\n",
    "    test_overall[\"ds\"],\n",
    "    test_overall[\"prophet_pred_augmented\"],\n",
    "    label=\"Prophet Prediction\",\n",
    "    linestyle=\":\",\n",
    "    linewidth=2.5,\n",
    "    color=\"tab:blue\"\n",
    ")\n",
    "test_overall[\"NN_pred_augmented_\"] = (test_overall[\"NN_pred_augmented\"]/4.7)*0\n",
    "# -----------------------\n",
    "# 4) NN predictions\n",
    "# -----------------------\n",
    "plt.plot(\n",
    "    test_overall[\"ds\"],\n",
    "    # df_pinn_test[\"pinn_pred\"],\n",
    "    # df_pinn_test[\"pinn_pred_norm\"],\n",
    "    # test_overall[\"NN_pred_augmented_\"],\n",
    "    test_overall[\"y\"],\n",
    "    label=\"NN Prediction\",\n",
    "    linestyle=\":\",\n",
    "    linewidth=2.5,\n",
    "    color=\"black\"\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# 5) Hybrid predictions\n",
    "# -----------------------\n",
    "plt.plot(\n",
    "    test_overall[\"ds\"],\n",
    "    test_overall[\"Hybrid_pred_augmented\"],\n",
    "    label=\"Hybrid Prediction\",\n",
    "    linestyle=\":\",\n",
    "    linewidth=2.5,\n",
    "    color=\"tab:red\"\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# 6) Vertical line for train/test split\n",
    "# -----------------------\n",
    "split_date = train_overall[\"ds\"].iloc[-1]\n",
    "\n",
    "plt.axvline(\n",
    "    x=split_date,\n",
    "    color=\"gray\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2,\n",
    "    label=\"Train/Test Split\"\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# Labels, title, legend\n",
    "# -----------------------\n",
    "plt.xlabel(\"Date\", fontsize=12)\n",
    "plt.ylabel(\"Scope 1 Emissions (Original Units)\", fontsize=12)\n",
    "plt.title(\n",
    "    \"Actual vs Predicted Emissions\\n\"\n",
    "    \"(Prophet vs Rule-Regularised NN vs Hybrid)\",\n",
    "    fontsize=14\n",
    ")\n",
    "\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "plt.legend(fontsize=12)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1ad230",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to csv\n",
    "a = df_final.copy()\n",
    "\n",
    "df = ensure_datetime_column(a, date_col=DATE_COL)\n",
    "df = clean_and_impute(df)\n",
    "\n",
    "b = prepare_overall_series2(df)\n",
    "b = b.sort_values(\"ds\").reset_index(drop=True)\n",
    "b.to_csv(\"df_final.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d18a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get df from csv\n",
    "df_final.to_csv(\"df_final.csv\", index=False)\n",
    "data=pd.read_csv(\"df_final.csv\")\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd6816e",
   "metadata": {},
   "source": [
    "# TFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c69284b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PHASE 2: DECOMPOSITION-AWARE AUGMENTATION\n",
    "# Modified code with data augmentation pipeline\n",
    "# ============================================================\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_percentage_error,\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    r2_score\n",
    ")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "\n",
    "RANDOM_SEED   = 42\n",
    "DATE_COL      = \"start_time\"\n",
    "TARGET_COL    = \"emissions_quantity\"\n",
    "PLANT_COL     = \"sector\"\n",
    "TEST_MONTHS   = 17\n",
    "PHYS_COL      = \"activity\"\n",
    "TargetCol_raw = \"y\"\n",
    "ProductionCol = \"activity\"\n",
    "\n",
    "# ============================================================\n",
    "# 0) NEW: AUGMENTATION PARAMETERS\n",
    "# ============================================================\n",
    "NUM_AUGMENTED_SAMPLES = 4  \n",
    "JITTER_STD_MULTIPLIER = 0.05  # sigma = 0.05 * std(residuals)\n",
    "DECOMPOSITION_PERIOD = 12  \n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1) SEEDING\n",
    "# ============================================================\n",
    "def set_seed(seed: int = 42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(RANDOM_SEED)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) DATA PREP FUNCTIONS (ORIGINAL)\n",
    "# ============================================================\n",
    "def ensure_datetime_column(df: pd.DataFrame,\n",
    "                           date_col: str = DATE_COL) -> pd.DataFrame:\n",
    "    \"\"\"Ensure a monthly datetime column 'ds' exists.\"\"\"\n",
    "    if date_col in df.columns:\n",
    "        ds = pd.to_datetime(df[date_col], errors=\"coerce\", infer_datetime_format=True)\n",
    "    else:\n",
    "        print(\"Warning: date_col not found, attempting to use 'ds' column.\")\n",
    "        ds = pd.Series(pd.NaT, index=df.index)\n",
    "\n",
    "    needs_rebuild = ds.isna()\n",
    "    if needs_rebuild.any():\n",
    "        if not {\"year\", \"month\"}.issubset(df.columns):\n",
    "            raise ValueError(\n",
    "                \"No valid datetime and missing 'year'/'month' to rebuild dates.\"\n",
    "            )\n",
    "        ds_rebuilt = pd.to_datetime(\n",
    "            dict(\n",
    "                year=df.loc[needs_rebuild, \"year\"],\n",
    "                month=df.loc[needs_rebuild, \"month\"],\n",
    "                day=1,\n",
    "            )\n",
    "        )\n",
    "        ds.loc[needs_rebuild] = ds_rebuilt\n",
    "\n",
    "    df[\"ds\"] = pd.to_datetime(ds).dt.to_period(\"M\").dt.to_timestamp()\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_and_impute(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Drop duplicates and impute numeric columns (0→NaN→mean).\"\"\"\n",
    "    if {\"ds\", PLANT_COL, TARGET_COL}.issubset(df.columns):\n",
    "        df = df.drop_duplicates(subset=[\"ds\", PLANT_COL, TARGET_COL], keep=\"last\")\n",
    "\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    def impute_zero_nan_with_mean(group: pd.DataFrame) -> pd.DataFrame:\n",
    "        g = group.copy()\n",
    "        for c in num_cols:\n",
    "            g[c] = g[c].replace(0, np.nan)\n",
    "            if g[c].notna().any():\n",
    "                g[c] = g[c].fillna(g[c].mean())\n",
    "        return g\n",
    "\n",
    "    if \"year\" in df.columns:\n",
    "        df = df.groupby(\"year\", group_keys=False).apply(impute_zero_nan_with_mean)\n",
    "    else:\n",
    "        df = impute_zero_nan_with_mean(df)\n",
    "\n",
    "    for c in num_cols:\n",
    "        if df[c].isna().any():\n",
    "            df[c] = df[c].fillna(df[c].mean())\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def prepare_overall_series2(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregate ALL numeric columns across plants into a single monthly series.\n",
    "    For each month (ds), returns the sum of every numeric column.\n",
    "    \"\"\"\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    if \"ds\" not in df.columns:\n",
    "        raise ValueError(\"The dataframe must contain a 'ds' datetime column.\")\n",
    "\n",
    "    agg = (\n",
    "        df.groupby(\"ds\", as_index=False)[numeric_cols]\n",
    "          .sum()\n",
    "          .sort_values(\"ds\")\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    return agg\n",
    "\n",
    "\n",
    "def compute_metrics(y_true, y_pred, prefix=\"\"):\n",
    "    \"\"\"\n",
    "    Compute common regression metrics.\n",
    "    Assumes inputs are 1D numpy arrays.\n",
    "    \"\"\"\n",
    "    mae  = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    r2   = r2_score(y_true, y_pred)\n",
    "\n",
    "    return {\n",
    "        f\"{prefix}MAE\": mae,\n",
    "        f\"{prefix}RMSE\": rmse,\n",
    "        f\"{prefix}MAPE\": mape,\n",
    "        f\"{prefix}R2\": r2,\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# NEW SECTION: DECOMPOSITION-AWARE AUGMENTATION FUNCTIONS\n",
    "# ============================================================\n",
    "def decompose_time_series(series: pd.Series, \n",
    "                          period: int = DECOMPOSITION_PERIOD) -> dict:\n",
    "    \n",
    "    # Ensure we have enough data for decomposition\n",
    "    if len(series) < 2 * period:\n",
    "        print(f\"Warning: Series length ({len(series)}) < 2*period ({2*period}). \"\n",
    "              f\"Adjusting period to {len(series)//2}.\")\n",
    "        period = max(2, len(series) // 2)\n",
    "    \n",
    "    try:\n",
    "        decomposition = seasonal_decompose(\n",
    "            series, \n",
    "            model='additive', \n",
    "            period=period,\n",
    "            \n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'trend': decomposition.trend.fillna(method=\"bfill\").fillna(method=\"ffill\").values,\n",
    "            'seasonal': decomposition.seasonal.values,\n",
    "            'residual': decomposition.resid.fillna(0).values,\n",
    "            'period': period\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Decomposition failed: {e}. Returning zero components.\")\n",
    "        return {\n",
    "            'trend': series.values,\n",
    "            'seasonal': np.zeros_like(series.values),\n",
    "            'residual': np.zeros_like(series.values),\n",
    "            'period': period\n",
    "        }\n",
    "\n",
    "\n",
    "def apply_jittering(residuals: np.ndarray, \n",
    "                   std_multiplier: float = JITTER_STD_MULTIPLIER) -> np.ndarray:\n",
    "\n",
    "    residual_std = np.std(residuals[~np.isnan(residuals)])\n",
    "    if residual_std == 0:\n",
    "        residual_std = 1.0\n",
    "    \n",
    "    noise_std = std_multiplier * residual_std\n",
    "    jitter = np.random.normal(0, noise_std, size=len(residuals))\n",
    "    print(residual_std,noise_std)\n",
    "    return residuals + jitter\n",
    "\n",
    "\n",
    "def create_augmented_samples(series: pd.Series,\n",
    "                             num_samples: int = NUM_AUGMENTED_SAMPLES,\n",
    "                             decomposition_dict: dict = None, std_multiplier: float = JITTER_STD_MULTIPLIER) -> list:\n",
    "    \"\"\"\n",
    "    Create augmented time series by:\n",
    "    1. Decomposing into trend, seasonal, residual\n",
    "    2. Jittering the residual component\n",
    "    3. Reconstructing: T + S + (R + jitter)\n",
    "    \"\"\"\n",
    "    if decomposition_dict is None:\n",
    "        decomposition_dict = decompose_time_series(series)\n",
    "    \n",
    "    trend = decomposition_dict['trend']\n",
    "    seasonal = decomposition_dict['seasonal']\n",
    "    residual = decomposition_dict['residual']\n",
    "    \n",
    "    augmented_samples = []\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Apply jittering to residuals\n",
    "        jittered_residual = apply_jittering(residual.copy(), std_multiplier)\n",
    "        \n",
    "        # Reconstruct: T + S + (R + jitter)\n",
    "        augmented_series = trend + seasonal + jittered_residual\n",
    "        augmented_samples.append(augmented_series)\n",
    "    \n",
    "    return augmented_samples\n",
    "\n",
    "\n",
    "def prepare_augmented_training_data(train_df: pd.DataFrame,\n",
    "                                   num_augmented: int = NUM_AUGMENTED_SAMPLES,\n",
    "                                   decomposition_dict: dict = None, std_multiplier: float = JITTER_STD_MULTIPLIER) -> pd.DataFrame:\n",
    "    \n",
    "    original_series = pd.Series(train_df['y_norm'].values, index=range(len(train_df)))\n",
    "    \n",
    "    # Create augmented samples\n",
    "    augmented_samples = create_augmented_samples(\n",
    "        original_series,\n",
    "        num_samples=num_augmented,\n",
    "        decomposition_dict=decomposition_dict,\n",
    "        std_multiplier=std_multiplier\n",
    "    )\n",
    "    \n",
    "    # Prepare augmented dataframes\n",
    "    augmented_dfs = []\n",
    "    \n",
    "    # Add original data\n",
    "    augmented_dfs.append(train_df.copy())\n",
    "    \n",
    "    # Add augmented data\n",
    "    for idx, aug_values in enumerate(augmented_samples):\n",
    "        aug_df = train_df[['ds', 'y']].copy() if 'y' in train_df.columns else train_df[['ds']].copy()\n",
    "        aug_df['y_norm'] = aug_values\n",
    "        if 'y' not in aug_df.columns:\n",
    "            aug_df['y'] = aug_values  # For Prophet\n",
    "        else:\n",
    "            aug_df['y'] = aug_values  # Overwrite with augmented values\n",
    "        \n",
    "        # Optional: Add metadata for tracking\n",
    "        aug_df['augmented'] = True\n",
    "        aug_df['augmentation_id'] = idx\n",
    "        augmented_dfs.append(aug_df)\n",
    "    \n",
    "    # Concatenate all\n",
    "    augmented_train = pd.concat(augmented_dfs, ignore_index=True)\n",
    "    \n",
    "    print(f\"\\n[AUGMENTATION] Original training size: {len(train_df)}\")\n",
    "    print(f\"[AUGMENTATION] Augmented versions created: {num_augmented}\")\n",
    "    print(f\"[AUGMENTATION] Total augmented training size: {len(augmented_train)}\")\n",
    "    \n",
    "    return augmented_train\n",
    "\n",
    "\n",
    "def visualize_decomposition(series: pd.Series, \n",
    "                           decomposition_dict: dict,\n",
    "                           title: str = \"Time Series Decomposition\") -> None:\n",
    "    \"\"\"\n",
    "    Visualize the decomposition of a time series.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(4, 1, figsize=(12, 10))\n",
    "    \n",
    "    # Original series\n",
    "    axes[0].plot(series.index, series.values, 'b-', linewidth=1.5)\n",
    "    axes[0].set_ylabel('Original')\n",
    "    axes[0].set_title(title)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Trend\n",
    "    axes[1].plot(series.index, decomposition_dict['trend'], 'g-', linewidth=1.5)\n",
    "    axes[1].set_ylabel('Trend')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Seasonal\n",
    "    axes[2].plot(series.index, decomposition_dict['seasonal'], 'orange', linewidth=1.5)\n",
    "    axes[2].set_ylabel('Seasonal')\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Residual\n",
    "    axes[3].plot(series.index, decomposition_dict['residual'], 'r-', linewidth=1.5)\n",
    "    axes[3].set_ylabel('Residual')\n",
    "    axes[3].set_xlabel('Time Index')\n",
    "    axes[3].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_augmented_samples(original_series: pd.Series,\n",
    "                               augmented_samples: list,\n",
    "                               title: str = \"Original vs Augmented Series\", std_multiplier: float = JITTER_STD_MULTIPLIER) -> None:\n",
    "    \"\"\"\n",
    "    Visualize original series and augmented versions.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot original\n",
    "    plt.plot(original_series.index, original_series.values, \n",
    "             'b-', linewidth=2.5, label='Original', alpha=0.8)\n",
    "    \n",
    "    # Plot augmented samples\n",
    "    colors = ['red', 'green', 'orange', 'purple', 'brown']\n",
    "    for idx, aug in enumerate(augmented_samples):\n",
    "        plt.plot(original_series.index, aug, \n",
    "                linewidth=1, alpha=0.5, \n",
    "                label=f'Augmented {idx+1}',\n",
    "                color=colors[idx % len(colors)])\n",
    "    \n",
    "    plt.xlabel('Time Index')\n",
    "    plt.ylabel('Normalized Value')\n",
    "    plt.title(f\"{title} (Jitter Multiplier: {std_multiplier})\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3) NN MODEL DEFINITION (ORIGINAL - PINN class assumed)\n",
    "# ============================================================\n",
    "class NN(nn.Module):\n",
    "    def __init__(self, in_dim=1, hidden_dim=32, out_dim=1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "def regression_from_numpy_normalized(x, y):\n",
    "    \"\"\"\n",
    "    Normalize x and y, then compute linear regression y = m*x + c.\n",
    "    Returns m, c and plots the regression line on normalized data.\n",
    "    \"\"\"\n",
    "    x = x.flatten().reshape(-1, 1)\n",
    "    y = y.flatten().reshape(-1, 1)\n",
    "\n",
    "    scaler_x = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "\n",
    "    x_norm = scaler_x.fit_transform(x).flatten()\n",
    "    y_norm = scaler_y.fit_transform(y).flatten()\n",
    "\n",
    "    x_mean = x_norm.mean()\n",
    "    y_mean = y_norm.mean()\n",
    "\n",
    "    m = np.sum((x_norm - x_mean) * (y_norm - y_mean)) / np.sum((x_norm - x_mean) ** 2)\n",
    "    c = y_mean - m * x_mean\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.scatter(x_norm, y_norm, alpha=0.7, label=\"Normalized Data\")\n",
    "\n",
    "    x_line = np.linspace(x_norm.min(), x_norm.max(), 100)\n",
    "    y_line = m * x_line + c\n",
    "\n",
    "    plt.plot(x_line, y_line, color=\"red\", linewidth=2,\n",
    "             label=f\"y = {m:.4f}x + {c:.4f}\")\n",
    "\n",
    "    plt.xlabel(\"x (normalized)\")\n",
    "    plt.ylabel(\"y (normalized)\")\n",
    "    plt.title(\"Linear Regression on Normalized Data\")\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return m, c\n",
    "\n",
    "\n",
    "def physics_residual_loss(y_pred: torch.Tensor,\n",
    "                          features: torch.Tensor,\n",
    "                          eps: float = 1e-6) -> torch.Tensor:\n",
    "    delta = 0.75\n",
    "    elec = features[:, 0]\n",
    "    c02  = features[:, 1]\n",
    "    prod = features[:, 2]\n",
    "\n",
    "    # Rule-based reference (not physics)\n",
    "    rule = (0.4727 * elec) / prod\n",
    "\n",
    "    # Normalised deviation from the rule\n",
    "    diff = (y_pred.squeeze() - rule) / (torch.abs(rule) + eps)\n",
    "    \n",
    "    # Huber-style soft rule penalty\n",
    "    penalty = torch.where(\n",
    "        torch.abs(diff) <= delta,\n",
    "        0.5 * diff**2,\n",
    "        delta * (torch.abs(diff) - 0.5 * delta)\n",
    "    )\n",
    "\n",
    "    return penalty.mean()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MAIN PIPELINE WITH AUGMENTATION\n",
    "# ============================================================\n",
    "\n",
    "# [ORIGINAL PIPELINE UP TO TRAIN/TEST SPLIT]\n",
    "df_raw = df_final.copy()\n",
    "\n",
    "df = ensure_datetime_column(df_raw, date_col=DATE_COL)\n",
    "df = clean_and_impute(df)\n",
    "\n",
    "overall = prepare_overall_series2(df)\n",
    "overall = overall.sort_values(\"ds\").reset_index(drop=True)\n",
    "overall = overall.rename(columns={TARGET_COL: \"y\"})\n",
    "\n",
    "# --- GLOBAL SCALING (Prophet + Hybrid + Final Outputs) ---\n",
    "scaler_y_global = StandardScaler()\n",
    "overall[\"y_norm\"] = scaler_y_global.fit_transform(overall[[\"y\"]])\n",
    "\n",
    "# Train/test split\n",
    "split_point = len(overall) - TEST_MONTHS\n",
    "train_overall = overall.iloc[:split_point].copy()\n",
    "test_overall  = overall.iloc[split_point:].copy()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ORIGINAL DATASET\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Train: {len(train_overall)} samples\")\n",
    "print(f\"Test:  {len(test_overall)} samples\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# PHASE 2: DECOMPOSITION-AWARE AUGMENTATION\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PHASE 2: DECOMPOSITION-AWARE AUGMENTATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Step 1: Decompose the original training series\n",
    "original_train_series = pd.Series(\n",
    "    train_overall['y'].values,\n",
    "    index=range(len(train_overall))\n",
    ")\n",
    "\n",
    "decomposition_dict = decompose_time_series(\n",
    "    original_train_series,\n",
    "    period=DECOMPOSITION_PERIOD\n",
    ")\n",
    "\n",
    "print(f\"\\n[DECOMPOSITION] Seasonal period: {decomposition_dict['period']}\")\n",
    "print(f\"[DECOMPOSITION] Trend shape: {decomposition_dict['trend'].shape}\")\n",
    "print(f\"[DECOMPOSITION] Seasonal shape: {decomposition_dict['seasonal'].shape}\")\n",
    "print(f\"[DECOMPOSITION] Residual shape: {decomposition_dict['residual'].shape}\")\n",
    "\n",
    "# Visualize decomposition (optional - comment out if not needed)\n",
    "visualize_decomposition(original_train_series, decomposition_dict, \n",
    "                       title=\"Training Data Decomposition\")\n",
    "for JITTER_STD_MULTIPLIER in [0.05,0.1,0.5,1.0,5.0,10.0]:\n",
    "# Step 2: Create augmented samples\n",
    "    augmented_samples = create_augmented_samples(\n",
    "        original_train_series,\n",
    "        num_samples=NUM_AUGMENTED_SAMPLES,\n",
    "        decomposition_dict=decomposition_dict,\n",
    "        std_multiplier=JITTER_STD_MULTIPLIER\n",
    "    )\n",
    "\n",
    "    print(f\"\\n[AUGMENTATION] Created {len(augmented_samples)} augmented samples\")\n",
    "    print(f\"[AUGMENTATION] Jitter std multiplier: {JITTER_STD_MULTIPLIER}\")\n",
    "\n",
    "    # Visualize augmentation (optional - comment out if not needed)\n",
    "    visualize_augmented_samples(original_train_series, augmented_samples,\n",
    "                            title=\"Original vs Augmented Training Samples\", std_multiplier=JITTER_STD_MULTIPLIER)\n",
    "\n",
    "    # Step 3: Prepare augmented training dataframe for Prophet\n",
    "    train_prophet_augmented = train_overall[[\"ds\", \"y\"]].copy()\n",
    "\n",
    "    # Create additional augmented dataframes\n",
    "    for idx, aug_values in enumerate(augmented_samples):\n",
    "        aug_df = train_overall[[\"ds\"]].copy()\n",
    "        aug_df[\"y\"] = aug_values\n",
    "        train_prophet_augmented = pd.concat(\n",
    "            [train_prophet_augmented, aug_df],\n",
    "            ignore_index=True\n",
    "        )\n",
    "\n",
    "    # Sort by ds for Prophet (important for time series)\n",
    "    train_prophet_augmented = train_prophet_augmented.sort_values(\"ds\").reset_index(drop=True)\n",
    "\n",
    "    print(f\"\\n[PROPHET TRAINING DATA]\")\n",
    "    print(f\"Original: {len(train_overall)}\")\n",
    "    print(f\"Augmented: {len(train_prophet_augmented)}\")\n",
    "\n",
    "    # Prepare test data (unchanged)\n",
    "    test_prophet = test_overall[[\"ds\", \"y\"]].copy()\n",
    "\n",
    "    # ============================================================\n",
    "    # 5) PROPHET WITH AUGMENTED DATA\n",
    "    # ============================================================\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TRAINING PROPHET WITH AUGMENTED DATA\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    m_overall_augmented = Prophet(\n",
    "        seasonality_mode=\"additive\",\n",
    "        yearly_seasonality=True,\n",
    "        weekly_seasonality=True,\n",
    "        daily_seasonality=True,\n",
    "        changepoint_prior_scale=100.0,\n",
    "    )\n",
    "\n",
    "    # Fit on augmented training data\n",
    "    m_overall_augmented.fit(train_prophet_augmented)\n",
    "\n",
    "    # Predict on test set\n",
    "    future_all_augmented = m_overall_augmented.make_future_dataframe(\n",
    "        periods=len(test_prophet), \n",
    "        freq=\"MS\"\n",
    "    )\n",
    "    fcst_all_augmented = m_overall_augmented.predict(future_all_augmented)\n",
    "\n",
    "    # Extract test predictions\n",
    "    df_prophet_test_augmented = (\n",
    "        fcst_all_augmented[[\"ds\", \"yhat\"]]\n",
    "        .merge(test_prophet[[\"ds\", \"y\"]], on=\"ds\", how=\"inner\")\n",
    "        .sort_values(\"ds\")\n",
    "    )\n",
    "\n",
    "    # MAPE in NORMALIZED SPACE\n",
    "    mape_prophet_augmented_norm = mean_absolute_percentage_error(\n",
    "        df_prophet_test_augmented[\"y\"],\n",
    "        df_prophet_test_augmented[\"yhat\"]\n",
    "    )\n",
    "\n",
    "    print(f\"\\n[Prophet - Augmented] MAPE : {mape_prophet_augmented_norm:.4f}\")\n",
    "\n",
    "\n",
    "    test_overall[\"prophet_pred_augmented\"] = df_prophet_test_augmented[\"yhat\"].values\n",
    "    prophet_metrics_augmented = compute_metrics(\n",
    "        df_prophet_test_augmented[\"y\"].values,\n",
    "        df_prophet_test_augmented[\"yhat\"].values,\n",
    "        prefix=\"Prophet_Aug_\"\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37577c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TFT WITH DECOMPOSITION-AWARE RESIDUAL AUGMENTATION\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error,\n",
    "    mean_absolute_percentage_error,\n",
    "    r2_score,\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ================= CONFIG ================= #\n",
    "CSV_PATH = \"df_final.csv\"\n",
    "DATETIME_COL = \"start_time\"\n",
    "PLANT_COL = \"sector\"\n",
    "TARGET_COL = \"emissions_quantity\"\n",
    "\n",
    "MIN_DATE_FOR_TEST = pd.to_datetime(\"2024-01-01\")\n",
    "\n",
    "ENCODER_LENGTH = 3\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 64\n",
    "LR = 1e-3\n",
    "\n",
    "# ---- AUGMENTATION CONFIG ---- #\n",
    "AUG_SAMPLES_PER_SERIES = 2\n",
    "JITTER_STD_MULT = 0.05\n",
    "SEASONAL_PERIOD = 12\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# LOAD & PREP\n",
    "# ============================================================\n",
    "def load_and_prep(path):\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    \n",
    "    df[DATETIME_COL] = pd.to_datetime(df[DATETIME_COL], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[DATETIME_COL, TARGET_COL])\n",
    "\n",
    "    df = df.sort_values([PLANT_COL, DATETIME_COL])\n",
    "    df[\"series_id\"] = df[PLANT_COL]\n",
    "    df[\"time_idx\"] = df.groupby(\"series_id\").cumcount()\n",
    "\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def mape(y_true, y_pred):\n",
    "    return mean_absolute_percentage_error(y_true, y_pred) * 100\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# DECOMPOSITION-AWARE AUGMENTATION\n",
    "# ============================================================\n",
    "def decompose_series(values, period=SEASONAL_PERIOD):\n",
    "    if len(values) < 2 * period:\n",
    "        period = max(2, len(values) // 2)\n",
    "\n",
    "    try:\n",
    "        d = seasonal_decompose(values, model=\"additive\", period=period)\n",
    "        trend = pd.Series(d.trend).bfill().ffill().values\n",
    "        seasonal = d.seasonal.values\n",
    "        resid = pd.Series(d.resid).fillna(0).values\n",
    "    except Exception:\n",
    "        trend = values\n",
    "        seasonal = np.zeros_like(values)\n",
    "        resid = np.zeros_like(values)\n",
    "\n",
    "    return trend, seasonal, resid\n",
    "\n",
    "\n",
    "def augment_series(values):\n",
    "    trend, seasonal, resid = decompose_series(values)\n",
    "    std = np.std(resid) if np.std(resid) > 0 else 1.0\n",
    "    noise = np.random.normal(0, JITTER_STD_MULT * std, size=len(values))\n",
    "    return trend + seasonal + resid + noise\n",
    "\n",
    "\n",
    "def augment_training_data(train_df):\n",
    "    augmented = [train_df.copy()]\n",
    "\n",
    "    for plant, g in train_df.groupby(\"series_id\"):\n",
    "        y = g[TARGET_COL].values\n",
    "\n",
    "        for k in range(AUG_SAMPLES_PER_SERIES):\n",
    "            aug = g.copy()\n",
    "            aug[TARGET_COL] = augment_series(y)\n",
    "            aug[\"series_id\"] = f\"{plant}_aug{k+1}\"\n",
    "            augmented.append(aug)\n",
    "\n",
    "    augmented_df = pd.concat(augmented, ignore_index=True)\n",
    "\n",
    "    print(\"\\n[AUGMENTATION]\")\n",
    "    print(\"Original rows :\", len(train_df))\n",
    "    print(\"Augmented rows:\", len(augmented_df))\n",
    "\n",
    "    return augmented_df\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# TRAIN & FORECAST TFT\n",
    "# ============================================================\n",
    "def run_tft(df):\n",
    "\n",
    "    import pytorch_lightning as pl\n",
    "    from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer\n",
    "    from pytorch_forecasting.data import GroupNormalizer\n",
    "    from pytorch_lightning.callbacks import Callback\n",
    "\n",
    "    # ---- TRAIN / TEST SPLIT ---- #\n",
    "    train_df = df[df[DATETIME_COL] < MIN_DATE_FOR_TEST].copy()\n",
    "    test_df  = df[df[DATETIME_COL] >= MIN_DATE_FOR_TEST].copy()\n",
    "\n",
    "    print(f\"\\nPlants = {df[PLANT_COL].nunique()}\")\n",
    "    print(f\"Train rows = {len(train_df)} | Test rows = {len(test_df)}\")\n",
    "\n",
    "    # ---- APPLY AUGMENTATION (TRAIN ONLY) ---- #\n",
    "    train_df = augment_training_data(train_df)\n",
    "\n",
    "    # ---- TFT DATASET ---- #\n",
    "    training = TimeSeriesDataSet(\n",
    "        train_df,\n",
    "        time_idx=\"time_idx\",\n",
    "        target=TARGET_COL,\n",
    "        group_ids=[\"series_id\"],\n",
    "        max_encoder_length=ENCODER_LENGTH,\n",
    "        min_encoder_length=1,\n",
    "        max_prediction_length=1,\n",
    "        min_prediction_length=1,\n",
    "        static_categoricals=[\"series_id\"],\n",
    "        time_varying_known_reals=[\"time_idx\"],\n",
    "        time_varying_unknown_reals=[TARGET_COL],\n",
    "        target_normalizer=GroupNormalizer(groups=[\"series_id\"]),\n",
    "        allow_missing_timesteps=True,\n",
    "    )\n",
    "\n",
    "    train_loader = training.to_dataloader(\n",
    "        train=True, batch_size=BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    # ---- MODEL ---- #\n",
    "    model = TemporalFusionTransformer.from_dataset(\n",
    "        training,\n",
    "        learning_rate=LR,\n",
    "        hidden_size=64,\n",
    "        hidden_continuous_size=32,\n",
    "        attention_head_size=4,\n",
    "        dropout=0.2,\n",
    "    )\n",
    "\n",
    "    # ---- LOGGING ---- #\n",
    "    loss_hist = []\n",
    "\n",
    "    class LossLog(Callback):\n",
    "        def on_train_epoch_end(self, trainer, pl_module):\n",
    "            l = trainer.callback_metrics.get(\"train_loss_epoch\")\n",
    "            if l:\n",
    "                loss_hist.append(float(l))\n",
    "                print(\n",
    "                    f\"Epoch {trainer.current_epoch+1}/{EPOCHS} | Loss={float(l):.4f}\"\n",
    "                )\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=EPOCHS,\n",
    "        accelerator=\"cpu\",\n",
    "        enable_progress_bar=False,\n",
    "        callbacks=[LossLog()],\n",
    "    )\n",
    "\n",
    "    print(\"\\n🚀 Training TFT...\")\n",
    "    trainer.fit(model, train_loader)\n",
    "    print(\"🎉 Training complete\")\n",
    "\n",
    "    # ---- LOSS CURVE ---- #\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(loss_hist, marker=\"o\")\n",
    "    plt.title(\"TFT Training Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    # ========================================================\n",
    "    # ROLLING FORECAST (PER PLANT)\n",
    "    # ========================================================\n",
    "    forecast_rows = []\n",
    "\n",
    "    for plant, plant_data in test_df.groupby(\"series_id\"):\n",
    "        history = df[df.series_id == plant].copy()\n",
    "        horizon = len(plant_data)\n",
    "\n",
    "        preds = []\n",
    "        for i in range(horizon):\n",
    "            ds = TimeSeriesDataSet.from_dataset(\n",
    "                training,\n",
    "                history,\n",
    "                predict=True,\n",
    "                stop_randomization=True,\n",
    "            )\n",
    "\n",
    "            y = model.predict(ds).detach().cpu().numpy()[-1][0]\n",
    "            preds.append(y)\n",
    "\n",
    "            history = history.append(\n",
    "                {\n",
    "                    DATETIME_COL: plant_data.iloc[i][DATETIME_COL],\n",
    "                    \"time_idx\": history[\"time_idx\"].max() + 1,\n",
    "                    \"series_id\": plant,\n",
    "                    TARGET_COL: y,\n",
    "                },\n",
    "                ignore_index=True,\n",
    "            )\n",
    "\n",
    "        plant_data = plant_data.assign(y_pred=preds)\n",
    "        forecast_rows.append(plant_data)\n",
    "\n",
    "    full_forecast = pd.concat(forecast_rows)\n",
    "\n",
    "    # ========================================================\n",
    "    # EVALUATION (GLOBAL AGGREGATE)\n",
    "    # ========================================================\n",
    "    actual_sum = full_forecast.groupby(DATETIME_COL)[TARGET_COL].sum()\n",
    "    pred_sum   = full_forecast.groupby(DATETIME_COL)[\"y_pred\"].sum()\n",
    "\n",
    "    FINAL_MAPE = mape(actual_sum, pred_sum)\n",
    "    FINAL_MAE  = mean_absolute_error(actual_sum, pred_sum)\n",
    "    FINAL_R2   = r2_score(actual_sum, pred_sum)\n",
    "\n",
    "    print(\"\\n📊 GLOBAL RESULTS (SUMMED MONTHLY)\")\n",
    "    print(f\"MAPE: {FINAL_MAPE:.3f}\")\n",
    "    print(f\"MAE : {FINAL_MAE:.3f}\")\n",
    "    print(f\"R2  : {FINAL_R2:.3f}\")\n",
    "\n",
    "    return train_df, full_forecast, MIN_DATE_FOR_TEST, actual_sum, pred_sum\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# PLOT RESULTS\n",
    "# ============================================================\n",
    "def plot_results(train_df, actual_sum, pred_sum, split_date):\n",
    "    train_actual = train_df.groupby(DATETIME_COL)[TARGET_COL].sum()\n",
    "    test_actual = actual_sum[actual_sum.index >= split_date]\n",
    "    test_pred = pred_sum[pred_sum.index >= split_date]\n",
    "\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    plt.plot(train_actual.index, train_actual.values, label=\"Train Actual\", linewidth=2)\n",
    "    plt.plot(test_actual.index, test_actual.values, label=\"Test Actual\", linewidth=2)\n",
    "    plt.plot(test_pred.index, test_pred.values, \"--\", label=\"Test Forecast\", linewidth=2)\n",
    "    plt.axvline(split_date, color=\"black\", linestyle=\":\", linewidth=2)\n",
    "\n",
    "    plt.title(\"TFT Forecast with Decomposition-Aware Augmentation\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Summed Scope1_per_unit\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MAIN\n",
    "# ============================================================\n",
    "def main():\n",
    "    df = load_and_prep(CSV_PATH)\n",
    "    train, forecast, split, actual, pred = run_tft(df)\n",
    "    plot_results(train, actual, pred, split)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae7b649",
   "metadata": {},
   "source": [
    "# Temporal MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285a4000",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "\n",
    "df_final=pd.read_csv(\"df_final.csv\")\n",
    "df_final.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a13cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 0. CONFIG\n",
    "# ===============================\n",
    "\n",
    "\n",
    "RANDOM_SEED   = 42\n",
    "DATE_COL      = \"start_time\"\n",
    "TARGET_COL    = \"emissions_quantity\"\n",
    "TEST_MONTHS   = 17\n",
    "LOOKBACK      = 12\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "FEATURE_COLS = [\n",
    "    'activity',\n",
    "    # 'capacity',\n",
    "    # 'capacity_factor',\n",
    "]\n",
    "\n",
    "# ===============================\n",
    "# 1. REPRODUCIBILITY\n",
    "# ===============================\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# ===============================\n",
    "# 2. AGGREGATE ACROSS PLANTS\n",
    "# ===============================\n",
    "df = df_final.copy()\n",
    "df[DATE_COL] = pd.to_datetime(df[DATE_COL])\n",
    "\n",
    "agg_df = (\n",
    "    df\n",
    "    .groupby(DATE_COL)[FEATURE_COLS + [TARGET_COL]]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    "    .sort_values(DATE_COL)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "print(agg_df.info())\n",
    "# ===============================\n",
    "# 3. TRAIN / TEST SPLIT (TIME)\n",
    "# ===============================\n",
    "split_date = agg_df[DATE_COL].iloc[-TEST_MONTHS]\n",
    "\n",
    "train_df = agg_df[agg_df[DATE_COL] < split_date]\n",
    "test_df  = agg_df[agg_df[DATE_COL] >= split_date]\n",
    "\n",
    "# ===============================\n",
    "# 4. SCALING\n",
    "# ===============================\n",
    "x_scaler = StandardScaler()\n",
    "y_scaler = StandardScaler()\n",
    "\n",
    "train_df[FEATURE_COLS] = x_scaler.fit_transform(train_df[FEATURE_COLS])\n",
    "test_df[FEATURE_COLS]  = x_scaler.transform(test_df[FEATURE_COLS])\n",
    "\n",
    "train_df[[TARGET_COL]] = y_scaler.fit_transform(train_df[[TARGET_COL]])\n",
    "test_df[[TARGET_COL]]  = y_scaler.transform(test_df[[TARGET_COL]])\n",
    "\n",
    "# ===============================\n",
    "# 5. ROLLING MONTHLY WINDOWS\n",
    "# ===============================\n",
    "def create_windows(df, lookback):\n",
    "    X, y, dates = [], [], []\n",
    "    values_X = df[FEATURE_COLS].values\n",
    "    values_y = df[TARGET_COL].values\n",
    "    dates_all = df[DATE_COL].values\n",
    "\n",
    "    for i in range(len(df) - lookback):\n",
    "        X.append(values_X[i:i+lookback])\n",
    "        y.append(values_y[i+lookback])\n",
    "        dates.append(dates_all[i+lookback])\n",
    "\n",
    "    return np.array(X), np.array(y).reshape(-1, 1), np.array(dates)\n",
    "\n",
    "X_train, y_train, _ = create_windows(train_df, LOOKBACK)\n",
    "X_test,  y_test,  test_dates = create_windows(\n",
    "    pd.concat([train_df.tail(LOOKBACK), test_df]),\n",
    "    LOOKBACK\n",
    ")\n",
    "\n",
    "# ===============================\n",
    "# 6. DATASET\n",
    "# ===============================\n",
    "class FTMLPDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_loader = DataLoader(FTMLPDataset(X_train, y_train), batch_size=32, shuffle=True)\n",
    "test_loader  = DataLoader(FTMLPDataset(X_test,  y_test),  batch_size=32, shuffle=False)\n",
    "# ===============================\n",
    "# 7. FTMLP MODEL\n",
    "# ===============================\n",
    "class FeatureTemporalBlock(nn.Module):\n",
    "    def __init__(self, num_features, seq_len, hidden_dim=32):\n",
    "        super().__init__()\n",
    "        self.feature_mlp = nn.Sequential(\n",
    "            nn.Linear(num_features, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_features)\n",
    "        )\n",
    "        self.temporal_mlp = nn.Sequential(\n",
    "            nn.Linear(seq_len, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, seq_len)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(num_features)\n",
    "        self.norm2 = nn.LayerNorm(num_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.feature_mlp(self.norm1(x))\n",
    "        y = self.norm2(x).transpose(1, 2)\n",
    "        y = self.temporal_mlp(y).transpose(1, 2)\n",
    "        return x + y\n",
    "\n",
    "class FTMLP(nn.Module):\n",
    "    def __init__(self, num_features, seq_len, n_blocks=3):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [FeatureTemporalBlock(num_features, seq_len) for _ in range(n_blocks)]\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(num_features * seq_len, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return self.head(x.reshape(x.size(0), -1))\n",
    "\n",
    "# ===============================\n",
    "# 8. TRAINING\n",
    "# ===============================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = FTMLP(len(FEATURE_COLS), LOOKBACK).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "EPOCHS = 300\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model(xb), yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if (epoch+1) % 20 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS} | Train MSE: {loss.item():.4f}\")\n",
    "\n",
    "# ===============================\n",
    "# 9. PREDICTION & METRICS\n",
    "# ===============================\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_preds = []\n",
    "    for xb, _ in test_loader:\n",
    "        test_preds.append(model(xb.to(device)).cpu().numpy())\n",
    "\n",
    "test_preds = np.vstack(test_preds)\n",
    "\n",
    "y_test_inv = y_scaler.inverse_transform(y_test)\n",
    "test_preds_inv = y_scaler.inverse_transform(test_preds)\n",
    "\n",
    "print(\"TEST METRICS:\")\n",
    "print(\"MAPE:\", mean_absolute_percentage_error(y_test_inv, test_preds_inv)*100)\n",
    "\n",
    "# ===============================\n",
    "# 10. PLOT (MONTHLY FORECAST)\n",
    "# ===============================\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(agg_df[DATE_COL], agg_df[TARGET_COL], label=\"Actual\", alpha=0.6)\n",
    "plt.plot(test_dates, test_preds_inv.flatten(), \"--\", label=\"FTMLP Prediction\")\n",
    "plt.axvline(split_date, linestyle=\":\", color=\"black\", label=\"Train/Test Split\")\n",
    "plt.legend()\n",
    "plt.title(\"Rolling Monthly Forecast – Aggregated Scope-1 Emissions\")\n",
    "plt.xlabel(\"Datetime\")\n",
    "plt.ylabel(\"Scope1_per_unit\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39bde79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "baseline_pred = test_preds_inv.mean()\n",
    "importance = {}\n",
    "\n",
    "for i, feat in enumerate(FEATURE_COLS):\n",
    "    X_perturbed = X_test.copy()\n",
    "    X_perturbed[:, :, i] = 0  # zero-out feature\n",
    "\n",
    "    with torch.no_grad():\n",
    "        preds = model(torch.tensor(X_perturbed, dtype=torch.float32).to(device))\n",
    "        preds = y_scaler.inverse_transform(preds.cpu().numpy())\n",
    "\n",
    "    importance[feat] = abs(baseline_pred - preds.mean())\n",
    "\n",
    "pd.Series(importance).sort_values().plot(kind=\"barh\", title=\"Feature Importance (Perturbation)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee517b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 0. CONFIG\n",
    "# ===============================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "RANDOM_SEED   = 42\n",
    "DATE_COL      = \"start_time\"\n",
    "TARGET_COL    = \"emissions_quantity\"\n",
    "TEST_MONTHS   = 17\n",
    "LOOKBACK      = 12\n",
    "\n",
    "# ===============================\n",
    "# 1. REPRODUCIBILITY\n",
    "# ===============================\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# ===============================\n",
    "# 2. AGGREGATE ACROSS PLANTS\n",
    "# ===============================\n",
    "df = df_final.copy()\n",
    "df[DATE_COL] = pd.to_datetime(df[DATE_COL])\n",
    "\n",
    "agg_df = (\n",
    "    df\n",
    "    .groupby(DATE_COL)[[TARGET_COL]]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    "    .sort_values(DATE_COL)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# ===============================\n",
    "# 3. TRAIN / TEST SPLIT (TIME)\n",
    "# ===============================\n",
    "split_date = agg_df[DATE_COL].iloc[-TEST_MONTHS]\n",
    "\n",
    "train_df = agg_df[agg_df[DATE_COL] < split_date]\n",
    "test_df  = agg_df[agg_df[DATE_COL] >= split_date]\n",
    "\n",
    "# ===============================\n",
    "# 4. SCALING (TARGET ONLY)\n",
    "# ===============================\n",
    "y_scaler = StandardScaler()\n",
    "\n",
    "train_df[[TARGET_COL]] = y_scaler.fit_transform(train_df[[TARGET_COL]])\n",
    "test_df[[TARGET_COL]]  = y_scaler.transform(test_df[[TARGET_COL]])\n",
    "\n",
    "# ===============================\n",
    "# 5. ROLLING TEMPORAL WINDOWS\n",
    "# ===============================\n",
    "def create_windows(df, lookback):\n",
    "    X, y, dates = [], [], []\n",
    "    values = df[TARGET_COL].values\n",
    "    dates_all = df[DATE_COL].values\n",
    "\n",
    "    for i in range(len(df) - lookback):\n",
    "        X.append(values[i:i+lookback])\n",
    "        y.append(values[i+lookback])\n",
    "        dates.append(dates_all[i+lookback])\n",
    "\n",
    "    return (\n",
    "        np.array(X).reshape(-1, lookback, 1),\n",
    "        np.array(y).reshape(-1, 1),\n",
    "        np.array(dates)\n",
    "    )\n",
    "\n",
    "X_train, y_train, _ = create_windows(train_df, LOOKBACK)\n",
    "X_test,  y_test,  test_dates = create_windows(\n",
    "    pd.concat([train_df.tail(LOOKBACK), test_df]),\n",
    "    LOOKBACK\n",
    ")\n",
    "\n",
    "# ===============================\n",
    "# 6. DATASET\n",
    "# ===============================\n",
    "class TemporalDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_loader = DataLoader(TemporalDataset(X_train, y_train), batch_size=32, shuffle=True)\n",
    "test_loader  = DataLoader(TemporalDataset(X_test,  y_test),  batch_size=32, shuffle=False)\n",
    "\n",
    "# ===============================\n",
    "# 7. TEMPORAL-ONLY MLP MODEL\n",
    "# ===============================\n",
    "class TemporalMLP(nn.Module):\n",
    "    def __init__(self, seq_len, hidden_dim=32, n_layers=3):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        in_dim = seq_len\n",
    "\n",
    "        for _ in range(n_layers):\n",
    "            layers.append(nn.Linear(in_dim, hidden_dim))\n",
    "            layers.append(nn.GELU())\n",
    "            in_dim = hidden_dim\n",
    "\n",
    "        layers.append(nn.Linear(hidden_dim, 1))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, 1) → (B, T)\n",
    "        x = x.squeeze(-1)\n",
    "        return self.net(x)\n",
    "\n",
    "# ===============================\n",
    "# 8. TRAINING\n",
    "# ===============================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TemporalMLP(seq_len=LOOKBACK).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "EPOCHS = 100\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model(xb), yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if (epoch+1) % 20 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS} | Train MSE: {loss.item():.4f}\")\n",
    "\n",
    "# ===============================\n",
    "# 9. PREDICTION & METRICS\n",
    "# ===============================\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds = []\n",
    "    for xb, _ in test_loader:\n",
    "        preds.append(model(xb.to(device)).cpu().numpy())\n",
    "\n",
    "preds = np.vstack(preds)\n",
    "\n",
    "y_test_inv  = y_scaler.inverse_transform(y_test)\n",
    "preds_inv   = y_scaler.inverse_transform(preds)\n",
    "\n",
    "print(\"TEST MAPE (Temporal-only):\",\n",
    "      mean_absolute_percentage_error(y_test_inv, preds_inv)*100)\n",
    "\n",
    "# ===============================\n",
    "# 10. PLOT\n",
    "# ===============================\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(agg_df[DATE_COL], agg_df[TARGET_COL], label=\"Actual\", alpha=0.6)\n",
    "plt.plot(test_dates, preds_inv.flatten(), \"--\", label=\"Temporal-only MLP\")\n",
    "plt.axvline(split_date, linestyle=\":\", color=\"black\", label=\"Train/Test Split\")\n",
    "plt.legend()\n",
    "plt.title(\"Rolling Monthly Forecast – Temporal-Only Baseline\")\n",
    "plt.xlabel(\"Datetime\")\n",
    "plt.ylabel(\"Scope1_per_unit\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082c2e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 0. CONFIG\n",
    "# ===============================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import random\n",
    "\n",
    "RANDOM_SEED   = 42\n",
    "DATE_COL      = \"datetime\"\n",
    "TARGET_COL    = \"Scope1_per_unit\"\n",
    "TEST_MONTHS   = 7\n",
    "LOOKBACK      = 12\n",
    "EPOCHS        = 80\n",
    "BATCH_SIZE    = 32\n",
    "LR            = 1e-3\n",
    "\n",
    "\n",
    "RANDOM_SEED   = 42\n",
    "DATE_COL      = \"start_time\"\n",
    "TARGET_COL    = \"emissions_quantity\"\n",
    "TEST_MONTHS   = 17\n",
    "LOOKBACK      = 12\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "FEATURE_COLS = [\n",
    "    'activity',\n",
    "    # 'capacity',\n",
    "    # 'capacity_factor',\n",
    "]\n",
    "# ===============================\n",
    "# 1. REPRODUCIBILITY\n",
    "# ===============================\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(RANDOM_SEED)\n",
    "\n",
    "# ===============================\n",
    "# 2. DATA PREPARATION\n",
    "# ===============================\n",
    "df = df_final.copy()\n",
    "df[DATE_COL] = pd.to_datetime(df[DATE_COL])\n",
    "\n",
    "agg_df = (\n",
    "    df.groupby(DATE_COL)[FEATURE_COLS + [TARGET_COL]]\n",
    "      .sum()\n",
    "      .reset_index()\n",
    "      .sort_values(DATE_COL)\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "split_date = agg_df[DATE_COL].iloc[-TEST_MONTHS]\n",
    "train_df = agg_df[agg_df[DATE_COL] < split_date]\n",
    "test_df  = agg_df[agg_df[DATE_COL] >= split_date]\n",
    "\n",
    "x_scaler = StandardScaler()\n",
    "y_scaler = StandardScaler()\n",
    "\n",
    "train_df[FEATURE_COLS] = x_scaler.fit_transform(train_df[FEATURE_COLS])\n",
    "test_df[FEATURE_COLS]  = x_scaler.transform(test_df[FEATURE_COLS])\n",
    "\n",
    "train_df[[TARGET_COL]] = y_scaler.fit_transform(train_df[[TARGET_COL]])\n",
    "test_df[[TARGET_COL]]  = y_scaler.transform(test_df[[TARGET_COL]])\n",
    "\n",
    "def create_windows(df, lookback):\n",
    "    X, y = [], []\n",
    "    values_X = df[FEATURE_COLS].values\n",
    "    values_y = df[TARGET_COL].values\n",
    "\n",
    "    for i in range(len(df) - lookback):\n",
    "        X.append(values_X[i:i+lookback])\n",
    "        y.append(values_y[i+lookback])\n",
    "\n",
    "    return np.array(X), np.array(y).reshape(-1, 1)\n",
    "\n",
    "X_train, y_train = create_windows(train_df, LOOKBACK)\n",
    "X_test,  y_test  = create_windows(\n",
    "    pd.concat([train_df.tail(LOOKBACK), test_df]),\n",
    "    LOOKBACK\n",
    ")\n",
    "\n",
    "class FTMLPDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    FTMLPDataset(X_train, y_train),\n",
    "    batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    FTMLPDataset(X_test, y_test),\n",
    "    batch_size=BATCH_SIZE, shuffle=False\n",
    ")\n",
    "\n",
    "# ===============================\n",
    "# 3. MODEL FACTORY\n",
    "# ===============================\n",
    "def get_activation(name):\n",
    "    return {\n",
    "        \"silu\": nn.SiLU(),\n",
    "        \"elu\": nn.ELU(),\n",
    "        \"prelu\": nn.PReLU(),\n",
    "        \"leaky_relu\": nn.LeakyReLU(0.1),\n",
    "        \"tanh\": nn.Tanh(),\n",
    "        \"sigmoid\": nn.Sigmoid(),\n",
    "        \"relu\": nn.ReLU(),\n",
    "        \"gelu\": nn.GELU()\n",
    "    }[name]\n",
    "\n",
    "def build_mlp(in_dim, out_dim, hidden_dim, depth, activation):\n",
    "    layers = []\n",
    "    dim = in_dim\n",
    "    for _ in range(depth):\n",
    "        layers.append(nn.Linear(dim, hidden_dim))\n",
    "        layers.append(activation)\n",
    "        dim = hidden_dim\n",
    "    layers.append(nn.Linear(dim, out_dim))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def build_mlp2(seq_len, seq_len_out, hidden_dim, depth, activation):\n",
    "    layers = []\n",
    "    for _ in range(depth):\n",
    "        layers.append(nn.Linear(seq_len, hidden_dim))\n",
    "        layers.append(activation)\n",
    "        seq_len = hidden_dim\n",
    "    layers.append(nn.Linear(seq_len, seq_len_out))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class FeatureTemporalBlock(nn.Module):\n",
    "    def __init__(self, num_features, seq_len,\n",
    "                 hidden_dim, depth_feature,depth_temporal,\n",
    "                 activation, use_ln):\n",
    "        super().__init__()\n",
    "\n",
    "        self.use_ln = use_ln\n",
    "        self.ln1 = nn.LayerNorm(num_features)\n",
    "        self.ln2 = nn.LayerNorm(num_features)\n",
    "\n",
    "        self.feature_mlp = build_mlp(\n",
    "            num_features, num_features,\n",
    "            hidden_dim, depth_feature, activation\n",
    "        )\n",
    "\n",
    "        self.temporal_mlp = build_mlp2(\n",
    "            seq_len, seq_len,\n",
    "            hidden_dim, depth_temporal, activation\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_ln:\n",
    "            x = x + self.feature_mlp(self.ln1(x))\n",
    "            y = self.ln2(x).transpose(1, 2)\n",
    "        else:\n",
    "            x = x + self.feature_mlp(x)\n",
    "            y = x.transpose(1, 2)\n",
    "\n",
    "        y = self.temporal_mlp(y).transpose(1, 2)\n",
    "        return x + y\n",
    "\n",
    "class FTMLP(nn.Module):\n",
    "    def __init__(self, num_features, seq_len,\n",
    "                 n_blocks, hidden_dim,\n",
    "                 depth_feature, depth_temporal, activation, use_ln):\n",
    "        super().__init__()\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            FeatureTemporalBlock(\n",
    "                num_features, seq_len,\n",
    "                hidden_dim, depth_feature, depth_temporal,\n",
    "                activation, use_ln\n",
    "            ) for _ in range(n_blocks)\n",
    "        ])\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(num_features * seq_len, 64),\n",
    "            activation,\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return self.head(x.reshape(x.size(0), -1))\n",
    "\n",
    "# ===============================\n",
    "# 4. ABLATION STUDY\n",
    "# ===============================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "activations = [\"silu\", \"elu\", \"prelu\", \"leaky_relu\", \"tanh\", \"sigmoid\",\"relu\",\"gelu\"]\n",
    "layer_norms = [True,False]\n",
    "depth_feature_      = [1,2,3,4]\n",
    "depth_temporal_    = [1,2,3,4]\n",
    "results = []\n",
    "\n",
    "for act_name in activations:\n",
    "    for use_ln in layer_norms:\n",
    "        for depth_feature in depth_feature_:\n",
    "            for depth_temporal in depth_temporal_:\n",
    "\n",
    "                set_seed(RANDOM_SEED)\n",
    "\n",
    "                model = FTMLP(\n",
    "                    num_features=len(FEATURE_COLS),\n",
    "                    seq_len=LOOKBACK,\n",
    "                    n_blocks=3,\n",
    "                    hidden_dim=32,\n",
    "                    depth_feature=depth_feature,\n",
    "                    depth_temporal=depth_temporal,\n",
    "                    activation=get_activation(act_name),\n",
    "                    use_ln=use_ln\n",
    "                ).to(device)\n",
    "\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "                criterion = nn.MSELoss()\n",
    "\n",
    "                # ---- Train ----\n",
    "                model.train()\n",
    "                for _ in range(EPOCHS):\n",
    "                    for xb, yb in train_loader:\n",
    "                        xb, yb = xb.to(device), yb.to(device)\n",
    "                        optimizer.zero_grad()\n",
    "                        loss = criterion(model(xb), yb)\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # ---- Evaluate ----\n",
    "                model.eval()\n",
    "                preds = []\n",
    "                with torch.no_grad():\n",
    "                    for xb, _ in test_loader:\n",
    "                        preds.append(model(xb.to(device)).cpu().numpy())\n",
    "\n",
    "                preds = np.vstack(preds)\n",
    "                y_inv = y_scaler.inverse_transform(y_test)\n",
    "                p_inv = y_scaler.inverse_transform(preds)\n",
    "\n",
    "                mape = mean_absolute_percentage_error(y_inv, p_inv)*100\n",
    "\n",
    "                results.append({\n",
    "                    \"activation\": act_name,\n",
    "                    \"layer_norm\": use_ln,\n",
    "                    \"feature_depth\": depth_feature,\n",
    "                    \"temporal_depth\": depth_temporal,\n",
    "                    \"MAPE\": round(mape, 4)\n",
    "                })\n",
    "\n",
    "                print(f\"Done | act={act_name}, LN={use_ln}, feature_depth={depth_feature}, temporal_depth={depth_temporal}, MAPE={mape:.4f}\")\n",
    "\n",
    "# ===============================\n",
    "# 5. RESULTS TABLE\n",
    "# ===============================\n",
    "results_df = pd.DataFrame(results).sort_values(\"MAPE\")\n",
    "print(\"\\n=== Ablation Results ===\")\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0625bda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(\"ablation_MLP_Open.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
