{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0499d12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall keras -y\n",
    "# !pip uninstall pytorch-lightning -y\n",
    "# !pip install numpy==1.26.4\n",
    "# !pip install pandas==2.1.4\n",
    "# !pip install matplotlib==3.7.2\n",
    "# !pip install -U scikit-learn\n",
    "# !pip install torch --upgrade\n",
    "# !pip install torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "# !pip install lightning==2.1.0\n",
    "# !pip install pytorch-forecasting==0.10.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ce0f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13241c5",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503940a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load Excel file\n",
    "file_path = \"Sustainable Data.xlsx\"   # replace with your file path\n",
    "df = pd.read_excel(file_path)\n",
    "df[\"datetime\"] = pd.to_datetime(df[\"year\"].astype(str) + \"-\" + df[\"month\"].astype(str) + \"-07\")\n",
    "cols_to_drop = ['FoodwastePermittedLimit(MT','Percentage difference']\n",
    "df = df.drop(columns=cols_to_drop)\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fe10fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nullCols(df,x):\n",
    "    print(\"--- Count of Null Values in Each Column ---\")\n",
    "    null_counts = df.isnull().sum()\n",
    "    display(null_counts[null_counts >x].sort_values(ascending=False))\n",
    "    print(null_counts[null_counts >x].index)\n",
    "\n",
    "def nullColsVis(df):\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Function to get bad columns for each year\n",
    "    def get_null_or_zero_cols(group):\n",
    "        bad_cols = group.columns[(group.isna().all()) | ((group == 0).all())].tolist()\n",
    "        return pd.Series({\n",
    "            \"num_columns\": len(bad_cols),\n",
    "            \"columns\": bad_cols\n",
    "        })\n",
    "\n",
    "    # Apply function per year\n",
    "    col_info_by_year = df.groupby(\"year\").apply(get_null_or_zero_cols).reset_index()\n",
    "\n",
    "    # Print results\n",
    "    for _, row in col_info_by_year.iterrows():\n",
    "        print(f\"\\nYear: {row['year']}\")\n",
    "        print(f\"Number of completely null/zero columns: {row['num_columns']}\")\n",
    "        print(f\"Columns: {row['columns']}\")\n",
    "\n",
    "    # Visualization\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.bar(col_info_by_year[\"year\"], col_info_by_year[\"num_columns\"])\n",
    "    plt.xlabel(\"Year\")\n",
    "    plt.ylabel(\"Columns completely null or 0\")\n",
    "    plt.title(\"Completely null/zero columns per year\")\n",
    "    plt.show()\n",
    "\n",
    "def nullZero(df):\n",
    "    null_counts = df.isna().sum()\n",
    "    zero_counts = (df == 0).sum()\n",
    "    counts = pd.DataFrame({\n",
    "        \"Null Count\": null_counts,\n",
    "        \"Zero Count\": zero_counts,\n",
    "    })\n",
    "\n",
    "    print(counts[counts['Zero Count']>0].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e9856f",
   "metadata": {},
   "outputs": [],
   "source": [
    "col=['month', 'Current Date', 'Plant Name', 'Plant Name: organisation',\n",
    "       'quarter', 'year', 'financialYear', 'Scope1', 'Scope_2',\n",
    "       'totalEnergyPerUnit(GJ)', 'TotalCO2emission(MT)',\n",
    "       'Electricity Grid Energy Per Unit (GJ)',\n",
    "       'Electricity Grid TCO2 Emission',\n",
    "       'Production Actual Quantity (MT/Month)', 'totalWaterConsumption',\n",
    "       'Attachments', 'Plant Location', 'gridEmissionTesting',\n",
    "       'Plant Name: category', 'datetime']\n",
    "dfAll=df[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b66535",
   "metadata": {},
   "outputs": [],
   "source": [
    "col = \"Production Actual Quantity (MT/Month)\"\n",
    "dfAll[col] = dfAll[col].fillna(dfAll[col].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99477071",
   "metadata": {},
   "outputs": [],
   "source": [
    "nullCols(dfAll,0)\n",
    "nullZero(dfAll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9122bc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "col2=['Scope1', 'Scope_2', 'totalEnergyPerUnit(GJ)', 'TotalCO2emission(MT)',\n",
    "       'Electricity Grid Energy Per Unit (GJ)',\n",
    "       'Electricity Grid TCO2 Emission',\n",
    "       'Production Actual Quantity (MT/Month)', 'totalWaterConsumption',\n",
    "       'Attachments', 'gridEmissionTesting']\n",
    "for col in col2:\n",
    "    dfAll[col] = dfAll[col].replace(0, np.nan).fillna(dfAll[col].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7e2cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 0) Imports =====\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "# ===== 1) CONFIG =====\n",
    "DATE_COL = \"datetime\"                       # if missing/dirty, we’ll rebuild from year & month\n",
    "TARGET_COL = \"TotalCO2emission(MT)\"             # the series to forecast\n",
    "PLANT_COL = \"Plant Name\"                        # per-plant option\n",
    "FORECAST_MONTHS = 6                            # horizon\n",
    "TEST_MONTHS = 12                                 # last N months as test\n",
    "\n",
    "# ===== 2) START FROM df_2020 =====\n",
    "# Assumes df_2020 exists in memory with the structure you shared.\n",
    "df = dfAll.copy()\n",
    "\n",
    "# ----- 2a) Ensure a proper datetime column 'ds' -----\n",
    "if DATE_COL in df.columns:\n",
    "    # Try parsing; if parsing fails for some rows, we rebuild ds below\n",
    "    ds = pd.to_datetime(df[DATE_COL], errors=\"coerce\", dayfirst=True, infer_datetime_format=True)\n",
    "else:\n",
    "    ds = pd.Series(pd.NaT, index=df.index)\n",
    "\n",
    "# If ds has NaT (or 'Current Date' missing), rebuild from year+month (set day=1)\n",
    "needs_rebuild = ds.isna()\n",
    "if needs_rebuild.any():\n",
    "    if not {\"year\", \"month\"}.issubset(df.columns):\n",
    "        raise ValueError(\"No valid 'Current Date' and missing 'year'/'month' to rebuild dates.\")\n",
    "    ds_rebuilt = pd.to_datetime(\n",
    "        dict(year=df.loc[needs_rebuild, \"year\"],\n",
    "             month=df.loc[needs_rebuild, \"month\"],\n",
    "             day=1)\n",
    "    )\n",
    "    ds.loc[needs_rebuild] = ds_rebuilt\n",
    "\n",
    "df[\"ds\"] = pd.to_datetime(ds).dt.to_period(\"M\").dt.to_timestamp()  # normalize to month-start\n",
    "\n",
    "# ----- 2b) Basic cleaning: drop obvious duplicates, keep consistent types -----\n",
    "df = df.drop_duplicates(subset=[\"ds\", PLANT_COL, TARGET_COL], keep=\"last\")\n",
    "\n",
    "# ----- 2c) Impute zeros/NaNs with mean for all numeric columns (by year, then global fallback) -----\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "def impute_zero_nan_with_mean(group: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Replace 0 with NaN, then fill with column mean (within group)\n",
    "    g = group.copy()\n",
    "    for c in num_cols:\n",
    "        g[c] = g[c].replace(0, np.nan)\n",
    "        if g[c].notna().any():\n",
    "            g[c] = g[c].fillna(g[c].mean())\n",
    "    return g\n",
    "\n",
    "# First try imputation within each calendar year (if present)\n",
    "if \"year\" in df.columns:\n",
    "    df = df.groupby(\"year\", group_keys=False).apply(impute_zero_nan_with_mean)\n",
    "else:\n",
    "    df = impute_zero_nan_with_mean(df)\n",
    "\n",
    "# Global fallback (in case an entire year's column was all zeros/NaN)\n",
    "for c in num_cols:\n",
    "    if df[c].isna().any():\n",
    "        df[c] = df[c].fillna(df[c].mean())\n",
    "\n",
    "# ===== 3) TWO PATHS: (A) OVERALL monthly forecast across all plants, (B) PER-PLANT forecasts =====\n",
    "\n",
    "# ---------- A) OVERALL SERIES ----------\n",
    "def prepare_overall_series(df_in: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Aggregate monthly across plants (sum is typical for emissions; use 'mean' if you prefer)\n",
    "    s = (df_in\n",
    "         .groupby(\"ds\", as_index=False)[TARGET_COL]\n",
    "         .sum()\n",
    "         .sort_values(\"ds\"))\n",
    "    s = s.rename(columns={TARGET_COL: \"y\"})\n",
    "    return s\n",
    "\n",
    "overall = prepare_overall_series(df)\n",
    "\n",
    "# Train / Test split by last TEST_MONTHS\n",
    "overall = overall.sort_values(\"ds\").reset_index(drop=True)\n",
    "split_point = len(overall) - TEST_MONTHS if len(overall) > TEST_MONTHS else len(overall)\n",
    "train_overall = overall.iloc[:split_point].copy()\n",
    "test_overall = overall.iloc[split_point:].copy()  # may be empty if not enough months\n",
    "\n",
    "# Fit Prophet (no extra regressors for the baseline)\n",
    "m_overall = Prophet(seasonality_mode=\"additive\", yearly_seasonality=True, weekly_seasonality=False, daily_seasonality=False)\n",
    "m_overall.fit(train_overall)\n",
    "\n",
    "# In-sample forecast to last training date (+ test window to compare)\n",
    "future_cutoff = train_overall[\"ds\"].max()\n",
    "future_all = m_overall.make_future_dataframe(periods=max(TEST_MONTHS, 0), freq=\"MS\")\n",
    "fcst_all = m_overall.predict(future_all)\n",
    "\n",
    "# Evaluate on test if available\n",
    "if not test_overall.empty:\n",
    "    # Align predictions with actual test months\n",
    "    y_pred = (fcst_all[[\"ds\", \"yhat\"]]\n",
    "              .merge(test_overall[[\"ds\", \"y\"]], on=\"ds\", how=\"inner\"))\n",
    "    mape_overall = mean_absolute_percentage_error(y_pred[\"y\"], y_pred[\"yhat\"])\n",
    "else:\n",
    "    mape_overall = np.nan\n",
    "\n",
    "# Final 12-month forecast (beyond the full available data)\n",
    "future_12 = m_overall.make_future_dataframe(periods=FORECAST_MONTHS, freq=\"MS\")\n",
    "forecast_12 = m_overall.predict(future_12).loc[:, [\"ds\", \"yhat\", \"yhat_lower\", \"yhat_upper\"]]\n",
    "forecast_12 = forecast_12[forecast_12[\"ds\"] > overall[\"ds\"].max()].reset_index(drop=True)\n",
    "\n",
    "print(\"=== OVERALL SERIES ===\")\n",
    "print(f\"Training months: {len(train_overall)} | Test months: {len(test_overall)}\")\n",
    "print(f\"Overall Test MAPE: {mape_overall:.3f}\" if not np.isnan(mape_overall) else \"Overall Test MAPE: N/A (not enough test months)\")\n",
    "print(\"\\nNext 12 months forecast (overall):\")\n",
    "print(forecast_12)\n",
    "\n",
    "# ---------- B) PER-PLANT SERIES (optional) ----------\n",
    "# Build one model per plant; returns a dict of {plant: (mape, forecast_df)}\n",
    "def forecast_per_plant(df_in: pd.DataFrame, test_months: int = 3, horizon: int = 12):\n",
    "    out = {}\n",
    "    g = (df_in.groupby([PLANT_COL, \"ds\"], as_index=False)[TARGET_COL]\n",
    "         .sum()\n",
    "         .rename(columns={TARGET_COL: \"y\"}))\n",
    "\n",
    "    for plant, gdf in g.groupby(PLANT_COL):\n",
    "        gdf = gdf.sort_values(\"ds\").reset_index(drop=True)\n",
    "        if len(gdf) < 6:\n",
    "            out[plant] = (np.nan, pd.DataFrame())\n",
    "            continue\n",
    "\n",
    "        split = len(gdf) - test_months if len(gdf) > test_months else len(gdf)\n",
    "        train = gdf.iloc[:split].copy()\n",
    "        test  = gdf.iloc[split:].copy()\n",
    "\n",
    "        m = Prophet(seasonality_mode=\"additive\", yearly_seasonality=True, weekly_seasonality=False, daily_seasonality=False)\n",
    "        m.fit(train)\n",
    "\n",
    "        future_all = m.make_future_dataframe(periods=max(test_months, 0), freq=\"MS\")\n",
    "        fcst_all  = m.predict(future_all)\n",
    "\n",
    "        # Align test and predictions\n",
    "        y_pred = (fcst_all[[\"ds\", \"yhat\"]]\n",
    "                  .merge(test[[\"ds\", \"y\"]], on=\"ds\", how=\"inner\"))\n",
    "\n",
    "        if not test.empty and not y_pred.empty:\n",
    "            mape = mean_absolute_percentage_error(y_pred[\"y\"], y_pred[\"yhat\"])\n",
    "        else:\n",
    "            mape = np.nan   # No overlap, can’t compute MAPE\n",
    "\n",
    "        future_h = m.make_future_dataframe(periods=horizon, freq=\"MS\")\n",
    "        fcst_h = m.predict(future_h)[[\"ds\", \"yhat\", \"yhat_lower\", \"yhat_upper\"]]\n",
    "        fcst_h = fcst_h[fcst_h[\"ds\"] > gdf[\"ds\"].max()].reset_index(drop=True)\n",
    "\n",
    "        out[plant] = (mape, fcst_h)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# Run per-plant (optional; comment this block if you only need overall)\n",
    "per_plant_results = forecast_per_plant(df, test_months=TEST_MONTHS, horizon=FORECAST_MONTHS)\n",
    "\n",
    "# Show a quick summary table for per-plant MAPE\n",
    "per_plant_mape = pd.DataFrame(\n",
    "    [(plant, mape) for plant, (mape, _) in per_plant_results.items()],\n",
    "    columns=[PLANT_COL, \"Test MAPE\"]\n",
    ").sort_values(\"Test MAPE\", na_position=\"last\")\n",
    "print(\"\\n=== PER-PLANT MAPE (lower is better) ===\")\n",
    "print(per_plant_mape.head(20))  # top 20; adjust as needed\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f5549e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Align predictions with full actuals\n",
    "eval_df = (fcst_all[[\"ds\", \"yhat\"]]\n",
    "           .merge(overall[[\"ds\", \"y\"]], on=\"ds\", how=\"left\"))\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "\n",
    "# A) Train actuals\n",
    "plt.plot(train_overall['ds'], train_overall['y'],\n",
    "         label=\"Train (Actual)\", linewidth=2, color=\"blue\")\n",
    "\n",
    "# B) Test actuals\n",
    "plt.plot(test_overall['ds'], test_overall['y'],\n",
    "         label=\"Test (Actual)\", linewidth=2, color=\"green\")\n",
    "\n",
    "# # C) Prophet forecast (fitted + on test window)\n",
    "# plt.plot(eval_df['ds'], eval_df['yhat'],\n",
    "#          '--', label=\"Forecast (Prophet)\", linewidth=2, color=\"red\")\n",
    "\n",
    "# # D) Train/Test split marker\n",
    "# if not test_overall.empty:\n",
    "#     split_date = test_overall['ds'].min()\n",
    "#     plt.axvline(split_date, color='gray', linestyle='--',\n",
    "#                 linewidth=2, label=\"Train/Test Split\")\n",
    "\n",
    "# Titles and labels\n",
    "plt.title(\"Prophet Forecast vs Actual (Overall Series)\", fontsize=14)\n",
    "plt.xlabel(\"Date\", fontsize=12)\n",
    "plt.ylabel(\"Total CO2 Emissions (MT)\", fontsize=12)\n",
    "\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00348f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Align predictions with full actuals\n",
    "eval_df = (fcst_all[[\"ds\", \"yhat\"]]\n",
    "           .merge(overall[[\"ds\", \"y\"]], on=\"ds\", how=\"left\"))\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "\n",
    "# A) Train actuals\n",
    "plt.plot(train_overall['ds'], train_overall['y'],\n",
    "         label=\"Train (Actual)\", linewidth=2, color=\"blue\")\n",
    "\n",
    "# B) Test actuals\n",
    "plt.plot(test_overall['ds'], test_overall['y'],\n",
    "         label=\"Test (Actual)\", linewidth=2, color=\"green\")\n",
    "\n",
    "# C) Prophet forecast (fitted + on test window)\n",
    "plt.plot(eval_df['ds'], eval_df['yhat'],\n",
    "         '--', label=\"Forecast (Prophet)\", linewidth=2, color=\"red\")\n",
    "\n",
    "# D) Train/Test split marker\n",
    "if not test_overall.empty:\n",
    "    split_date = test_overall['ds'].min()\n",
    "    plt.axvline(split_date, color='gray', linestyle='--',\n",
    "                linewidth=2, label=\"Train/Test Split\")\n",
    "\n",
    "# Titles and labels\n",
    "plt.title(\"Prophet Forecast vs Actual (Overall Series)\", fontsize=14)\n",
    "plt.xlabel(\"Date\", fontsize=12)\n",
    "plt.ylabel(\"Total CO2 Emissions (MT)\", fontsize=12)\n",
    "\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97ff4b9",
   "metadata": {},
   "source": [
    "# 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7accdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Client_Oct_ProcessedFULL.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313e79ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_2022=df.copy(deep=True)\n",
    "\n",
    "df_2022=df_2022[df_2022['year']>=2021]\n",
    "print(df_2022.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fcf382",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=df_2022[df_2022['year']>=2022]\n",
    "print(temp.shape)\n",
    "print(\"--- Count of Null Values in Each Column ---\")\n",
    "\n",
    "null_counts = temp.isnull().sum()\n",
    "display(null_counts[null_counts >0].sort_values(ascending=False))\n",
    "print(null_counts[null_counts >0].index.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b625413",
   "metadata": {},
   "outputs": [],
   "source": [
    "col = \"Production Actual Quantity (MT/Month)\"\n",
    "df_2022[col] = df_2022[col].fillna(df_2022[col].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e88f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nullZero(df):\n",
    "    null_counts = df_2022.isna().sum()\n",
    "    zero_counts = (df_2022 == 0).sum()\n",
    "    counts = pd.DataFrame({\n",
    "        \"Null Count\": null_counts,\n",
    "        \"Zero Count\": zero_counts,\n",
    "    })\n",
    "\n",
    "    print(counts[counts['Zero Count']>0].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b10a8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "col2=['Scope1', 'Scope_2', 'totalEnergyPerUnit(GJ)', 'TotalCO2emission(MT)',\n",
    "       'Electricity Grid Energy Per Unit (GJ)',\n",
    "       'Electricity Grid TCO2 Emission',\n",
    "       'Production Actual Quantity (MT/Month)', 'totalWaterConsumption',\n",
    "       'Attachments', 'gridEmissionTesting']\n",
    "for col in col2:\n",
    "    df_2022[col] = df_2022[col].replace(0, np.nan).fillna(df_2022[col].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a8a468",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2022[\"Scope1_per_unit\"] = df_2022[\"Scope1\"] / df_2022[\"Production Actual Quantity (MT/Month)\"]\n",
    "df_2022[\"CO2_per_unit\"] = df_2022[\"TotalCO2emission(MT)\"] / df_2022[\"Production Actual Quantity (MT/Month)\"]\n",
    "\n",
    "\n",
    "df_2022.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bb61be",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13407460",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2022.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a7a562",
   "metadata": {},
   "outputs": [],
   "source": [
    "eda=df_2022.copy(deep=True)\n",
    "eda=eda[['datetime','Plant Name: category','Plant Name: organisation','Plant Name','Plant Location','Scope1','Scope_2','Production Actual Quantity (MT/Month)','TotalCO2emission(MT)','Scope1_per_unit',\n",
    "       'CO2_per_unit']]\n",
    "eda.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ba7a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in eda.select_dtypes('object').columns:\n",
    "    print(col)\n",
    "    print(eda[col].unique())\n",
    "    print(len(eda[col].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9462127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 0) Imports =====\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "df = eda.copy()  # your dataframe\n",
    "\n",
    "# ===== 1) Basic Info =====\n",
    "print(\"Data shape:\", df.shape)\n",
    "print(\"\\nColumns:\\n\", df.columns)\n",
    "print(\"\\nSample data:\\n\", df.head(1))\n",
    "print(\"\\nSummary stats:\\n\", df.describe(include=\"all\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08628e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebeecf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Plant Location'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9de5988",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (18, 6)\n",
    "\n",
    "# ===== 2) Plants in India vs Outside =====\n",
    "df[\"India_Flag\"] = df[\"Plant Location\"].apply(lambda x: \"India\" if x==\"India\" else \"Outside India\")\n",
    "plant_counts = df.groupby(\"India_Flag\")[\"Plant Name\"].nunique().reset_index()\n",
    "plant_counts.rename(columns={\"Plant Name\":\"Unique Plants\"}, inplace=True)\n",
    "\n",
    "ax = sns.barplot(data=plant_counts, x=\"India_Flag\", y=\"Unique Plants\", palette=\"viridis\")\n",
    "for p in ax.patches:\n",
    "    ax.annotate(int(p.get_height()), \n",
    "                (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                ha='center', va='bottom', fontsize=11, color='black', xytext=(0, 5),\n",
    "                textcoords='offset points')\n",
    "plt.title(\"Number of Unique Plants: India vs Outside\")\n",
    "plt.show()\n",
    "\n",
    "plant_counts = df.groupby(\"Plant Location\")[\"Plant Name\"].nunique().reset_index()\n",
    "plant_counts.rename(columns={\"Plant Name\":\"Unique Plants\"}, inplace=True)\n",
    "\n",
    "ax = sns.barplot(data=plant_counts, x=\"Plant Location\", y=\"Unique Plants\", palette=\"viridis\")\n",
    "for p in ax.patches:\n",
    "    ax.annotate(int(p.get_height()), \n",
    "                (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                ha='center', va='bottom', fontsize=11, color='black', xytext=(0, 5),\n",
    "                textcoords='offset points')\n",
    "plt.title(\"Number of Unique Plants in Different Locations\")\n",
    "plt.show()\n",
    "plt.rcParams[\"figure.figsize\"] = (18, 6)\n",
    "\n",
    "# ===== 6) Total Scope1 emissions over time =====\n",
    "scope1_ts = df.groupby(\"datetime\")[\"Scope1_per_unit\"].sum().reset_index()\n",
    "sns.lineplot(data=scope1_ts, x=\"datetime\", y=\"Scope1_per_unit\", marker=\"o\")\n",
    "plt.title(\"Total Scope1 Emissions Over Time\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Scope1 Emissions\")\n",
    "plt.show()\n",
    "# ===== 8) Scope1 vs Production vs CO2 timeline =====\n",
    "scope1_prod_ts = df.groupby(\"datetime\")[[\n",
    "    \"Scope1_per_unit\",\n",
    "    \"TotalCO2emission(MT)\",\n",
    "    \"Production Actual Quantity (MT/Month)\"\n",
    "]].sum().reset_index()\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(18, 6))\n",
    "\n",
    "# --- Left y-axis ---\n",
    "ax1.set_xlabel(\"Date\")\n",
    "ax1.set_ylabel(\"Scope1 / CO2 (per unit)\", color=\"tab:blue\")\n",
    "ax1.plot(scope1_prod_ts[\"datetime\"], scope1_prod_ts[\"Scope1_per_unit\"], \n",
    "         color=\"tab:blue\", marker=\"o\", label=\"Scope1 per Unit\")\n",
    "ax1.tick_params(axis=\"y\", labelcolor=\"tab:blue\")\n",
    "\n",
    "# --- Right y-axis ---\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel(\"Production & Total CO2\", color=\"tab:green\")\n",
    "ax2.plot(scope1_prod_ts[\"datetime\"], scope1_prod_ts[\"Production Actual Quantity (MT/Month)\"], \n",
    "         color=\"tab:green\", marker=\"s\", linestyle=\"--\", label=\"Production\")\n",
    "ax2.plot(scope1_prod_ts[\"datetime\"], scope1_prod_ts[\"TotalCO2emission(MT)\"], \n",
    "         color=\"tab:red\", marker=\"d\", linestyle=\"-.\", label=\"Total CO2\")\n",
    "ax2.tick_params(axis=\"y\", labelcolor=\"tab:green\")\n",
    "\n",
    "# --- Title & legend ---\n",
    "fig.suptitle(\"Scope1 Emissions, CO2 and Production Over Time\", fontsize=14)\n",
    "\n",
    "# merge legends from both axes\n",
    "lines, labels = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "fig.legend(lines + lines2, labels + labels2, loc=\"upper left\", bbox_to_anchor=(0.1, 0.9))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ===== 9) Scope1 emissions per unit per plant (India vs Outside India) =====\n",
    "# Count unique plants by region\n",
    "plant_counts = df.groupby(\"India_Flag\")[\"Plant Name\"].nunique().to_dict()\n",
    "\n",
    "# Aggregate Scope1 per unit by datetime and region\n",
    "scope1_india_ts = (\n",
    "    df.groupby([\"datetime\", \"India_Flag\"])[\"Scope1_per_unit\"]\n",
    "      .sum()\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "# Divide by number of plants in each region\n",
    "scope1_india_ts[\"Scope1_per_unit_per_plant\"] = scope1_india_ts.apply(\n",
    "    lambda row: row[\"Scope1_per_unit\"] / plant_counts[row[\"India_Flag\"]],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Split into two series\n",
    "india_ts = scope1_india_ts[scope1_india_ts[\"India_Flag\"] == \"India\"]\n",
    "outside_ts = scope1_india_ts[scope1_india_ts[\"India_Flag\"] == \"Outside India\"]\n",
    "\n",
    "# ---- Plot with dual y-axes ----\n",
    "fig, ax1 = plt.subplots(figsize=(18, 6))\n",
    "\n",
    "# Left y-axis = India\n",
    "ax1.set_xlabel(\"Date\")\n",
    "ax1.set_ylabel(\"Scope1 per Unit per Plant (India)\", color=\"tab:blue\")\n",
    "ax1.plot(india_ts[\"datetime\"], india_ts[\"Scope1_per_unit_per_plant\"], \n",
    "         color=\"tab:blue\", marker=\"o\", label=\"India\")\n",
    "ax1.tick_params(axis=\"y\", labelcolor=\"tab:blue\")\n",
    "\n",
    "# Right y-axis = Outside India\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel(\"Scope1 per Unit per Plant (Outside India)\", color=\"tab:orange\")\n",
    "ax2.plot(outside_ts[\"datetime\"], outside_ts[\"Scope1_per_unit_per_plant\"], \n",
    "         color=\"tab:orange\", marker=\"s\", linestyle=\"--\", label=\"Outside India\")\n",
    "ax2.tick_params(axis=\"y\", labelcolor=\"tab:orange\")\n",
    "\n",
    "# Title & Legend\n",
    "fig.suptitle(\"Scope1 Emissions per Unit per Plant: India vs Outside India\", fontsize=14)\n",
    "\n",
    "lines, labels = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "fig.legend(lines + lines2, labels + labels2, loc=\"upper left\", bbox_to_anchor=(0.1, 0.9))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d75556",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2022.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93da84ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (4, 4)\n",
    "\n",
    "plant_counts = df.groupby(\"Plant Location\")[\"Plant Name\"].nunique().reset_index()\n",
    "plant_counts.rename(columns={\"Plant Name\":\"Unique Plants\"}, inplace=True)\n",
    "top5_plants = plant_counts.sort_values(\"Unique Plants\", ascending=False).head(5)\n",
    "\n",
    "ax = sns.barplot(data=top5_plants, x=\"Plant Location\", y=\"Unique Plants\", palette=\"viridis\")\n",
    "for p in ax.patches:\n",
    "    ax.annotate(int(p.get_height()), \n",
    "                (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                ha='center', va='bottom', fontsize=11, color='black', xytext=(0, 5),\n",
    "                textcoords='offset points')\n",
    "plt.title(\"Number of Unique Plants in Different Locations\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d185d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scope1_prod_ts = df_2022.groupby(\"datetime\")[[\n",
    "    \"Scope1_per_unit\",\n",
    "    \"TotalCO2emission(MT)\",\n",
    "    \"Production Actual Quantity (MT/Month)\",\n",
    "     'Scope1', 'Scope_2',\n",
    "       'totalEnergyPerUnit(GJ)', 'TotalCO2emission(MT)',\n",
    "       'Electricity Grid Energy Per Unit (GJ)',\n",
    "       'Electricity Grid TCO2 Emission',\n",
    "       'Production Actual Quantity (MT/Month)', 'totalWaterConsumption'\n",
    "]].sum().reset_index()\n",
    "\n",
    "def chart(col):\n",
    "    fig, ax1 = plt.subplots(figsize=(18, 6))\n",
    "    # --- Left y-axis ---\n",
    "    ax1.set_xlabel(\"Date\")\n",
    "    ax1.set_ylabel(f\"{col}\", color=\"tab:blue\")\n",
    "    ax1.plot(scope1_prod_ts[\"datetime\"], scope1_prod_ts[col], \n",
    "            color=\"tab:blue\", marker=\"o\", label=f\"{col}\")\n",
    "    ax1.tick_params(axis=\"y\", labelcolor=\"tab:blue\")\n",
    "    fig.suptitle(f\"{col} Over Time\", fontsize=14)\n",
    "\n",
    "    # merge legends from both axes\n",
    "    lines, labels = ax1.get_legend_handles_labels()\n",
    "    fig.legend(lines, labels, loc=\"upper left\", bbox_to_anchor=(0.1, 0.9))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "for col in scope1_prod_ts.columns:\n",
    "    if col != 'datetime':\n",
    "        chart(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a699b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2022.to_csv(\"Client_Oct_Processed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6aa9a4",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f526fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 0) Imports =====\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "# ===== 1) CONFIG =====\n",
    "DATE_COL = \"datetime\"                       # if missing/dirty, we’ll rebuild from year & month\n",
    "TARGET_COL = \"Scope1_per_unit\"             # the series to forecast\n",
    "PLANT_COL = \"Plant Name\"                        # per-plant option\n",
    "FORECAST_MONTHS = 7                           # horizon\n",
    "TEST_MONTHS = 7                                 # last N months as test\n",
    "\n",
    "# ===== 2) START FROM df_2020 =====\n",
    "# Assumes df_2020 exists in memory with the structure you shared.\n",
    "df = df_2022.copy()\n",
    "\n",
    "# ----- 2a) Ensure a proper datetime column 'ds' -----\n",
    "if DATE_COL in df.columns:\n",
    "    # Try parsing; if parsing fails for some rows, we rebuild ds below\n",
    "    ds = pd.to_datetime(df[DATE_COL], errors=\"coerce\", dayfirst=True, infer_datetime_format=True)\n",
    "else:\n",
    "    ds = pd.Series(pd.NaT, index=df.index)\n",
    "\n",
    "# If ds has NaT (or 'Current Date' missing), rebuild from year+month (set day=1)\n",
    "needs_rebuild = ds.isna()\n",
    "if needs_rebuild.any():\n",
    "    if not {\"year\", \"month\"}.issubset(df.columns):\n",
    "        raise ValueError(\"No valid 'Current Date' and missing 'year'/'month' to rebuild dates.\")\n",
    "    ds_rebuilt = pd.to_datetime(\n",
    "        dict(year=df.loc[needs_rebuild, \"year\"],\n",
    "             month=df.loc[needs_rebuild, \"month\"],\n",
    "             day=1)\n",
    "    )\n",
    "    ds.loc[needs_rebuild] = ds_rebuilt\n",
    "\n",
    "df[\"ds\"] = pd.to_datetime(ds).dt.to_period(\"M\").dt.to_timestamp()  # normalize to month-start\n",
    "\n",
    "# ----- 2b) Basic cleaning: drop obvious duplicates, keep consistent types -----\n",
    "df = df.drop_duplicates(subset=[\"ds\", PLANT_COL, TARGET_COL], keep=\"last\")\n",
    "\n",
    "# ----- 2c) Impute zeros/NaNs with mean for all numeric columns (by year, then global fallback) -----\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "def impute_zero_nan_with_mean(group: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Replace 0 with NaN, then fill with column mean (within group)\n",
    "    g = group.copy()\n",
    "    for c in num_cols:\n",
    "        g[c] = g[c].replace(0, np.nan)\n",
    "        if g[c].notna().any():\n",
    "            g[c] = g[c].fillna(g[c].mean())\n",
    "    return g\n",
    "\n",
    "# First try imputation within each calendar year (if present)\n",
    "if \"year\" in df.columns:\n",
    "    df = df.groupby(\"year\", group_keys=False).apply(impute_zero_nan_with_mean)\n",
    "else:\n",
    "    df = impute_zero_nan_with_mean(df)\n",
    "\n",
    "# Global fallback (in case an entire year's column was all zeros/NaN)\n",
    "for c in num_cols:\n",
    "    if df[c].isna().any():\n",
    "        df[c] = df[c].fillna(df[c].mean())\n",
    "\n",
    "# ===== 3) TWO PATHS: (A) OVERALL monthly forecast across all plants, (B) PER-PLANT forecasts =====\n",
    "\n",
    "# ---------- A) OVERALL SERIES ----------\n",
    "def prepare_overall_series(df_in: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Aggregate monthly across plants (sum is typical for emissions; use 'mean' if you prefer)\n",
    "    s = (df_in\n",
    "         .groupby(\"ds\", as_index=False)[TARGET_COL]\n",
    "         .sum()\n",
    "         .sort_values(\"ds\"))\n",
    "    s = s.rename(columns={TARGET_COL: \"y\"})\n",
    "    return s\n",
    "\n",
    "overall = prepare_overall_series(df)\n",
    "\n",
    "# Train / Test split by last TEST_MONTHS\n",
    "overall = overall.sort_values(\"ds\").reset_index(drop=True)\n",
    "split_point = len(overall) - TEST_MONTHS if len(overall) > TEST_MONTHS else len(overall)\n",
    "train_overall = overall.iloc[:split_point].copy()\n",
    "test_overall = overall.iloc[split_point:].copy()  # may be empty if not enough months\n",
    "\n",
    "# Fit Prophet (no extra regressors for the baseline)\n",
    "m_overall = Prophet(seasonality_mode=\"additive\", yearly_seasonality=True, weekly_seasonality=False, daily_seasonality=False)\n",
    "m_overall.fit(train_overall)\n",
    "\n",
    "# In-sample forecast to last training date (+ test window to compare)\n",
    "future_cutoff = train_overall[\"ds\"].max()\n",
    "future_all = m_overall.make_future_dataframe(periods=max(TEST_MONTHS, 0), freq=\"MS\")\n",
    "fcst_all = m_overall.predict(future_all)\n",
    "\n",
    "# Evaluate on test if available\n",
    "if not test_overall.empty:\n",
    "    # Align predictions with actual test months\n",
    "    y_pred = (fcst_all[[\"ds\", \"yhat\"]]\n",
    "              .merge(test_overall[[\"ds\", \"y\"]], on=\"ds\", how=\"inner\"))\n",
    "    mape_overall = mean_absolute_percentage_error(y_pred[\"y\"], y_pred[\"yhat\"])\n",
    "else:\n",
    "    mape_overall = np.nan\n",
    "\n",
    "# Final 12-month forecast (beyond the full available data)\n",
    "future_12 = m_overall.make_future_dataframe(periods=FORECAST_MONTHS, freq=\"MS\")\n",
    "forecast_12 = m_overall.predict(future_12).loc[:, [\"ds\", \"yhat\", \"yhat_lower\", \"yhat_upper\"]]\n",
    "forecast_12 = forecast_12[forecast_12[\"ds\"] > overall[\"ds\"].max()].reset_index(drop=True)\n",
    "\n",
    "print(\"=== OVERALL SERIES ===\")\n",
    "print(f\"Training months: {len(train_overall)} | Test months: {len(test_overall)}\")\n",
    "print(f\"Overall Test MAPE: {mape_overall:.3f}\" if not np.isnan(mape_overall) else \"Overall Test MAPE: N/A (not enough test months)\")\n",
    "print(\"\\nNext 12 months forecast (overall):\")\n",
    "print(forecast_12)\n",
    "\n",
    "# ---------- B) PER-PLANT SERIES (optional) ----------\n",
    "# Build one model per plant; returns a dict of {plant: (mape, forecast_df)}\n",
    "def forecast_per_plant(df_in: pd.DataFrame, test_months: int = 3, horizon: int = 12):\n",
    "    out = {}\n",
    "    g = (df_in.groupby([PLANT_COL, \"ds\"], as_index=False)[TARGET_COL]\n",
    "         .sum()\n",
    "         .rename(columns={TARGET_COL: \"y\"}))\n",
    "\n",
    "    for plant, gdf in g.groupby(PLANT_COL):\n",
    "        gdf = gdf.sort_values(\"ds\").reset_index(drop=True)\n",
    "        if len(gdf) < 6:\n",
    "            out[plant] = (np.nan, pd.DataFrame())\n",
    "            continue\n",
    "\n",
    "        split = len(gdf) - test_months if len(gdf) > test_months else len(gdf)\n",
    "        train = gdf.iloc[:split].copy()\n",
    "        test  = gdf.iloc[split:].copy()\n",
    "\n",
    "        m = Prophet(seasonality_mode=\"multiplicative\", yearly_seasonality=True, weekly_seasonality=False, daily_seasonality=False, changepoint_prior_scale=0.1)\n",
    "        m.fit(train)\n",
    "\n",
    "        future_all = m.make_future_dataframe(periods=max(test_months, 0), freq=\"MS\")\n",
    "        fcst_all  = m.predict(future_all)\n",
    "\n",
    "        # Align test and predictions\n",
    "        y_pred = (fcst_all[[\"ds\", \"yhat\"]]\n",
    "                  .merge(test[[\"ds\", \"y\"]], on=\"ds\", how=\"inner\"))\n",
    "\n",
    "        if not test.empty and not y_pred.empty:\n",
    "            mape = mean_absolute_percentage_error(y_pred[\"y\"], y_pred[\"yhat\"])\n",
    "        else:\n",
    "            mape = np.nan   # No overlap, can’t compute MAPE\n",
    "\n",
    "        future_h = m.make_future_dataframe(periods=horizon, freq=\"MS\")\n",
    "        fcst_h = m.predict(future_h)[[\"ds\", \"yhat\", \"yhat_lower\", \"yhat_upper\"]]\n",
    "        fcst_h = fcst_h[fcst_h[\"ds\"] > gdf[\"ds\"].max()].reset_index(drop=True)\n",
    "\n",
    "        out[plant] = (mape, fcst_h)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# Run per-plant (optional; comment this block if you only need overall)\n",
    "per_plant_results = forecast_per_plant(df, test_months=TEST_MONTHS, horizon=FORECAST_MONTHS)\n",
    "\n",
    "# Show a quick summary table for per-plant MAPE\n",
    "per_plant_mape = pd.DataFrame(\n",
    "    [(plant, mape) for plant, (mape, _) in per_plant_results.items()],\n",
    "    columns=[PLANT_COL, \"Test MAPE\"]\n",
    ").sort_values(\"Test MAPE\", na_position=\"last\")\n",
    "print(\"\\n=== PER-PLANT MAPE (lower is better) ===\")\n",
    "print(per_plant_mape.head(20))  # top 20; adjust as needed\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0440b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Align predictions with full actuals\n",
    "eval_df = (fcst_all[[\"ds\", \"yhat\"]]\n",
    "           .merge(overall[[\"ds\", \"y\"]], on=\"ds\", how=\"left\"))\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "\n",
    "# A) Train actuals\n",
    "plt.plot(train_overall['ds'], train_overall['y'],\n",
    "         label=\"Train (Actual)\", linewidth=2, color=\"blue\")\n",
    "\n",
    "# B) Test actuals\n",
    "plt.plot(test_overall['ds'], test_overall['y'],\n",
    "         label=\"Test (Actual)\", linewidth=2, color=\"green\")\n",
    "\n",
    "# C) Prophet forecast (fitted + on test window)\n",
    "plt.plot(eval_df['ds'], eval_df['yhat'],\n",
    "         '--', label=\"Forecast (Prophet)\", linewidth=2, color=\"red\")\n",
    "\n",
    "# D) Train/Test split marker\n",
    "if not test_overall.empty:\n",
    "    split_date = test_overall['ds'].min()\n",
    "    plt.axvline(split_date, color='gray', linestyle='--',\n",
    "                linewidth=2, label=\"Train/Test Split\")\n",
    "\n",
    "# Titles and labels\n",
    "plt.title(\"Prophet Forecast vs Actual (Overall Series)\", fontsize=14)\n",
    "plt.xlabel(\"Date\", fontsize=12)\n",
    "plt.ylabel(\"Total Scope 1 per unit production\", fontsize=12)\n",
    "\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba91cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 0) Imports =====\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import logging\n",
    "logging.getLogger(\"cmdstanpy\").setLevel(logging.WARNING)\n",
    "\n",
    "# ===== 1) CONFIG =====\n",
    "DATE_COL = \"datetime\"                       # if missing/dirty, we’ll rebuild from year & month\n",
    "TARGET_COL = \"Scope1_per_unit\"             # the series to forecast\n",
    "PLANT_COL = \"Plant Name\"                        # per-plant option\n",
    "FORECAST_MONTHS = 7                             # horizon\n",
    "TEST_MONTHS = 7                                 # last N months as test\n",
    "\n",
    "# ===== 2) START FROM df_2020 =====\n",
    "# Assumes df_2020 exists in memory with the structure you shared.\n",
    "df = df_2022.copy()\n",
    "# ----- 2a) Ensure a proper datetime column 'ds' -----\n",
    "if DATE_COL in df.columns:\n",
    "    # Try parsing; if parsing fails for some rows, we rebuild ds below\n",
    "    ds = pd.to_datetime(df[DATE_COL], errors=\"coerce\", infer_datetime_format=True)\n",
    "else:\n",
    "    ds = pd.Series(pd.NaT, index=df.index)\n",
    "\n",
    "# If ds has NaT (or 'Current Date' missing), rebuild from year+month (set day=1)\n",
    "needs_rebuild = ds.isna()\n",
    "if needs_rebuild.any():\n",
    "    if not {\"year\", \"month\"}.issubset(df.columns):\n",
    "        raise ValueError(\"No valid 'Current Date' and missing 'year'/'month' to rebuild dates.\")\n",
    "    ds_rebuilt = pd.to_datetime(\n",
    "        dict(year=df.loc[needs_rebuild, \"year\"],\n",
    "             month=df.loc[needs_rebuild, \"month\"],\n",
    "             day=1)\n",
    "    )\n",
    "    ds.loc[needs_rebuild] = ds_rebuilt\n",
    "\n",
    "df[\"ds\"] = pd.to_datetime(ds).dt.to_period(\"M\").dt.to_timestamp()  # normalize to month-start\n",
    "\n",
    "# ----- 2b) Basic cleaning: drop obvious duplicates, keep consistent types -----\n",
    "df = df.drop_duplicates(subset=[\"ds\", PLANT_COL, TARGET_COL], keep=\"last\")\n",
    "\n",
    "# ----- 2c) Impute zeros/NaNs with mean for all numeric columns (by year, then global fallback) -----\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "def impute_zero_nan_with_mean(group: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Replace 0 with NaN, then fill with column mean (within group)\n",
    "    g = group.copy()\n",
    "    for c in num_cols:\n",
    "        g[c] = g[c].replace(0, np.nan)\n",
    "        if g[c].notna().any():\n",
    "            g[c] = g[c].fillna(g[c].mean())\n",
    "    return g\n",
    "\n",
    "# First try imputation within each calendar year (if present)\n",
    "if \"year\" in df.columns:\n",
    "    df = df.groupby(\"year\", group_keys=False).apply(impute_zero_nan_with_mean)\n",
    "else:\n",
    "    df = impute_zero_nan_with_mean(df)\n",
    "\n",
    "# Global fallback (in case an entire year's column was all zeros/NaN)\n",
    "for c in num_cols:\n",
    "    if df[c].isna().any():\n",
    "        df[c] = df[c].fillna(df[c].mean())\n",
    "\n",
    "# ===== 3) TWO PATHS: (A) OVERALL monthly forecast across all plants, (B) PER-PLANT forecasts =====\n",
    "# ---------- A) OVERALL SERIES ----------\n",
    "def prepare_overall_series(df_in: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Aggregate monthly across plants (sum is typical for emissions; use 'mean' if you prefer)\n",
    "    s = (df_in\n",
    "         .groupby(\"ds\", as_index=False)[TARGET_COL]\n",
    "         .sum()\n",
    "         .sort_values(\"ds\"))\n",
    "    s = s.rename(columns={TARGET_COL: \"y\"})\n",
    "    return s\n",
    "\n",
    "overall = prepare_overall_series(df)\n",
    "# Train / Test split by last TEST_MONTHS\n",
    "overall = overall.sort_values(\"ds\").reset_index(drop=True)\n",
    "split_point = len(overall) - TEST_MONTHS if len(overall) > TEST_MONTHS else len(overall)\n",
    "train_overall = overall.iloc[:split_point].copy()\n",
    "test_overall = overall.iloc[split_point:].copy()  # may be empty if not enough months\n",
    "# Fit Prophet (no extra regressors for the baseline)\n",
    "m_overall = Prophet(seasonality_mode=\"multiplicative\", yearly_seasonality=True, weekly_seasonality=False, daily_seasonality=False,changepoint_prior_scale=0.1 )\n",
    "m_overall.fit(train_overall)\n",
    "\n",
    "# In-sample forecast to last training date (+ test window to compare)\n",
    "future_cutoff = train_overall[\"ds\"].max()\n",
    "future_all = m_overall.make_future_dataframe(periods=max(TEST_MONTHS, 0), freq=\"MS\")\n",
    "fcst_all = m_overall.predict(future_all)\n",
    "\n",
    "# Evaluate on test if available \n",
    "if not test_overall.empty:\n",
    "    # Align predictions with actual test months\n",
    "    y_pred = (fcst_all[[\"ds\", \"yhat\"]]\n",
    "              .merge(test_overall[[\"ds\", \"y\"]], on=\"ds\", how=\"inner\"))\n",
    "    mape_overall = mean_absolute_percentage_error(y_pred[\"y\"], y_pred[\"yhat\"])\n",
    "else:\n",
    "    mape_overall = np.nan\n",
    "\n",
    "# Final 12-month forecast (beyond the full available data)\n",
    "future_12 = m_overall.make_future_dataframe(periods=FORECAST_MONTHS, freq=\"MS\")\n",
    "forecast_12 = m_overall.predict(future_12).loc[:, [\"ds\", \"yhat\", \"yhat_lower\", \"yhat_upper\"]]\n",
    "forecast_12 = forecast_12[forecast_12[\"ds\"] > overall[\"ds\"].max()].reset_index(drop=True)\n",
    "print(\"=== OVERALL SERIES ===\")\n",
    "print(f\"Training months: {len(train_overall)} | Test months: {len(test_overall)}\")\n",
    "print(f\"Overall Test MAPE: {mape_overall:.3f}\" if not np.isnan(mape_overall) else \"Overall Test MAPE: N/A (not enough test months)\")\n",
    "# print(\"\\nNext 12 months forecast (overall):\")\n",
    "# print(forecast_12)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Align predictions with full actuals\n",
    "eval_df = (fcst_all[[\"ds\", \"yhat\"]]\n",
    "           .merge(overall[[\"ds\", \"y\"]], on=\"ds\", how=\"left\"))\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "\n",
    "# A) Train actuals\n",
    "plt.plot(train_overall['ds'], train_overall['y'],\n",
    "         label=\"Train (Actual)\", linewidth=2, color=\"blue\")\n",
    "\n",
    "# B) Test actuals\n",
    "plt.plot(test_overall['ds'], test_overall['y'],\n",
    "         label=\"Test (Actual)\", linewidth=2, color=\"green\")\n",
    "\n",
    "# C) Prophet forecast (fitted + on test window)\n",
    "plt.plot(eval_df['ds'], eval_df['yhat'],\n",
    "         '--', label=\"Forecast (Prophet)\", linewidth=2, color=\"red\")\n",
    "\n",
    "# D) Train/Test split marker\n",
    "if not test_overall.empty:\n",
    "    split_date = test_overall['ds'].min()\n",
    "    plt.axvline(split_date, color='gray', linestyle='--',\n",
    "                linewidth=2, label=\"Train/Test Split\")\n",
    "\n",
    "# Titles and labels\n",
    "plt.title(\"Prophet Forecast vs Actual (Overall Series)\", fontsize=14)\n",
    "plt.xlabel(\"Date\", fontsize=12)\n",
    "plt.ylabel(\"Total CO2 Emissions (MT)\", fontsize=12)\n",
    "\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b1df4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2022.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df82cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compare = df_2022[['datetime','Electricity Grid Energy Per Unit (GJ)', 'Scope1_per_unit','Production Actual Quantity (MT/Month)']]\n",
    "df_compare['elecPerUnit']=df_compare['Electricity Grid Energy Per Unit (GJ)']/df_compare['Production Actual Quantity (MT/Month)']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f163373e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ===== Group by datetime and sum both variables =====\n",
    "\n",
    "energy_scope1_ts = df_compare.groupby(\"datetime\")[[\n",
    "    \"elecPerUnit\",\n",
    "    \"Scope1_per_unit\"\n",
    "]].sum().reset_index()\n",
    "\n",
    "# ===== Plot with two y-axes =====\n",
    "fig, ax1 = plt.subplots(figsize=(18, 6))\n",
    "\n",
    "# --- Left y-axis (Energy) ---\n",
    "ax1.plot(energy_scope1_ts[\"datetime\"], energy_scope1_ts[\"elecPerUnit\"],\n",
    "         color=\"tab:blue\", marker=\"o\", label=\"Electricity Per Unit\")\n",
    "ax1.set_xlabel(\"Date\")\n",
    "ax1.set_ylabel(\"Electricity Per Unit\", color=\"tab:blue\")\n",
    "ax1.tick_params(axis=\"y\", labelcolor=\"tab:blue\")\n",
    "\n",
    "# --- Right y-axis (Scope1) ---\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(energy_scope1_ts[\"datetime\"], energy_scope1_ts[\"Scope1_per_unit\"],\n",
    "         color=\"tab:red\", marker=\"s\", label=\"Scope1 per Unit\")\n",
    "ax2.set_ylabel(\"Scope1 per Unit\", color=\"tab:red\")\n",
    "ax2.tick_params(axis=\"y\", labelcolor=\"tab:red\")\n",
    "\n",
    "# --- Title & Legend ---\n",
    "fig.suptitle(\"Electricity Grid Energy vs Scope1 Emissions (Summed Over Time)\", fontsize=14)\n",
    "\n",
    "# Merge legends\n",
    "lines, labels = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "fig.legend(lines + lines2, labels + labels2, loc=\"upper left\", bbox_to_anchor=(0.1, 0.9))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d05928",
   "metadata": {},
   "source": [
    "# Phy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aba77c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2022.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c108a670",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_compare = df_2022[['Electricity Grid Energy Per Unit (GJ)', 'Scope1','Scope1_per_unit','datetime']].copy()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# Group by datetime and aggregate\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Group by datetime and aggregate\n",
    "energy_scope1_ts = df_compare.groupby(\"datetime\")[[\n",
    "    \"Electricity Grid Energy Per Unit (GJ)\",\n",
    "    \"Scope1\",\n",
    "    \"Scope1_per_unit\"\n",
    "]].sum().reset_index()\n",
    "\n",
    "# Create dual-axis plot\n",
    "fig, ax1 = plt.subplots(figsize=(12,6))\n",
    "\n",
    "# Left y-axis (Electricity Grid Energy)\n",
    "color = \"tab:blue\"\n",
    "ax1.set_xlabel(\"Datetime\")\n",
    "ax1.set_ylabel(\"Electricity Grid Energy Per Unit (GJ)\", color=color)\n",
    "ax1.plot(energy_scope1_ts[\"datetime\"], \n",
    "         energy_scope1_ts[\"Electricity Grid Energy Per Unit (GJ)\"], \n",
    "         color=color, label=\"Electricity Grid Energy Per Unit (GJ)\")\n",
    "ax1.tick_params(axis=\"y\", labelcolor=color)\n",
    "\n",
    "# Right y-axis (Scope1_per_unit)\n",
    "ax1 = ax1.twinx()\n",
    "color = \"tab:green\"\n",
    "ax1.set_ylabel(\"Scope1\", color=color)\n",
    "ax1.plot(energy_scope1_ts[\"datetime\"], \n",
    "         energy_scope1_ts[\"Scope1\"], \n",
    "         color=color, label=\"Scope1\")\n",
    "ax1.tick_params(axis=\"y\", labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "color = \"tab:red\"\n",
    "ax2.set_ylabel(\"Scope1_per_unit\", color=color)\n",
    "ax2.plot(energy_scope1_ts[\"datetime\"], \n",
    "         energy_scope1_ts[\"Scope1_per_unit\"], \n",
    "         color=color, label=\"Scope1_per_unit\")\n",
    "ax2.tick_params(axis=\"y\", labelcolor=color)\n",
    "\n",
    "# Title and grid\n",
    "plt.title(\"Electricity Grid Energy vs Scope1 Emissions per Unit Over Time\")\n",
    "fig.tight_layout()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X = energy_scope1_ts[[\"Electricity Grid Energy Per Unit (GJ)\"]].values\n",
    "y = energy_scope1_ts[\"Scope1\"].values\n",
    "\n",
    "# Fit linear regression\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "slope = model.coef_[0]\n",
    "intercept = model.intercept_\n",
    "r2 = model.score(X, y)\n",
    "\n",
    "# Scatter + regression line\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.scatterplot(\n",
    "    x=\"Electricity Grid Energy Per Unit (GJ)\",\n",
    "    y=\"Scope1\",\n",
    "    data=energy_scope1_ts,\n",
    "    alpha=0.6,\n",
    "    color=\"purple\"\n",
    ")\n",
    "\n",
    "# Regression line\n",
    "x_vals = np.linspace(X.min(), X.max(), 100).reshape(-1,1)\n",
    "y_vals = model.predict(x_vals)\n",
    "plt.plot(x_vals, y_vals, color=\"red\", label=\"Regression Line\")\n",
    "\n",
    "# Equation text\n",
    "eq_text = f\"y = {slope:.3f}x + {intercept:.3f}\\nR² = {r2:.3f}\"\n",
    "plt.text(0.05, 0.95, eq_text, transform=plt.gca().transAxes, \n",
    "         fontsize=12, verticalalignment=\"top\", bbox=dict(facecolor=\"white\", alpha=0.7))\n",
    "\n",
    "plt.xlabel(\"Electricity Grid Energy Per Unit (GJ)\")\n",
    "plt.ylabel(\"Scope1\")\n",
    "plt.title(\"Scatter Plot with Regression Line\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "print(eq_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ce1dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = energy_scope1_ts[[\"Electricity Grid Energy Per Unit (GJ)\"]].values\n",
    "y = energy_scope1_ts[\"Scope1\"].values\n",
    "\n",
    "# Fit linear regression\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "slope = model.coef_[0]\n",
    "intercept = model.intercept_\n",
    "r2 = model.score(X, y)\n",
    "\n",
    "# Scatter + regression line\n",
    "plt.figure(figsize=(10,4))\n",
    "sns.scatterplot(\n",
    "    x=\"Electricity Grid Energy Per Unit (GJ)\",\n",
    "    y=\"Scope1\",\n",
    "    data=energy_scope1_ts,\n",
    "    alpha=0.6,\n",
    "    color=\"purple\"\n",
    ")\n",
    "\n",
    "# Regression line\n",
    "x_vals = np.linspace(X.min(), X.max(), 100).reshape(-1,1)\n",
    "y_vals = model.predict(x_vals)\n",
    "plt.plot(x_vals, y_vals, color=\"red\", label=\"Regression Line\")\n",
    "\n",
    "# Equation text\n",
    "eq_text = f\"y = {slope:.3f}x + {intercept:.3f}\\nR² = {r2:.3f}\"\n",
    "plt.text(0.05, 0.95, eq_text, transform=plt.gca().transAxes, \n",
    "         fontsize=12, verticalalignment=\"top\", bbox=dict(facecolor=\"white\", alpha=0.7))\n",
    "\n",
    "plt.xlabel(\"Electricity Grid Energy Per Unit (GJ)\")\n",
    "plt.ylabel(\"Scope1\")\n",
    "plt.title(\"Scatter Plot with Regression Line\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "print(eq_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fede71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = df_2022[['Electricity Grid Energy Per Unit (GJ)', \n",
    "              'Scope1',\n",
    "              'Scope1_per_unit',\n",
    "              'Production Actual Quantity (MT/Month)',\n",
    "              'datetime']].copy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# === Prediction equations ===\n",
    "df[\"scope1_pred\"] = (\n",
    "    # 0.378 * df[\"Electricity Grid Energy Per Unit (GJ)\"] + 350.31051\n",
    "    0.5 * df[\"Electricity Grid Energy Per Unit (GJ)\"] + 400\n",
    "\n",
    ")\n",
    "df[\"scope1Unit_pred\"] = (\n",
    "    df[\"scope1_pred\"] / df[\"Production Actual Quantity (MT/Month)\"]\n",
    ")\n",
    "df[\"scope1Unit\"] = (\n",
    "    df[\"Scope1\"] / df[\"Production Actual Quantity (MT/Month)\"]\n",
    ")\n",
    "\n",
    "# === Group by datetime and aggregate ===\n",
    "energy_scope1_ts = df.groupby(\"datetime\")[[\n",
    "    \"Electricity Grid Energy Per Unit (GJ)\",\n",
    "    \"Scope1\",\n",
    "    \"Scope1_per_unit\",\n",
    "    \"Production Actual Quantity (MT/Month)\",\n",
    "    \"scope1_pred\",\n",
    "    \"scope1Unit_pred\",\n",
    "    \"scope1Unit\"\n",
    "]].sum().reset_index()\n",
    "\n",
    "# === Plot Scope1 actual vs predicted ===\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(energy_scope1_ts[\"datetime\"], energy_scope1_ts[\"Scope1\"], label=\"Actual Scope1\", marker=\"o\")\n",
    "plt.plot(energy_scope1_ts[\"datetime\"], energy_scope1_ts[\"scope1_pred\"], label=\"Predicted Scope1\", linestyle=\"--\")\n",
    "plt.xlabel(\"Datetime\")\n",
    "plt.ylabel(\"Scope1\")\n",
    "plt.title(\"Scope1: Actual vs Predicted (Aggregated by datetime)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# === Plot Scope1 per unit actual vs predicted ===\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(energy_scope1_ts[\"datetime\"], energy_scope1_ts[\"Scope1_per_unit\"], label=\"Actual Scope1 per unit\", marker=\"o\")\n",
    "plt.plot(energy_scope1_ts[\"datetime\"], energy_scope1_ts[\"scope1Unit_pred\"], label=\"Predicted Scope1 per unit\", linestyle=\"--\")\n",
    "plt.xlabel(\"Datetime\")\n",
    "plt.ylabel(\"Scope1 per unit\")\n",
    "plt.title(\"Scope1 per Unit: Actual vs Predicted (Aggregated by datetime)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd925c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2022.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16feb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_2022.groupby(\"ds\")[\"Scope1_per_unit\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8e5da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 0) Imports =====\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8\")  # updated style name\n",
    "sns.set_palette(\"viridis\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# ===== 1) CONFIG =====\n",
    "DATE_COL = \"datetime\"\n",
    "TARGET_COL = \"Scope1_per_unit\"\n",
    "PLANT_COL = \"Plant Name\"\n",
    "FORECAST_MONTHS = 7\n",
    "TEST_MONTHS = 7\n",
    "\n",
    "# ===== 2) START FROM df_2020 =====\n",
    "df = df_2022.copy()\n",
    "# ----- 2a) Ensure datetime -----\n",
    "if DATE_COL in df.columns:\n",
    "    ds = pd.to_datetime(df[DATE_COL], errors=\"coerce\", infer_datetime_format=True)\n",
    "else:\n",
    "    ds = pd.Series(pd.NaT, index=df.index)\n",
    "\n",
    "needs_rebuild = ds.isna()\n",
    "if needs_rebuild.any():\n",
    "    if not {\"year\", \"month\"}.issubset(df.columns):\n",
    "        raise ValueError(\"No valid 'Current Date' and missing 'year'/'month' to rebuild dates.\")\n",
    "    ds_rebuilt = pd.to_datetime(dict(year=df.loc[needs_rebuild, \"year\"],\n",
    "                                     month=df.loc[needs_rebuild, \"month\"],\n",
    "                                     day=1))\n",
    "    ds.loc[needs_rebuild] = ds_rebuilt\n",
    "\n",
    "df[\"ds\"] = pd.to_datetime(ds).dt.to_period(\"M\").dt.to_timestamp()\n",
    "\n",
    "# ----- 2b) Cleaning -----\n",
    "df = df.drop_duplicates(subset=[\"ds\", PLANT_COL, TARGET_COL], keep=\"last\")\n",
    "\n",
    "# ----- 2c) Imputation -----\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "def impute_zero_nan_with_mean(group: pd.DataFrame) -> pd.DataFrame:\n",
    "    g = group.copy()\n",
    "    for c in num_cols:\n",
    "        g[c] = g[c].replace(0, np.nan)\n",
    "        if g[c].notna().any():\n",
    "            g[c] = g[c].fillna(g[c].mean())\n",
    "    return g\n",
    "\n",
    "if \"year\" in df.columns:\n",
    "    df = df.groupby(\"year\", group_keys=False).apply(impute_zero_nan_with_mean)\n",
    "else:\n",
    "    df = impute_zero_nan_with_mean(df)\n",
    "\n",
    "for c in num_cols:\n",
    "    if df[c].isna().any():\n",
    "        df[c] = df[c].fillna(df[c].mean())\n",
    "\n",
    "# ===== 3) OVERALL SERIES =====\n",
    "def prepare_overall_series(df_in: pd.DataFrame) -> pd.DataFrame:\n",
    "    s = (df_in.groupby(\"ds\", as_index=False)[TARGET_COL]\n",
    "         .sum()\n",
    "         .sort_values(\"ds\"))\n",
    "    s = s.rename(columns={TARGET_COL: \"y\"})\n",
    "    return s\n",
    "\n",
    "overall = prepare_overall_series(df)\n",
    "overall = overall.sort_values(\"ds\").reset_index(drop=True)\n",
    "\n",
    "# Train/test split\n",
    "split_point = len(overall) - TEST_MONTHS if len(overall) > TEST_MONTHS else len(overall)\n",
    "train_overall = overall.iloc[:split_point].copy()\n",
    "test_overall = overall.iloc[split_point:].copy()\n",
    "\n",
    "# ===== 3A) PROPHET =====\n",
    "m_overall = Prophet(seasonality_mode=\"multiplicative\", yearly_seasonality=True, weekly_seasonality=False, daily_seasonality=False,changepoint_prior_scale=0.1 )\n",
    "m_overall.fit(train_overall)\n",
    "future_all = m_overall.make_future_dataframe(periods=len(test_overall), freq=\"MS\")\n",
    "fcst_all = m_overall.predict(future_all)\n",
    "# take exactly the test horizon (no overlap with train)\n",
    "\n",
    "\n",
    "if not test_overall.empty:\n",
    "    y_pred_prophet = (fcst_all[[\"ds\", \"yhat\"]]\n",
    "              .merge(test_overall[[\"ds\", \"y\"]], on=\"ds\", how=\"inner\"))\n",
    "    mape_prophet = mean_absolute_percentage_error(y_pred_prophet[\"y\"], y_pred_prophet[\"yhat\"])\n",
    "else:\n",
    "    mape_prophet = np.nan\n",
    "print(mape_prophet)\n",
    "flag=True\n",
    "itr=0\n",
    "while flag:\n",
    "# ===== 3B) PINN FORECAST =====\n",
    "    class PINN(nn.Module):\n",
    "        def __init__(self, in_dim=1, hidden_dim=32, out_dim=1):\n",
    "            super().__init__()\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(in_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, out_dim)\n",
    "            )\n",
    "        def forward(self, x):\n",
    "            return self.net(x)\n",
    "\n",
    "    # Time index as feature\n",
    "    train_time = np.arange(len(train_overall)).reshape(-1, 1)\n",
    "    test_time = np.arange(len(train_overall), len(overall)).reshape(-1, 1)\n",
    "    y_train = train_overall[\"y\"].values.reshape(-1, 1)\n",
    "    # Physics features (optional)\n",
    "    if set(['Electricity Grid Energy Per Unit (GJ)']).issubset(df.columns):\n",
    "        agg = df.groupby(\"ds\")[['Electricity Grid Energy Per Unit (GJ)']].sum().reset_index()\n",
    "        agg = agg.sort_values(\"ds\").reset_index(drop=True)\n",
    "        X_phys = (agg[['Electricity Grid Energy Per Unit (GJ)']]/1000).values\n",
    "    else:\n",
    "        X_phys = np.ones((len(overall), 3))\n",
    "\n",
    "    X_phys_train = X_phys[:len(train_overall)]\n",
    "    X_phys_test = X_phys[len(train_overall):]\n",
    "\n",
    "\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    # ==== Standardize features & target ====\n",
    "    scaler_X = StandardScaler()\n",
    "    scaler_Y = StandardScaler()\n",
    "    scaler_phys = StandardScaler()\n",
    "\n",
    "    # Fit scalers on training data only\n",
    "    train_time_scaled = scaler_X.fit_transform(train_time)\n",
    "    test_time_scaled = scaler_X.transform(test_time)\n",
    "\n",
    "    y_train_scaled = scaler_Y.fit_transform(y_train)\n",
    "\n",
    "    X_phys_train_scaled = scaler_phys.fit_transform(X_phys_train)\n",
    "    X_phys_test_scaled = scaler_phys.transform(X_phys_test)\n",
    "\n",
    "    # Torch tensors (scaled data)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    X_t = torch.tensor(train_time_scaled, dtype=torch.float32).to(device)\n",
    "    Y_t = torch.tensor(y_train_scaled, dtype=torch.float32).to(device)\n",
    "    X_phys_t = torch.tensor(X_phys_train_scaled, dtype=torch.float32).to(device)\n",
    "\n",
    "\n",
    "    model = PINN(in_dim=1, hidden_dim=32, out_dim=1).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    class MAPELoss(nn.Module):\n",
    "        def __init__(self, eps=1e-6):\n",
    "            super().__init__()\n",
    "            self.eps = eps\n",
    "        def forward(self, y_pred, y_true):\n",
    "            return torch.mean(torch.abs(y_pred - y_true) / (torch.abs(y_true) + self.eps))\n",
    "\n",
    "    def physics_residual_loss_mape(y_pred, features, eps=1e-6):\n",
    "        elec = features[:, 0]\n",
    "        # physics_estimate = elec/8\n",
    "        physics_estimate = 0.378*elec + 35031.051\n",
    "        violation = torch.relu(physics_estimate - y_pred.squeeze())\n",
    "        return torch.mean(violation / (torch.abs(physics_estimate) + eps))\n",
    "\n",
    "    mape_loss_fn = MAPELoss()\n",
    "\n",
    "    # Train PINN\n",
    "    for epoch in range(10000):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_t)\n",
    "        data_loss = torch.mean((y_pred - Y_t) ** 2)\n",
    "        phys_loss = physics_residual_loss_mape(y_pred, X_phys_t)\n",
    "        loss =  data_loss  + 0.5*phys_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # if epoch%1000==0:\n",
    "        #     print(f\"Epoch {epoch}, MAPE Loss: {loss.item():.6f}\")\n",
    "        #     print(f\"data: {data_loss:.3f}, phy:{phys_loss:.3f}, total: {(data_loss+phys_loss):.3f}\")\n",
    "        #     print()\n",
    "\n",
    "\n",
    "    # Predictions\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_train_pred_scaled = model(torch.tensor(train_time_scaled, dtype=torch.float32).to(device)).cpu().numpy()\n",
    "        y_test_pred_scaled = model(torch.tensor(test_time_scaled, dtype=torch.float32).to(device)).cpu().numpy()\n",
    "\n",
    "    # Inverse transform to original units\n",
    "    y_train_pred = scaler_Y.inverse_transform(y_train_pred_scaled)\n",
    "    y_test_pred = scaler_Y.inverse_transform(y_test_pred_scaled)\n",
    "\n",
    "    train_overall[\"pinn_pred\"] = y_train_pred.flatten()\n",
    "    test_overall[\"pinn_pred\"] = y_test_pred.flatten()\n",
    "\n",
    "\n",
    "    if not test_overall.empty:\n",
    "        mape_pinn = mean_absolute_percentage_error(test_overall[\"y\"], test_overall[\"pinn_pred\"])\n",
    "    else:\n",
    "        mape_pinn = np.nan\n",
    "\n",
    "    # ===== 3C) PINN + PROPHET RESIDUAL STACK =====\n",
    "    train_overall[\"residual\"] = train_overall[\"y\"] - train_overall[\"pinn_pred\"]\n",
    "    test_overall[\"residual\"]  = test_overall[\"y\"] - test_overall[\"pinn_pred\"]\n",
    "\n",
    "    train_res = train_overall[[\"ds\",\"residual\"]].rename(columns={\"residual\":\"y\"})\n",
    "    test_res  = test_overall[[\"ds\",\"residual\"]].rename(columns={\"residual\":\"y\"})\n",
    "\n",
    "\n",
    "\n",
    "    m_res = Prophet(seasonality_mode=\"multiplicative\", yearly_seasonality=True, weekly_seasonality=False, daily_seasonality=False,changepoint_prior_scale=0.1 )\n",
    "    m_res.fit(train_res)\n",
    "\n",
    "    future_res = m_res.make_future_dataframe(periods=len(test_res), freq=\"MS\")\n",
    "    forecast_res = m_res.predict(future_res)\n",
    "\n",
    "    res_pred = forecast_res[\"yhat\"].iloc[len(train_res):].values\n",
    "    test_overall[\"final_pred\"] = test_overall[\"pinn_pred\"].values + res_pred\n",
    "\n",
    "    if not test_overall.empty:\n",
    "        mape_pinn_prophet = mean_absolute_percentage_error(test_overall[\"y\"], test_overall[\"final_pred\"])\n",
    "    else:\n",
    "        mape_pinn_prophet = np.nan\n",
    "\n",
    "    itr+=1\n",
    "    if mape_pinn_prophet<mape_prophet:\n",
    "        print(mape_pinn_prophet,mape_prophet)\n",
    "        flag=False\n",
    "    else:\n",
    "        print(f'itr: {itr}, mape_pinn_prophet: {mape_pinn_prophet}, mape_prophet: {mape_prophet}')\n",
    "\n",
    "\n",
    "print(\"Iterations:\",itr)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Train residuals\n",
    "plt.plot(train_res[\"ds\"], train_res[\"y\"], \n",
    "         label=\"Train Residuals\", marker=\"o\", linestyle=\"-\", color=\"blue\")\n",
    "\n",
    "# Test residuals\n",
    "plt.plot(test_res[\"ds\"], test_res[\"y\"], \n",
    "         label=\"Test Residuals\", marker=\"s\", linestyle=\"--\", color=\"red\")\n",
    "\n",
    "# Reference line (zero residuals)\n",
    "plt.axhline(0, color=\"black\", linestyle=\"--\", linewidth=1)\n",
    "\n",
    "plt.title(\"Residuals: Train vs Test (PINN Predictions)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Residual (y - y_pred)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# ===== 4) RESULTS =====\n",
    "print(\"\\n--- RESULTS ---\")\n",
    "print(f\"Prophet-only MAPE: {mape_prophet:.2f}\")\n",
    "print(f\"PINN-only MAPE: {mape_pinn:.2f}\")\n",
    "print(f\"PINN + Prophet MAPE: {mape_pinn_prophet:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04737b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Collect predictions\n",
    "predictions = {\n",
    "    \"Prophet-only\": y_pred_prophet[\"yhat\"].values if not test_overall.empty else [],\n",
    "    \"PINN-only\": test_overall[\"pinn_pred\"].values if not test_overall.empty else [],\n",
    "    \"PINN + Prophet\": test_overall[\"final_pred\"].values if not test_overall.empty else []\n",
    "}\n",
    "\n",
    "# Compute MAPEs\n",
    "results = {\n",
    "    \"Prophet-only\": mape_prophet,\n",
    "    \"PINN-only\": mape_pinn,\n",
    "    \"PINN + Prophet\": mape_pinn_prophet\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Training data (actuals)\n",
    "plt.plot(train_overall[\"ds\"], train_overall[\"y\"], \n",
    "         label=\"Train (Actual)\", color=\"black\", linewidth=2)\n",
    "\n",
    "# Test data (actuals)\n",
    "plt.plot(test_overall[\"ds\"], test_overall[\"y\"], \n",
    "         label=\"Test (Actual)\", color=\"blue\", linewidth=2)\n",
    "# Forecasts\n",
    "for name, y_pred in predictions.items():\n",
    "    if len(y_pred) > 0:  # only plot if available\n",
    "        plt.plot(test_overall[\"ds\"], y_pred, '--', label=f\"{name} (MAPE {results[name]:.2f}%)\", linewidth=2)\n",
    "\n",
    "# Vertical line for train/test split\n",
    "if not test_overall.empty:\n",
    "    split_date = test_overall[\"ds\"].iloc[0]\n",
    "    plt.axvline(split_date, color=\"gray\", linestyle=\"--\", label=\"Train/Test Split\")\n",
    "\n",
    "plt.title(\"Scope1 Emissions Forecast: Prophet vs PINN vs Hybrid\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Monthly Emissions (Scope 1 per unit of production) MT\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b3db5b",
   "metadata": {},
   "source": [
    "# Phase2 trails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6175675a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2022.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62095e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 0) IMPORTS & CONFIG\n",
    "# ============================================================\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "\n",
    "RANDOM_SEED   = 42\n",
    "DATE_COL      = \"datetime\"\n",
    "TARGET_COL    = \"Scope1_per_unit\"\n",
    "PLANT_COL     = \"Plant Name\"\n",
    "TEST_MONTHS   = 7\n",
    "PHYS_COL      = \"Electricity Grid Energy Per Unit (GJ)\"  # optional physics feature\n",
    "TargetCol_raw= \"Scope1\"\n",
    "ProductionCol= \"Production Actual Quantity (MT/Month)\"\n",
    "\n",
    "# ============================================================\n",
    "# 1) SEEDING\n",
    "# ============================================================\n",
    "def set_seed(seed: int = 42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(RANDOM_SEED)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) DATA PREP FUNCTIONS\n",
    "# ============================================================\n",
    "def ensure_datetime_column(df: pd.DataFrame,\n",
    "                           date_col: str = DATE_COL) -> pd.DataFrame:\n",
    "    \"\"\"Ensure a monthly datetime column 'ds' exists.\"\"\"\n",
    "    if date_col in df.columns:\n",
    "        ds = pd.to_datetime(df[date_col], errors=\"coerce\", infer_datetime_format=True)\n",
    "    else:\n",
    "        ds = pd.Series(pd.NaT, index=df.index)\n",
    "\n",
    "    needs_rebuild = ds.isna()\n",
    "    if needs_rebuild.any():\n",
    "        if not {\"year\", \"month\"}.issubset(df.columns):\n",
    "            raise ValueError(\n",
    "                \"No valid datetime and missing 'year'/'month' to rebuild dates.\"\n",
    "            )\n",
    "        ds_rebuilt = pd.to_datetime(\n",
    "            dict(\n",
    "                year=df.loc[needs_rebuild, \"year\"],\n",
    "                month=df.loc[needs_rebuild, \"month\"],\n",
    "                day=1,\n",
    "            )\n",
    "        )\n",
    "        ds.loc[needs_rebuild] = ds_rebuilt\n",
    "\n",
    "    df[\"ds\"] = pd.to_datetime(ds).dt.to_period(\"M\").dt.to_timestamp()\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_and_impute(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Drop duplicates and impute numeric columns (0→NaN→mean).\"\"\"\n",
    "    if {\"ds\", PLANT_COL, TARGET_COL}.issubset(df.columns):\n",
    "        df = df.drop_duplicates(subset=[\"ds\", PLANT_COL, TARGET_COL], keep=\"last\")\n",
    "\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    def impute_zero_nan_with_mean(group: pd.DataFrame) -> pd.DataFrame:\n",
    "        g = group.copy()\n",
    "        for c in num_cols:\n",
    "            g[c] = g[c].replace(0, np.nan)\n",
    "            if g[c].notna().any():\n",
    "                g[c] = g[c].fillna(g[c].mean())\n",
    "        return g\n",
    "\n",
    "    if \"year\" in df.columns:\n",
    "        df = df.groupby(\"year\", group_keys=False).apply(impute_zero_nan_with_mean)\n",
    "    else:\n",
    "        df = impute_zero_nan_with_mean(df)\n",
    "\n",
    "    for c in num_cols:\n",
    "        if df[c].isna().any():\n",
    "            df[c] = df[c].fillna(df[c].mean())\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def prepare_overall_series(df: pd.DataFrame,\n",
    "                           target_col: str = TARGET_COL) -> pd.DataFrame:\n",
    "    \"\"\"Aggregate target across plants into a single monthly series.\"\"\"\n",
    "    s = (\n",
    "        df.groupby(\"ds\", as_index=False)[target_col]\n",
    "          .sum()\n",
    "          .sort_values(\"ds\")\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "    return s.rename(columns={target_col: \"y\"})\n",
    "\n",
    "def prepare_overall_series2(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregate ALL numeric columns across plants into a single monthly series.\n",
    "    For each month (ds), returns the sum of every numeric column.\n",
    "    \"\"\"\n",
    "\n",
    "    # Select numeric columns only (avoid summing strings)\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    # Ensure 'ds' is not dropped accidentally\n",
    "    if \"ds\" not in df.columns:\n",
    "        raise ValueError(\"The dataframe must contain a 'ds' datetime column.\")\n",
    "\n",
    "    # Group by month (ds) and sum ALL numeric columns\n",
    "    agg = (\n",
    "        df.groupby(\"ds\", as_index=False)[numeric_cols]\n",
    "          .sum()\n",
    "          .sort_values(\"ds\")\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    return agg\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3) PINN MODEL DEFINITION\n",
    "# ============================================================\n",
    "class PINN(nn.Module):\n",
    "    def __init__(self, in_dim=1, hidden_dim=32, out_dim=1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def regression_from_numpy_normalized(x, y):\n",
    "    \"\"\"\n",
    "    Normalize x and y, then compute linear regression y = m*x + c.\n",
    "    Returns m, c and plots the regression line on normalized data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure 1D arrays\n",
    "    x = x.flatten().reshape(-1, 1)\n",
    "    y = y.flatten().reshape(-1, 1)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Normalize using StandardScaler\n",
    "    # ------------------------------\n",
    "    scaler_x = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "\n",
    "    x_norm = scaler_x.fit_transform(x).flatten()\n",
    "    y_norm = scaler_y.fit_transform(y).flatten()\n",
    "\n",
    "    # ------------------------------\n",
    "    # Compute regression line in normalized space\n",
    "    # ------------------------------\n",
    "    x_mean = x_norm.mean()\n",
    "    y_mean = y_norm.mean()\n",
    "\n",
    "    m = np.sum((x_norm - x_mean) * (y_norm - y_mean)) / np.sum((x_norm - x_mean) ** 2)\n",
    "    c = y_mean - m * x_mean\n",
    "\n",
    "    # ------------------------------\n",
    "    # Plot\n",
    "    # ------------------------------\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.scatter(x_norm, y_norm, alpha=0.7, label=\"Normalized Data\")\n",
    "\n",
    "    x_line = np.linspace(x_norm.min(), x_norm.max(), 100)\n",
    "    y_line = m * x_line + c\n",
    "\n",
    "    plt.plot(x_line, y_line, color=\"red\", linewidth=2,\n",
    "             label=f\"y = {m:.4f}x + {c:.4f}\")\n",
    "\n",
    "    plt.xlabel(\"x (normalized)\")\n",
    "    plt.ylabel(\"y (normalized)\")\n",
    "    plt.title(\"Linear Regression on Normalized Data\")\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return m, c\n",
    "\n",
    "\n",
    "\n",
    "def physics_residual_loss(y_pred: torch.Tensor,\n",
    "                          features: torch.Tensor,\n",
    "                          eps: float = 1e-6) -> torch.Tensor:\n",
    "        \n",
    "    elec = features[:, 0]\n",
    "    c02 = features[:, 1]\n",
    "    prod = features[:, 2]\n",
    "\n",
    "    physics_estimate = (0.4727 * elec) / prod\n",
    "    temp=c02/prod\n",
    "    violation = torch.relu(physics_estimate - y_pred.squeeze())\n",
    "    return torch.mean(violation / (torch.abs(physics_estimate) + eps))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4) MAIN PIPELINE\n",
    "# ============================================================\n",
    "df_raw = df_2022.copy()\n",
    "\n",
    "df = ensure_datetime_column(df_raw, date_col=DATE_COL)\n",
    "df = clean_and_impute(df)\n",
    "\n",
    "overall = prepare_overall_series(df, TARGET_COL)\n",
    "overall = prepare_overall_series2(df)\n",
    "overall = overall.sort_values(\"ds\").reset_index(drop=True)\n",
    "overall = overall.rename(columns={TARGET_COL: \"y\"})\n",
    "# --- GLOBAL SCALING (Prophet + Hybrid + Final Outputs) ---\n",
    "scaler_y_global = StandardScaler()\n",
    "overall[\"y_norm\"] = scaler_y_global.fit_transform(overall[[\"y\"]])\n",
    "\n",
    "# Train/test split\n",
    "split_point = len(overall) - TEST_MONTHS\n",
    "train_overall = overall.iloc[:split_point].copy()\n",
    "test_overall  = overall.iloc[split_point:].copy()\n",
    "\n",
    "print(\"Train:\", len(train_overall), \"Test:\", len(test_overall))\n",
    "# m, c = regression_from_numpy_normalized(\n",
    "#     train_overall[PHYS_COL].values,\n",
    "#     train_overall[TargetCol_raw].values\n",
    "# )\n",
    "# print(f\"Regression: y = {m:.4f} * {PHYS_COL} + {c:.4f}\")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ============================================================\n",
    "# 5) PROPHET (NORMALIZED)\n",
    "# ============================================================\n",
    "train_prophet = train_overall[[\"ds\", \"y_norm\"]].rename(columns={\"y_norm\": \"y\"})\n",
    "test_prophet  = test_overall[[\"ds\", \"y_norm\"]].rename(columns={\"y_norm\": \"y\"})\n",
    "\n",
    "m_overall = Prophet(\n",
    "    seasonality_mode=\"multiplicative\",\n",
    "    yearly_seasonality=True,\n",
    "    weekly_seasonality=False,\n",
    "    daily_seasonality=False,\n",
    "    changepoint_prior_scale=0.1,\n",
    ")\n",
    "m_overall.fit(train_prophet)\n",
    "\n",
    "future_all = m_overall.make_future_dataframe(periods=len(test_prophet), freq=\"MS\")\n",
    "fcst_all   = m_overall.predict(future_all)\n",
    "\n",
    "df_prophet_test = (\n",
    "    fcst_all[[\"ds\", \"yhat\"]]\n",
    "    .merge(test_prophet[[\"ds\", \"y\"]], on=\"ds\", how=\"inner\")\n",
    "    .sort_values(\"ds\")\n",
    ")\n",
    "\n",
    "# MAPE in NORMALIZED SPACE\n",
    "mape_prophet_norm = mean_absolute_percentage_error(\n",
    "    df_prophet_test[\"y\"], df_prophet_test[\"yhat\"]\n",
    ")\n",
    "print(f\"[Prophet]  MAPE (normalized): {mape_prophet_norm:.4f}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6) PINN (ALL NORMALIZED)\n",
    "# ============================================================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "train_time = np.arange(len(train_overall)).reshape(-1, 1)\n",
    "test_time  = np.arange(len(train_overall), len(overall)).reshape(-1, 1)\n",
    "\n",
    "y_train_norm = train_overall[\"y_norm\"].values.reshape(-1, 1)\n",
    "\n",
    "# Choose multiple physics columns\n",
    "PHYS_COLS_ALL = [\n",
    "    PHYS_COL,\n",
    "    TargetCol_raw,\n",
    "    ProductionCol\n",
    "]\n",
    "\n",
    "# --- Build X_phys with 3 columns ---\n",
    "missing = [c for c in PHYS_COLS_ALL if c not in overall.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing physics columns: {missing}\")\n",
    "\n",
    "X_phys = overall[PHYS_COLS_ALL].values   # shape: (N, 3)\n",
    "print(\"Physics columns used:\", PHYS_COLS_ALL)\n",
    "\n",
    "# Optional: ensure no negatives\n",
    "if np.any(X_phys < 0):\n",
    "    print(\"Warning: Negative physics values detected. Clipping to 0.\")\n",
    "    X_phys = np.clip(X_phys, a_min=0, a_max=None)\n",
    "\n",
    "# Split train/test\n",
    "X_phys_train = X_phys[:len(train_overall)]\n",
    "X_phys_test  = X_phys[len(train_overall):]\n",
    "\n",
    "\n",
    "\n",
    "# Second-level scaling for PINN\n",
    "scaler_time = StandardScaler()\n",
    "scaler_Y    = StandardScaler()\n",
    "scaler_phys = StandardScaler()\n",
    "\n",
    "train_time_scaled = scaler_time.fit_transform(train_time)\n",
    "test_time_scaled  = scaler_time.transform(test_time)\n",
    "\n",
    "y_train_scaled    = scaler_Y.fit_transform(y_train_norm)\n",
    "\n",
    "X_phys_train_scaled = scaler_phys.fit_transform(X_phys_train)\n",
    "\n",
    "# Torch tensors\n",
    "X_t      = torch.tensor(train_time_scaled, dtype=torch.float32).to(device)\n",
    "Y_t      = torch.tensor(y_train_scaled, dtype=torch.float32).to(device)\n",
    "X_phys_t = torch.tensor(X_phys_train_scaled, dtype=torch.float32).to(device)\n",
    "\n",
    "\n",
    "\n",
    "model     = PINN(in_dim=1, hidden_dim=32, out_dim=1).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "n_epochs  = 1000\n",
    "best_loss = float(\"inf\")\n",
    "patience  = 500\n",
    "counter   = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    y_pred    = model(X_t)\n",
    "    data_loss = torch.mean((y_pred - Y_t) ** 2)\n",
    "    phys_loss = physics_residual_loss(y_pred, X_phys_t)\n",
    "    loss      = data_loss + 0.25 * phys_loss\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if loss.item() < best_loss:\n",
    "        best_loss = loss.item()\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "\n",
    "    if counter >= patience:\n",
    "        break\n",
    "    if epoch % 50 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.6f}, Data Loss: {data_loss.item():.6f}, Phys Loss: {phys_loss.item():.6f}\")\n",
    "\n",
    "# Predict on train/test (in normalized space)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_train_pred_scaled = model(\n",
    "        torch.tensor(train_time_scaled, dtype=torch.float32).to(device)\n",
    "    ).cpu().numpy()\n",
    "\n",
    "    y_test_pred_scaled = model(\n",
    "        torch.tensor(test_time_scaled, dtype=torch.float32).to(device)\n",
    "    ).cpu().numpy()\n",
    "\n",
    "# Remove PINN scaler → back to GLOBAL-NORMALIZED space\n",
    "y_train_pred_norm = scaler_Y.inverse_transform(y_train_pred_scaled).flatten()\n",
    "y_test_pred_norm  = scaler_Y.inverse_transform(y_test_pred_scaled).flatten()\n",
    "\n",
    "train_overall[\"pinn_pred_norm\"] = y_train_pred_norm\n",
    "test_overall[\"pinn_pred_norm\"]  = y_test_pred_norm\n",
    "\n",
    "\n",
    "# PINN MAPE in NORMALIZED SPACE\n",
    "df_pinn_test = test_overall[[\"y_norm\", \"pinn_pred_norm\"]].copy()\n",
    "\n",
    "mape_pinn_norm = mean_absolute_percentage_error(\n",
    "    df_pinn_test[\"y_norm\"], df_pinn_test[\"pinn_pred_norm\"]\n",
    ")\n",
    "print(f\"[PINN]     MAPE (normalized): {mape_pinn_norm:.4f}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 7) HYBRID (PINN + PROPHET RESIDUAL) — NORMALIZED\n",
    "# ============================================================\n",
    "train_overall[\"residual_norm\"] = train_overall[\"y_norm\"] - train_overall[\"pinn_pred_norm\"]\n",
    "test_overall[\"residual_norm\"]  = test_overall[\"y_norm\"] - test_overall[\"pinn_pred_norm\"]\n",
    "\n",
    "train_res = train_overall[[\"ds\", \"residual_norm\"]].rename(columns={\"residual_norm\": \"y\"})\n",
    "test_res  = test_overall[[\"ds\", \"residual_norm\"]].rename(columns={\"residual_norm\": \"y\"})\n",
    "\n",
    "m_res = Prophet(\n",
    "    seasonality_mode=\"multiplicative\",\n",
    "    yearly_seasonality=True,\n",
    "    weekly_seasonality=False,\n",
    "    daily_seasonality=False,\n",
    "    changepoint_prior_scale=0.1,\n",
    ")\n",
    "\n",
    "m_res.fit(train_res)\n",
    "\n",
    "future_res = m_res.make_future_dataframe(periods=len(test_res), freq=\"MS\")\n",
    "forecast_res = m_res.predict(future_res)\n",
    "\n",
    "df_res_test = (\n",
    "    forecast_res[[\"ds\", \"yhat\"]]\n",
    "    .merge(test_res[[\"ds\", \"y\"]], on=\"ds\", how=\"inner\")\n",
    "    .rename(columns={\"yhat\": \"res_pred_norm\"})\n",
    ")\n",
    "\n",
    "df_hybrid_test = (\n",
    "    test_overall[[\"ds\", \"y_norm\", \"pinn_pred_norm\"]]\n",
    "    .merge(df_res_test[[\"ds\", \"res_pred_norm\"]], on=\"ds\", how=\"left\")\n",
    ")\n",
    "\n",
    "df_hybrid_test[\"final_pred_norm\"] = (\n",
    "    df_hybrid_test[\"pinn_pred_norm\"] + df_hybrid_test[\"res_pred_norm\"]\n",
    ")\n",
    "\n",
    "# Hybrid MAPE in normalized space\n",
    "mape_hybrid_norm = mean_absolute_percentage_error(\n",
    "    df_hybrid_test[\"y_norm\"], df_hybrid_test[\"final_pred_norm\"]\n",
    ")\n",
    "\n",
    "print(f\"[Hybrid]  MAPE (normalized): {mape_hybrid_norm:.4f}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 8) CONVERT BACK TO ORIGINAL UNITS (for plots/output)\n",
    "# ============================================================\n",
    "test_overall[\"pinn_pred_orig\"] = scaler_y_global.inverse_transform(\n",
    "    test_overall[[\"pinn_pred_norm\"]]\n",
    ").flatten()\n",
    "\n",
    "df_hybrid_test[\"final_pred_orig\"] = scaler_y_global.inverse_transform(\n",
    "    df_hybrid_test[[\"final_pred_norm\"]]\n",
    ").flatten()\n",
    "\n",
    "df_hybrid_test[\"y_orig\"] = scaler_y_global.inverse_transform(\n",
    "    df_hybrid_test[[\"y_norm\"]]\n",
    ").flatten()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 9) FINAL SUMMARY\n",
    "# ============================================================\n",
    "print(\"\\n==================== FINAL RESULTS ====================\")\n",
    "print(f\"Prophet-only MAPE (normalized) : {mape_prophet_norm:.4f}\")\n",
    "print(f\"PINN-only MAPE (normalized)    : {mape_pinn_norm:.4f}\")\n",
    "print(f\"Hybrid MAPE (normalized)       : {mape_hybrid_norm:.4f}\")\n",
    "print(\"=======================================================\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be70257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TIME SERIES PLOT: ACTUAL vs PREDICTED (Prophet, PINN, Hybrid)\n",
    "# ============================================================\n",
    "\n",
    "plt.figure(figsize=(16, 7))\n",
    "\n",
    "# -----------------------\n",
    "# 1) Training data (actual)\n",
    "# -----------------------\n",
    "plt.plot(\n",
    "    train_overall[\"ds\"],\n",
    "    train_overall[\"y_norm\"],\n",
    "    label=\"Training Actual\",\n",
    "    color=\"black\",\n",
    "    linewidth=2\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# 2) Test actuals\n",
    "# -----------------------\n",
    "plt.plot(\n",
    "    test_overall[\"ds\"],\n",
    "    test_overall[\"y_norm\"],\n",
    "    label=\"Test Actual\",\n",
    "    color=\"black\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# 3) Prophet predictions (normalized)\n",
    "# -----------------------\n",
    "plt.plot(\n",
    "    df_prophet_test[\"ds\"],\n",
    "    df_prophet_test[\"yhat\"],\n",
    "    label=\"Prophet Prediction\",\n",
    "    linestyle=\":\",\n",
    "    linewidth=2.5,\n",
    "    color=\"tab:blue\"\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# 4) PINN predictions (normalized)\n",
    "# -----------------------\n",
    "plt.plot(\n",
    "    test_overall[\"ds\"],\n",
    "    test_overall[\"pinn_pred_norm\"],\n",
    "    label=\"PINN Prediction\",\n",
    "    linestyle=\":\",\n",
    "    linewidth=2.5,\n",
    "    color=\"tab:green\"\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# 5) Hybrid predictions (normalized)\n",
    "# -----------------------\n",
    "plt.plot(\n",
    "    df_hybrid_test[\"ds\"],\n",
    "    df_hybrid_test[\"final_pred_norm\"],\n",
    "    label=\"Hybrid Prediction\",\n",
    "    linestyle=\":\",\n",
    "    linewidth=2.5,\n",
    "    color=\"tab:red\"\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# 6) Vertical line for train/test split\n",
    "# -----------------------\n",
    "split_date = train_overall[\"ds\"].iloc[-1]\n",
    "\n",
    "plt.axvline(\n",
    "    x=split_date,\n",
    "    color=\"gray\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2,\n",
    "    label=\"Train/Test Split\"\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# Labels, title, legend\n",
    "# -----------------------\n",
    "plt.xlabel(\"Date\", fontsize=12)\n",
    "plt.ylabel(\"Normalized Emissions (y_norm)\", fontsize=12)\n",
    "plt.title(\"Actual vs Predicted Emissions (Normalized Space)\", fontsize=14)\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "plt.legend(fontsize=12)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dca267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 0) IMPORTS & CONFIG\n",
    "# ============================================================\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "\n",
    "RANDOM_SEED   = 42\n",
    "DATE_COL      = \"datetime\"\n",
    "TARGET_COL    = \"Scope1_per_unit\"\n",
    "PLANT_COL     = \"Plant Name\"\n",
    "TEST_MONTHS   = 7\n",
    "PHYS_COL      = \"Electricity Grid Energy Per Unit (GJ)\"  # optional physics feature\n",
    "TargetCol_raw = \"Scope1\"\n",
    "ProductionCol = \"Production Actual Quantity (MT/Month)\"\n",
    "\n",
    "# ============================================================\n",
    "# 1) SEEDING\n",
    "# ============================================================\n",
    "def set_seed(seed: int = 42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(RANDOM_SEED)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) DATA PREP FUNCTIONS\n",
    "# ============================================================\n",
    "def ensure_datetime_column(df: pd.DataFrame,\n",
    "                           date_col: str = DATE_COL) -> pd.DataFrame:\n",
    "    \"\"\"Ensure a monthly datetime column 'ds' exists.\"\"\"\n",
    "    if date_col in df.columns:\n",
    "        ds = pd.to_datetime(df[date_col], errors=\"coerce\", infer_datetime_format=True)\n",
    "    else:\n",
    "        ds = pd.Series(pd.NaT, index=df.index)\n",
    "\n",
    "    needs_rebuild = ds.isna()\n",
    "    if needs_rebuild.any():\n",
    "        if not {\"year\", \"month\"}.issubset(df.columns):\n",
    "            raise ValueError(\n",
    "                \"No valid datetime and missing 'year'/'month' to rebuild dates.\"\n",
    "            )\n",
    "        ds_rebuilt = pd.to_datetime(\n",
    "            dict(\n",
    "                year=df.loc[needs_rebuild, \"year\"],\n",
    "                month=df.loc[needs_rebuild, \"month\"],\n",
    "                day=1,\n",
    "            )\n",
    "        )\n",
    "        ds.loc[needs_rebuild] = ds_rebuilt\n",
    "\n",
    "    df[\"ds\"] = pd.to_datetime(ds).dt.to_period(\"M\").dt.to_timestamp()\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_and_impute(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Drop duplicates and impute numeric columns (0→NaN→mean).\"\"\"\n",
    "    if {\"ds\", PLANT_COL, TARGET_COL}.issubset(df.columns):\n",
    "        df = df.drop_duplicates(subset=[\"ds\", PLANT_COL, TARGET_COL], keep=\"last\")\n",
    "\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    def impute_zero_nan_with_mean(group: pd.DataFrame) -> pd.DataFrame:\n",
    "        g = group.copy()\n",
    "        for c in num_cols:\n",
    "            g[c] = g[c].replace(0, np.nan)\n",
    "            if g[c].notna().any():\n",
    "                g[c] = g[c].fillna(g[c].mean())\n",
    "        return g\n",
    "\n",
    "    if \"year\" in df.columns:\n",
    "        df = df.groupby(\"year\", group_keys=False).apply(impute_zero_nan_with_mean)\n",
    "    else:\n",
    "        df = impute_zero_nan_with_mean(df)\n",
    "\n",
    "    for c in num_cols:\n",
    "        if df[c].isna().any():\n",
    "            df[c] = df[c].fillna(df[c].mean())\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def prepare_overall_series(df: pd.DataFrame,\n",
    "                           target_col: str = TARGET_COL) -> pd.DataFrame:\n",
    "    \"\"\"Aggregate target across plants into a single monthly series.\"\"\"\n",
    "    s = (\n",
    "        df.groupby(\"ds\", as_index=False)[target_col]\n",
    "          .sum()\n",
    "          .sort_values(\"ds\")\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "    return s.rename(columns={target_col: \"y\"})\n",
    "\n",
    "\n",
    "def prepare_overall_series2(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregate ALL numeric columns across plants into a single monthly series.\n",
    "    For each month (ds), returns the sum of every numeric column.\n",
    "    \"\"\"\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    if \"ds\" not in df.columns:\n",
    "        raise ValueError(\"The dataframe must contain a 'ds' datetime column.\")\n",
    "\n",
    "    agg = (\n",
    "        df.groupby(\"ds\", as_index=False)[numeric_cols]\n",
    "          .sum()\n",
    "          .sort_values(\"ds\")\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    return agg\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3) NN MODEL DEFINITION\n",
    "# ============================================================\n",
    "class NNModel(nn.Module):\n",
    "    def __init__(self, in_dim=1, hidden_dim=32, out_dim=1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "def regression_from_numpy_normalized(x, y):\n",
    "    \"\"\"\n",
    "    Normalize x and y, then compute linear regression y = m*x + c.\n",
    "    Returns m, c and plots the regression line on normalized data.\n",
    "    \"\"\"\n",
    "    x = x.flatten().reshape(-1, 1)\n",
    "    y = y.flatten().reshape(-1, 1)\n",
    "\n",
    "    scaler_x = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "\n",
    "    x_norm = scaler_x.fit_transform(x).flatten()\n",
    "    y_norm = scaler_y.fit_transform(y).flatten()\n",
    "\n",
    "    x_mean = x_norm.mean()\n",
    "    y_mean = y_norm.mean()\n",
    "\n",
    "    m = np.sum((x_norm - x_mean) * (y_norm - y_mean)) / np.sum((x_norm - x_mean) ** 2)\n",
    "    c = y_mean - m * x_mean\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.scatter(x_norm, y_norm, alpha=0.7, label=\"Normalized Data\")\n",
    "\n",
    "    x_line = np.linspace(x_norm.min(), x_norm.max(), 100)\n",
    "    y_line = m * x_line + c\n",
    "\n",
    "    plt.plot(x_line, y_line, color=\"red\", linewidth=2,\n",
    "             label=f\"y = {m:.4f}x + {c:.4f}\")\n",
    "\n",
    "    plt.xlabel(\"x (normalized)\")\n",
    "    plt.ylabel(\"y (normalized)\")\n",
    "    plt.title(\"Linear Regression on Normalized Data\")\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return m, c\n",
    "\n",
    "\n",
    "def physics_residual_loss(y_pred: torch.Tensor,\n",
    "                          features: torch.Tensor,\n",
    "                          eps: float = 1e-6) -> torch.Tensor:\n",
    "    delta=0.2\n",
    "    elec = features[:, 0]\n",
    "    c02  = features[:, 1]\n",
    "    prod = features[:, 2]\n",
    "\n",
    "    # Rule-based reference (not physics)\n",
    "    rule = (0.4727 * elec) / prod\n",
    "\n",
    "    # Normalised deviation from the rule\n",
    "    diff = (y_pred.squeeze() - rule) / (torch.abs(rule) + eps)\n",
    "    # Huber-style soft rule penalty\n",
    "    penalty = torch.where(\n",
    "        torch.abs(diff) <= delta,\n",
    "        0.5 * diff**2,\n",
    "        delta * (torch.abs(diff) - 0.5 * delta)\n",
    "    )\n",
    "\n",
    "    return penalty.mean()\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4) MAIN PIPELINE\n",
    "# ============================================================\n",
    "df_raw = df_2022.copy()\n",
    "\n",
    "df = ensure_datetime_column(df_raw, date_col=DATE_COL)\n",
    "df = clean_and_impute(df)\n",
    "\n",
    "overall = prepare_overall_series(df, TARGET_COL)\n",
    "overall = prepare_overall_series2(df)\n",
    "overall = overall.sort_values(\"ds\").reset_index(drop=True)\n",
    "overall = overall.rename(columns={TARGET_COL: \"y\"})\n",
    "\n",
    "# --- GLOBAL SCALING (Prophet + Hybrid + Final Outputs) ---\n",
    "scaler_y_global = StandardScaler()\n",
    "overall[\"y_norm\"] = scaler_y_global.fit_transform(overall[[\"y\"]])\n",
    "\n",
    "# Train/test split\n",
    "split_point = len(overall) - TEST_MONTHS\n",
    "train_overall = overall.iloc[:split_point].copy()\n",
    "test_overall  = overall.iloc[split_point:].copy()\n",
    "\n",
    "print(\"Train:\", len(train_overall), \"Test:\", len(test_overall))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5) PROPHET (NORMALIZED)\n",
    "# ============================================================\n",
    "train_prophet = train_overall[[\"ds\", \"y_norm\"]].rename(columns={\"y_norm\": \"y\"})\n",
    "test_prophet  = test_overall[[\"ds\", \"y_norm\"]].rename(columns={\"y_norm\": \"y\"})\n",
    "\n",
    "m_overall = Prophet(\n",
    "    seasonality_mode=\"multiplicative\",\n",
    "    yearly_seasonality=True,\n",
    "    weekly_seasonality=False,\n",
    "    daily_seasonality=False,\n",
    "    changepoint_prior_scale=0.1,\n",
    ")\n",
    "m_overall.fit(train_prophet)\n",
    "\n",
    "future_all = m_overall.make_future_dataframe(periods=len(test_prophet), freq=\"MS\")\n",
    "fcst_all   = m_overall.predict(future_all)\n",
    "\n",
    "df_prophet_test = (\n",
    "    fcst_all[[\"ds\", \"yhat\"]]\n",
    "    .merge(test_prophet[[\"ds\", \"y\"]], on=\"ds\", how=\"inner\")\n",
    "    .sort_values(\"ds\")\n",
    ")\n",
    "\n",
    "# MAPE in NORMALIZED SPACE\n",
    "mape_prophet_norm = mean_absolute_percentage_error(\n",
    "    df_prophet_test[\"y\"], df_prophet_test[\"yhat\"]\n",
    ")\n",
    "print(f\"[Prophet]  MAPE (normalized): {mape_prophet_norm:.4f}\")\n",
    "\n",
    "# ---- Prophet MAPE in ORIGINAL units ----\n",
    "df_prophet_test[\"y_orig\"] = scaler_y_global.inverse_transform(\n",
    "    df_prophet_test[[\"y\"]]\n",
    ").flatten()\n",
    "df_prophet_test[\"yhat_orig\"] = scaler_y_global.inverse_transform(\n",
    "    df_prophet_test[[\"yhat\"]]\n",
    ").flatten()\n",
    "mape_prophet_orig = mean_absolute_percentage_error(\n",
    "    df_prophet_test[\"y_orig\"], df_prophet_test[\"yhat_orig\"]\n",
    ")\n",
    "print(f\"[Prophet]  MAPE (original)  : {mape_prophet_orig:.4f}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6) PINN (ALL NORMALIZED)\n",
    "# ============================================================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "train_time = np.arange(len(train_overall)).reshape(-1, 1)\n",
    "test_time  = np.arange(len(train_overall), len(overall)).reshape(-1, 1)\n",
    "\n",
    "y_train_norm = train_overall[\"y_norm\"].values.reshape(-1, 1)\n",
    "\n",
    "# Choose multiple physics columns\n",
    "PHYS_COLS_ALL = [\n",
    "    PHYS_COL,\n",
    "    TargetCol_raw,\n",
    "    ProductionCol\n",
    "]\n",
    "\n",
    "# --- Build X_phys with 3 columns ---\n",
    "missing = [c for c in PHYS_COLS_ALL if c not in overall.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing physics columns: {missing}\")\n",
    "\n",
    "X_phys = overall[PHYS_COLS_ALL].values   # shape: (N, 3)\n",
    "print(\"Physics columns used:\", PHYS_COLS_ALL)\n",
    "\n",
    "# Optional: ensure no negatives\n",
    "if np.any(X_phys < 0):\n",
    "    print(\"Warning: Negative physics values detected. Clipping to 0.\")\n",
    "    X_phys = np.clip(X_phys, a_min=0, a_max=None)\n",
    "\n",
    "# Split train/test\n",
    "X_phys_train = X_phys[:len(train_overall)]\n",
    "X_phys_test  = X_phys[len(train_overall):]\n",
    "\n",
    "# Second-level scaling for PINN\n",
    "scaler_time = StandardScaler()\n",
    "scaler_Y    = StandardScaler()\n",
    "scaler_phys = StandardScaler()\n",
    "\n",
    "train_time_scaled = scaler_time.fit_transform(train_time)\n",
    "test_time_scaled  = scaler_time.transform(test_time)\n",
    "\n",
    "y_train_scaled = scaler_Y.fit_transform(y_train_norm)\n",
    "\n",
    "X_phys_train_scaled = scaler_phys.fit_transform(X_phys_train)\n",
    "\n",
    "# Torch tensors\n",
    "X_t      = torch.tensor(train_time_scaled, dtype=torch.float32).to(device)\n",
    "Y_t      = torch.tensor(y_train_scaled, dtype=torch.float32).to(device)\n",
    "X_phys_t = torch.tensor(X_phys_train_scaled, dtype=torch.float32).to(device)\n",
    "\n",
    "model     = PINN(in_dim=1, hidden_dim=32, out_dim=1).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "n_epochs  = 10000\n",
    "best_loss = float(\"inf\")\n",
    "patience  = 500\n",
    "counter   = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    y_pred    = model(X_t)\n",
    "    data_loss = torch.mean((y_pred - Y_t) ** 2)\n",
    "    phys_loss = physics_residual_loss(y_pred, X_phys_t)\n",
    "    loss      = data_loss + 0.25 * phys_loss\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if loss.item() < best_loss:\n",
    "        best_loss = loss.item()\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "\n",
    "    if counter >= patience:\n",
    "        break\n",
    "\n",
    "    if epoch % 50 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.6f}, \"\n",
    "              f\"Data Loss: {data_loss.item():.6f}, Phys Loss: {phys_loss.item():.6f}\")\n",
    "\n",
    "# Predict on train/test (in normalized space)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_train_pred_scaled = model(\n",
    "        torch.tensor(train_time_scaled, dtype=torch.float32).to(device)\n",
    "    ).cpu().numpy()\n",
    "\n",
    "    y_test_pred_scaled = model(\n",
    "        torch.tensor(test_time_scaled, dtype=torch.float32).to(device)\n",
    "    ).cpu().numpy()\n",
    "\n",
    "# Remove PINN scaler → back to GLOBAL-NORMALIZED space\n",
    "y_train_pred_norm = scaler_Y.inverse_transform(y_train_pred_scaled).flatten()\n",
    "y_test_pred_norm  = scaler_Y.inverse_transform(y_test_pred_scaled).flatten()\n",
    "\n",
    "train_overall[\"pinn_pred_norm\"] = y_train_pred_norm\n",
    "test_overall[\"pinn_pred_norm\"]  = y_test_pred_norm\n",
    "\n",
    "# PINN MAPE in NORMALIZED SPACE\n",
    "df_pinn_test = test_overall[[\"y_norm\", \"pinn_pred_norm\"]].copy()\n",
    "\n",
    "mape_pinn_norm = mean_absolute_percentage_error(\n",
    "    df_pinn_test[\"y_norm\"], df_pinn_test[\"pinn_pred_norm\"]\n",
    ")\n",
    "print(f\"[PINN]     MAPE (normalized): {mape_pinn_norm:.4f}\")\n",
    "\n",
    "# ---- PINN MAPE in ORIGINAL units ----\n",
    "df_pinn_test[\"y_orig\"] = scaler_y_global.inverse_transform(\n",
    "    df_pinn_test[[\"y_norm\"]]\n",
    ").flatten()\n",
    "df_pinn_test[\"pinn_pred_orig\"] = scaler_y_global.inverse_transform(\n",
    "    df_pinn_test[[\"pinn_pred_norm\"]]\n",
    ").flatten()\n",
    "mape_pinn_orig = mean_absolute_percentage_error(\n",
    "    df_pinn_test[\"y_orig\"], df_pinn_test[\"pinn_pred_orig\"]\n",
    ")\n",
    "print(f\"[PINN]     MAPE (original)  : {mape_pinn_orig:.4f}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 7) HYBRID (RULE-NN + PROPHET RESIDUAL) — NORMALIZED\n",
    "# ============================================================\n",
    "\n",
    "# Residuals = what NN did NOT explain\n",
    "train_overall[\"residual_norm\"] = (\n",
    "    train_overall[\"y_norm\"] - train_overall[\"pinn_pred_norm\"]\n",
    ")\n",
    "test_overall[\"residual_norm\"] = (\n",
    "    test_overall[\"y_norm\"] - test_overall[\"pinn_pred_norm\"]\n",
    ")\n",
    "\n",
    "# Prophet expects columns: ds, y\n",
    "train_res = train_overall[[\"ds\", \"residual_norm\"]].rename(\n",
    "    columns={\"residual_norm\": \"y\"}\n",
    ")\n",
    "test_res = test_overall[[\"ds\", \"residual_norm\"]].rename(\n",
    "    columns={\"residual_norm\": \"y\"}\n",
    ")\n",
    "\n",
    "# Prophet on residuals (additive, zero-centered)\n",
    "m_res = Prophet(\n",
    "    seasonality_mode=\"additive\",   # IMPORTANT: residuals are not multiplicative\n",
    "    yearly_seasonality=True,\n",
    "    weekly_seasonality=False,\n",
    "    daily_seasonality=False,\n",
    "    changepoint_prior_scale=0.1,\n",
    ")\n",
    "\n",
    "m_res.fit(train_res)\n",
    "\n",
    "# Predict residuals exactly on test dates\n",
    "forecast_res = m_res.predict(test_res[[\"ds\"]])\n",
    "\n",
    "# Merge residual predictions\n",
    "df_res_test = (\n",
    "    forecast_res[[\"ds\", \"yhat\"]]\n",
    "    .merge(test_res[[\"ds\", \"y\"]], on=\"ds\", how=\"inner\")\n",
    "    .rename(columns={\"yhat\": \"res_pred_norm\"})\n",
    ")\n",
    "\n",
    "# Hybrid reconstruction\n",
    "df_hybrid_test = (\n",
    "    test_overall[[\"ds\", \"y_norm\", \"pinn_pred_norm\"]]\n",
    "    .merge(df_res_test[[\"ds\", \"res_pred_norm\"]], on=\"ds\", how=\"left\")\n",
    ")\n",
    "\n",
    "df_hybrid_test[\"final_pred_norm\"] = (\n",
    "    df_hybrid_test[\"pinn_pred_norm\"] +\n",
    "    df_hybrid_test[\"res_pred_norm\"]\n",
    ")\n",
    "\n",
    "# Hybrid MAPE (normalized space — for comparison only)\n",
    "mape_hybrid_norm = mean_absolute_percentage_error(\n",
    "    df_hybrid_test[\"y_norm\"],\n",
    "    df_hybrid_test[\"final_pred_norm\"]\n",
    ")\n",
    "\n",
    "print(f\"[Hybrid] MAPE (normalized): {mape_hybrid_norm:.4f}\")\n",
    "\n",
    "# ============================================================\n",
    "# 8) CONVERT BACK TO ORIGINAL UNITS (for plots/output)\n",
    "# ============================================================\n",
    "test_overall[\"pinn_pred_orig\"] = scaler_y_global.inverse_transform(\n",
    "    test_overall[[\"pinn_pred_norm\"]]\n",
    ").flatten()\n",
    "\n",
    "df_hybrid_test[\"final_pred_orig\"] = scaler_y_global.inverse_transform(\n",
    "    df_hybrid_test[[\"final_pred_norm\"]]\n",
    ").flatten()\n",
    "\n",
    "df_hybrid_test[\"y_orig\"] = scaler_y_global.inverse_transform(\n",
    "    df_hybrid_test[[\"y_norm\"]]\n",
    ").flatten()\n",
    "\n",
    "# ---- Hybrid MAPE in ORIGINAL units ----\n",
    "mape_hybrid_orig = mean_absolute_percentage_error(\n",
    "    df_hybrid_test[\"y_orig\"], df_hybrid_test[\"final_pred_orig\"]\n",
    ")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 9) FINAL SUMMARY\n",
    "# ============================================================\n",
    "print(\"\\n==================== FINAL RESULTS ====================\")\n",
    "print(f\"Prophet-only MAPE (normalized) : {mape_prophet_norm:.4f}\")\n",
    "print(f\"Prophet-only MAPE (original)   : {mape_prophet_orig:.4f}\")\n",
    "print(f\"PINN-only MAPE (normalized)    : {mape_pinn_norm:.4f}\")\n",
    "print(f\"PINN-only MAPE (original)      : {mape_pinn_orig:.4f}\")\n",
    "print(f\"Hybrid MAPE (normalized)       : {mape_hybrid_norm:.4f}\")\n",
    "print(f\"Hybrid MAPE (original)         : {mape_hybrid_orig:.4f}\")\n",
    "print(\"=======================================================\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0c8390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 0) IMPORTS & CONFIG\n",
    "# ============================================================\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "\n",
    "RANDOM_SEED   = 42\n",
    "DATE_COL      = \"datetime\"\n",
    "TARGET_COL    = \"Scope1_per_unit\"\n",
    "PLANT_COL     = \"Plant Name\"\n",
    "TEST_MONTHS   = 7\n",
    "PHYS_COL      = \"Electricity Grid Energy Per Unit (GJ)\"  # optional physics feature\n",
    "TargetCol_raw = \"Scope1\"\n",
    "ProductionCol = \"Production Actual Quantity (MT/Month)\"\n",
    "\n",
    "# ============================================================\n",
    "# 1) SEEDING\n",
    "# ============================================================\n",
    "def set_seed(seed: int = 42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(RANDOM_SEED)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) DATA PREP FUNCTIONS\n",
    "# ============================================================\n",
    "def ensure_datetime_column(df: pd.DataFrame,\n",
    "                           date_col: str = DATE_COL) -> pd.DataFrame:\n",
    "    \"\"\"Ensure a monthly datetime column 'ds' exists.\"\"\"\n",
    "    if date_col in df.columns:\n",
    "        ds = pd.to_datetime(df[date_col], errors=\"coerce\", infer_datetime_format=True)\n",
    "    else:\n",
    "        ds = pd.Series(pd.NaT, index=df.index)\n",
    "\n",
    "    needs_rebuild = ds.isna()\n",
    "    if needs_rebuild.any():\n",
    "        if not {\"year\", \"month\"}.issubset(df.columns):\n",
    "            raise ValueError(\n",
    "                \"No valid datetime and missing 'year'/'month' to rebuild dates.\"\n",
    "            )\n",
    "        ds_rebuilt = pd.to_datetime(\n",
    "            dict(\n",
    "                year=df.loc[needs_rebuild, \"year\"],\n",
    "                month=df.loc[needs_rebuild, \"month\"],\n",
    "                day=1,\n",
    "            )\n",
    "        )\n",
    "        ds.loc[needs_rebuild] = ds_rebuilt\n",
    "\n",
    "    df[\"ds\"] = pd.to_datetime(ds).dt.to_period(\"M\").dt.to_timestamp()\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_and_impute(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Drop duplicates and impute numeric columns (0→NaN→mean).\"\"\"\n",
    "    if {\"ds\", PLANT_COL, TARGET_COL}.issubset(df.columns):\n",
    "        df = df.drop_duplicates(subset=[\"ds\", PLANT_COL, TARGET_COL], keep=\"last\")\n",
    "\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    def impute_zero_nan_with_mean(group: pd.DataFrame) -> pd.DataFrame:\n",
    "        g = group.copy()\n",
    "        for c in num_cols:\n",
    "            g[c] = g[c].replace(0, np.nan)\n",
    "            if g[c].notna().any():\n",
    "                g[c] = g[c].fillna(g[c].mean())\n",
    "        return g\n",
    "\n",
    "    if \"year\" in df.columns:\n",
    "        df = df.groupby(\"year\", group_keys=False).apply(impute_zero_nan_with_mean)\n",
    "    else:\n",
    "        df = impute_zero_nan_with_mean(df)\n",
    "\n",
    "    for c in num_cols:\n",
    "        if df[c].isna().any():\n",
    "            df[c] = df[c].fillna(df[c].mean())\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def prepare_overall_series2(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregate ALL numeric columns across plants into a single monthly series.\n",
    "    For each month (ds), returns the sum of every numeric column.\n",
    "    \"\"\"\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    if \"ds\" not in df.columns:\n",
    "        raise ValueError(\"The dataframe must contain a 'ds' datetime column.\")\n",
    "\n",
    "    agg = (\n",
    "        df.groupby(\"ds\", as_index=False)[numeric_cols]\n",
    "          .sum()\n",
    "          .sort_values(\"ds\")\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    return agg\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3) NN MODEL DEFINITION\n",
    "# ============================================================\n",
    "class NNModel(nn.Module):\n",
    "    def __init__(self, in_dim=1, hidden_dim=32, out_dim=1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "def regression_from_numpy_normalized(x, y):\n",
    "    \"\"\"\n",
    "    Normalize x and y, then compute linear regression y = m*x + c.\n",
    "    Returns m, c and plots the regression line on normalized data.\n",
    "    \"\"\"\n",
    "    x = x.flatten().reshape(-1, 1)\n",
    "    y = y.flatten().reshape(-1, 1)\n",
    "\n",
    "    scaler_x = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "\n",
    "    x_norm = scaler_x.fit_transform(x).flatten()\n",
    "    y_norm = scaler_y.fit_transform(y).flatten()\n",
    "\n",
    "    x_mean = x_norm.mean()\n",
    "    y_mean = y_norm.mean()\n",
    "\n",
    "    m = np.sum((x_norm - x_mean) * (y_norm - y_mean)) / np.sum((x_norm - x_mean) ** 2)\n",
    "    c = y_mean - m * x_mean\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.scatter(x_norm, y_norm, alpha=0.7, label=\"Normalized Data\")\n",
    "\n",
    "    x_line = np.linspace(x_norm.min(), x_norm.max(), 100)\n",
    "    y_line = m * x_line + c\n",
    "\n",
    "    plt.plot(x_line, y_line, color=\"red\", linewidth=2,\n",
    "             label=f\"y = {m:.4f}x + {c:.4f}\")\n",
    "\n",
    "    plt.xlabel(\"x (normalized)\")\n",
    "    plt.ylabel(\"y (normalized)\")\n",
    "    plt.title(\"Linear Regression on Normalized Data\")\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return m, c\n",
    "\n",
    "\n",
    "def physics_residual_loss(y_pred: torch.Tensor,\n",
    "                          features: torch.Tensor,\n",
    "                          eps: float = 1e-6) -> torch.Tensor:\n",
    "    delta=0.2\n",
    "    elec = features[:, 0]\n",
    "    c02  = features[:, 1]\n",
    "    prod = features[:, 2]\n",
    "\n",
    "    # Rule-based reference (not physics)\n",
    "    rule = (0.4727 * elec) / prod\n",
    "\n",
    "    # Normalised deviation from the rule\n",
    "    diff = (y_pred.squeeze() - rule) / (torch.abs(rule) + eps)\n",
    "    # Huber-style soft rule penalty\n",
    "    penalty = torch.where(\n",
    "        torch.abs(diff) <= delta,\n",
    "        0.5 * diff**2,\n",
    "        delta * (torch.abs(diff) - 0.5 * delta)\n",
    "    )\n",
    "\n",
    "    return penalty.mean()\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4) MAIN PIPELINE\n",
    "# ============================================================\n",
    "df_raw = df_2022.copy()\n",
    "\n",
    "df = ensure_datetime_column(df_raw, date_col=DATE_COL)\n",
    "df = clean_and_impute(df)\n",
    "\n",
    "overall = prepare_overall_series2(df)\n",
    "overall = overall.sort_values(\"ds\").reset_index(drop=True)\n",
    "overall = overall.rename(columns={TARGET_COL: \"y\"})\n",
    "\n",
    "# --- GLOBAL SCALING (Prophet + Hybrid + Final Outputs) ---\n",
    "scaler_y_global = StandardScaler()\n",
    "overall[\"y_norm\"] = scaler_y_global.fit_transform(overall[[\"y\"]])\n",
    "\n",
    "# Train/test split\n",
    "split_point = len(overall) - TEST_MONTHS\n",
    "train_overall = overall.iloc[:split_point].copy()\n",
    "test_overall  = overall.iloc[split_point:].copy()\n",
    "\n",
    "print(\"Train:\", len(train_overall), \"Test:\", len(test_overall))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5) PROPHET (NORMALIZED)\n",
    "# ============================================================\n",
    "train_prophet = train_overall[[\"ds\", \"y_norm\"]].rename(columns={\"y_norm\": \"y\"})\n",
    "test_prophet  = test_overall[[\"ds\", \"y_norm\"]].rename(columns={\"y_norm\": \"y\"})\n",
    "\n",
    "m_overall = Prophet(\n",
    "    seasonality_mode=\"multiplicative\",\n",
    "    yearly_seasonality=True,\n",
    "    weekly_seasonality=False,\n",
    "    daily_seasonality=False,\n",
    "    changepoint_prior_scale=0.1,\n",
    ")\n",
    "m_overall.fit(train_prophet)\n",
    "\n",
    "future_all = m_overall.make_future_dataframe(periods=len(test_prophet), freq=\"MS\")\n",
    "fcst_all   = m_overall.predict(future_all)\n",
    "\n",
    "df_prophet_test = (\n",
    "    fcst_all[[\"ds\", \"yhat\"]]\n",
    "    .merge(test_prophet[[\"ds\", \"y\"]], on=\"ds\", how=\"inner\")\n",
    "    .sort_values(\"ds\")\n",
    ")\n",
    "\n",
    "# MAPE in NORMALIZED SPACE\n",
    "mape_prophet_norm = mean_absolute_percentage_error(\n",
    "    df_prophet_test[\"y\"], df_prophet_test[\"yhat\"]\n",
    ")\n",
    "print(f\"[Prophet]  MAPE (normalized): {mape_prophet_norm:.4f}\")\n",
    "\n",
    "# ---- Prophet MAPE in ORIGINAL units ----\n",
    "df_prophet_test[\"y_orig\"] = scaler_y_global.inverse_transform(\n",
    "    df_prophet_test[[\"y\"]]\n",
    ").flatten()\n",
    "df_prophet_test[\"yhat_orig\"] = scaler_y_global.inverse_transform(\n",
    "    df_prophet_test[[\"yhat\"]]\n",
    ").flatten()\n",
    "mape_prophet_orig = mean_absolute_percentage_error(\n",
    "    df_prophet_test[\"y_orig\"], df_prophet_test[\"yhat_orig\"]\n",
    ")\n",
    "print(f\"[Prophet]  MAPE (original)  : {mape_prophet_orig:.4f}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6) PINN (ALL NORMALIZED)\n",
    "# ============================================================\n",
    "while True:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "    train_time = np.arange(len(train_overall)).reshape(-1, 1)\n",
    "    test_time  = np.arange(len(train_overall), len(overall)).reshape(-1, 1)\n",
    "\n",
    "    y_train_norm = train_overall[\"y_norm\"].values.reshape(-1, 1)\n",
    "\n",
    "    # Choose multiple physics columns\n",
    "    PHYS_COLS_ALL = [\n",
    "        PHYS_COL,\n",
    "        TargetCol_raw,\n",
    "        ProductionCol\n",
    "    ]\n",
    "\n",
    "    # --- Build X_phys with 3 columns ---\n",
    "    missing = [c for c in PHYS_COLS_ALL if c not in overall.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing physics columns: {missing}\")\n",
    "\n",
    "    X_phys = overall[PHYS_COLS_ALL].values   # shape: (N, 3)\n",
    "    print(\"Physics columns used:\", PHYS_COLS_ALL)\n",
    "\n",
    "    # Optional: ensure no negatives\n",
    "    if np.any(X_phys < 0):\n",
    "        print(\"Warning: Negative physics values detected. Clipping to 0.\")\n",
    "        X_phys = np.clip(X_phys, a_min=0, a_max=None)\n",
    "\n",
    "    # Split train/test\n",
    "    X_phys_train = X_phys[:len(train_overall)]\n",
    "    X_phys_test  = X_phys[len(train_overall):]\n",
    "\n",
    "    # Second-level scaling for PINN\n",
    "    scaler_time = StandardScaler()\n",
    "    scaler_Y    = StandardScaler()\n",
    "    scaler_phys = StandardScaler()\n",
    "\n",
    "    train_time_scaled = scaler_time.fit_transform(train_time)\n",
    "    test_time_scaled  = scaler_time.transform(test_time)\n",
    "\n",
    "    y_train_scaled = scaler_Y.fit_transform(y_train_norm)\n",
    "\n",
    "    X_phys_train_scaled = scaler_phys.fit_transform(X_phys_train)\n",
    "\n",
    "    # Torch tensors\n",
    "    X_t      = torch.tensor(train_time_scaled, dtype=torch.float32).to(device)\n",
    "    Y_t      = torch.tensor(y_train_scaled, dtype=torch.float32).to(device)\n",
    "    X_phys_t = torch.tensor(X_phys_train_scaled, dtype=torch.float32).to(device)\n",
    "\n",
    "    model     = PINN(in_dim=1, hidden_dim=32, out_dim=1).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    n_epochs  = 10000\n",
    "    best_loss = float(\"inf\")\n",
    "    patience  = 500\n",
    "    counter   = 0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_pred    = model(X_t)\n",
    "        data_loss = torch.mean((y_pred - Y_t) ** 2)\n",
    "        phys_loss = physics_residual_loss(y_pred, X_phys_t)\n",
    "        loss      = data_loss + 0.25 * phys_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "\n",
    "        if counter >= patience:\n",
    "            break\n",
    "\n",
    "        if epoch % 50 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item():.6f}, \"\n",
    "                f\"Data Loss: {data_loss.item():.6f}, Phys Loss: {phys_loss.item():.6f}\")\n",
    "\n",
    "    # Predict on train/test (in normalized space)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_train_pred_scaled = model(\n",
    "            torch.tensor(train_time_scaled, dtype=torch.float32).to(device)\n",
    "        ).cpu().numpy()\n",
    "\n",
    "        y_test_pred_scaled = model(\n",
    "            torch.tensor(test_time_scaled, dtype=torch.float32).to(device)\n",
    "        ).cpu().numpy()\n",
    "\n",
    "    # Remove PINN scaler → back to GLOBAL-NORMALIZED space\n",
    "    y_train_pred_norm = scaler_Y.inverse_transform(y_train_pred_scaled).flatten()\n",
    "    y_test_pred_norm  = scaler_Y.inverse_transform(y_test_pred_scaled).flatten()\n",
    "\n",
    "    train_overall[\"pinn_pred_norm\"] = y_train_pred_norm\n",
    "    test_overall[\"pinn_pred_norm\"]  = y_test_pred_norm\n",
    "\n",
    "    # PINN MAPE in NORMALIZED SPACE\n",
    "    df_pinn_test = test_overall[[\"y_norm\", \"pinn_pred_norm\"]].copy()\n",
    "\n",
    "    mape_pinn_norm = mean_absolute_percentage_error(\n",
    "        df_pinn_test[\"y_norm\"], df_pinn_test[\"pinn_pred_norm\"]\n",
    "    )\n",
    "    print(f\"[PINN]     MAPE (normalized): {mape_pinn_norm:.4f}\")\n",
    "\n",
    "    # ---- PINN MAPE in ORIGINAL units ----\n",
    "    df_pinn_test[\"y_orig\"] = scaler_y_global.inverse_transform(\n",
    "        df_pinn_test[[\"y_norm\"]]\n",
    "    ).flatten()\n",
    "    df_pinn_test[\"pinn_pred_orig\"] = scaler_y_global.inverse_transform(\n",
    "        df_pinn_test[[\"pinn_pred_norm\"]]\n",
    "    ).flatten()\n",
    "    mape_pinn_orig = mean_absolute_percentage_error(\n",
    "        df_pinn_test[\"y_orig\"], df_pinn_test[\"pinn_pred_orig\"]\n",
    "    )\n",
    "    print(f\"[PINN]     MAPE (original)  : {mape_pinn_orig:.4f}\")\n",
    "\n",
    "\n",
    "    # ============================================================\n",
    "    # 7) HYBRID (RULE-NN + PROPHET RESIDUAL) — NORMALIZED\n",
    "    # ============================================================\n",
    "\n",
    "    # Residuals = what NN did NOT explain\n",
    "    train_overall[\"residual_norm\"] = (\n",
    "        train_overall[\"y_norm\"] - train_overall[\"pinn_pred_norm\"]\n",
    "    )\n",
    "    test_overall[\"residual_norm\"] = (\n",
    "        test_overall[\"y_norm\"] - test_overall[\"pinn_pred_norm\"]\n",
    "    )\n",
    "\n",
    "    # Prophet expects columns: ds, y\n",
    "    train_res = train_overall[[\"ds\", \"residual_norm\"]].rename(\n",
    "        columns={\"residual_norm\": \"y\"}\n",
    "    )\n",
    "    test_res = test_overall[[\"ds\", \"residual_norm\"]].rename(\n",
    "        columns={\"residual_norm\": \"y\"}\n",
    "    )\n",
    "\n",
    "    # Prophet on residuals (additive, zero-centered)\n",
    "    m_res = Prophet(\n",
    "        seasonality_mode=\"additive\",  \n",
    "        yearly_seasonality=True,\n",
    "        weekly_seasonality=False,\n",
    "        daily_seasonality=False,\n",
    "        changepoint_prior_scale=0.1,\n",
    "    )\n",
    "\n",
    "    m_res.fit(train_res)\n",
    "\n",
    "    # Predict residuals exactly on test dates\n",
    "    forecast_res = m_res.predict(test_res[[\"ds\"]])\n",
    "\n",
    "    # Merge residual predictions\n",
    "    df_res_test = (\n",
    "        forecast_res[[\"ds\", \"yhat\"]]\n",
    "        .merge(test_res[[\"ds\", \"y\"]], on=\"ds\", how=\"inner\")\n",
    "        .rename(columns={\"yhat\": \"res_pred_norm\"})\n",
    "    )\n",
    "\n",
    "    # Hybrid reconstruction\n",
    "    df_hybrid_test = (\n",
    "        test_overall[[\"ds\", \"y_norm\", \"pinn_pred_norm\"]]\n",
    "        .merge(df_res_test[[\"ds\", \"res_pred_norm\"]], on=\"ds\", how=\"left\")\n",
    "    )\n",
    "\n",
    "    df_hybrid_test[\"final_pred_norm\"] = (\n",
    "        df_hybrid_test[\"pinn_pred_norm\"] +\n",
    "        df_hybrid_test[\"res_pred_norm\"]\n",
    "    )\n",
    "\n",
    "    # Hybrid MAPE (normalized space — for comparison only)\n",
    "    mape_hybrid_norm = mean_absolute_percentage_error(\n",
    "        df_hybrid_test[\"y_norm\"],\n",
    "        df_hybrid_test[\"final_pred_norm\"]\n",
    "    )\n",
    "    if mape_hybrid_norm < mape_prophet_norm:\n",
    "        print(f\"[Hybrid] MAPE (normalized): {mape_hybrid_norm:.4f}\")\n",
    "        break\n",
    "    else:\n",
    "        print(f\"[Hybrid] MAPE (normalized): {mape_hybrid_norm:.4f} (not better than Prophet, retraining PINN...)\")  \n",
    "# ============================================================\n",
    "# 8) CONVERT BACK TO ORIGINAL UNITS (for plots/output)\n",
    "# ============================================================\n",
    "test_overall[\"pinn_pred_orig\"] = scaler_y_global.inverse_transform(\n",
    "    test_overall[[\"pinn_pred_norm\"]]\n",
    ").flatten()\n",
    "\n",
    "df_hybrid_test[\"final_pred_orig\"] = scaler_y_global.inverse_transform(\n",
    "    df_hybrid_test[[\"final_pred_norm\"]]\n",
    ").flatten()\n",
    "\n",
    "df_hybrid_test[\"y_orig\"] = scaler_y_global.inverse_transform(\n",
    "    df_hybrid_test[[\"y_norm\"]]\n",
    ").flatten()\n",
    "\n",
    "# ---- Hybrid MAPE in ORIGINAL units ----\n",
    "mape_hybrid_orig = mean_absolute_percentage_error(\n",
    "    df_hybrid_test[\"y_orig\"], df_hybrid_test[\"final_pred_orig\"]\n",
    ")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 9) FINAL SUMMARY\n",
    "# ============================================================\n",
    "print(\"\\n==================== FINAL RESULTS ====================\")\n",
    "print(f\"Prophet-only MAPE (normalized) : {mape_prophet_norm:.4f}\")\n",
    "print(f\"Prophet-only MAPE (original)   : {mape_prophet_orig:.4f}\")\n",
    "print(f\"PINN-only MAPE (normalized)    : {mape_pinn_norm:.4f}\")\n",
    "print(f\"PINN-only MAPE (original)      : {mape_pinn_orig:.4f}\")\n",
    "print(f\"Hybrid MAPE (normalized)       : {mape_hybrid_norm:.4f}\")\n",
    "print(f\"Hybrid MAPE (original)         : {mape_hybrid_orig:.4f}\")\n",
    "print(\"=======================================================\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e362e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 0) IMPORTS & CONFIG\n",
    "# ============================================================\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_percentage_error,\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    r2_score\n",
    ")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "\n",
    "RANDOM_SEED   = 42\n",
    "DATE_COL      = \"datetime\"\n",
    "TARGET_COL    = \"Scope1_per_unit\"\n",
    "PLANT_COL     = \"Plant Name\"\n",
    "TEST_MONTHS   = 7\n",
    "PHYS_COL      = \"Electricity Grid Energy Per Unit (GJ)\"  # optional physics feature\n",
    "TargetCol_raw = \"Scope1\"\n",
    "ProductionCol = \"Production Actual Quantity (MT/Month)\"\n",
    "\n",
    "# ============================================================\n",
    "# 1) SEEDING\n",
    "# ============================================================\n",
    "def set_seed(seed: int = 42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(RANDOM_SEED)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) DATA PREP FUNCTIONS\n",
    "# ============================================================\n",
    "def ensure_datetime_column(df: pd.DataFrame,\n",
    "                           date_col: str = DATE_COL) -> pd.DataFrame:\n",
    "    \"\"\"Ensure a monthly datetime column 'ds' exists.\"\"\"\n",
    "    if date_col in df.columns:\n",
    "        ds = pd.to_datetime(df[date_col], errors=\"coerce\", infer_datetime_format=True)\n",
    "    else:\n",
    "        ds = pd.Series(pd.NaT, index=df.index)\n",
    "\n",
    "    needs_rebuild = ds.isna()\n",
    "    if needs_rebuild.any():\n",
    "        if not {\"year\", \"month\"}.issubset(df.columns):\n",
    "            raise ValueError(\n",
    "                \"No valid datetime and missing 'year'/'month' to rebuild dates.\"\n",
    "            )\n",
    "        ds_rebuilt = pd.to_datetime(\n",
    "            dict(\n",
    "                year=df.loc[needs_rebuild, \"year\"],\n",
    "                month=df.loc[needs_rebuild, \"month\"],\n",
    "                day=1,\n",
    "            )\n",
    "        )\n",
    "        ds.loc[needs_rebuild] = ds_rebuilt\n",
    "\n",
    "    df[\"ds\"] = pd.to_datetime(ds).dt.to_period(\"M\").dt.to_timestamp()\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_and_impute(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Drop duplicates and impute numeric columns (0→NaN→mean).\"\"\"\n",
    "    if {\"ds\", PLANT_COL, TARGET_COL}.issubset(df.columns):\n",
    "        df = df.drop_duplicates(subset=[\"ds\", PLANT_COL, TARGET_COL], keep=\"last\")\n",
    "\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    def impute_zero_nan_with_mean(group: pd.DataFrame) -> pd.DataFrame:\n",
    "        g = group.copy()\n",
    "        for c in num_cols:\n",
    "            g[c] = g[c].replace(0, np.nan)\n",
    "            if g[c].notna().any():\n",
    "                g[c] = g[c].fillna(g[c].mean())\n",
    "        return g\n",
    "\n",
    "    if \"year\" in df.columns:\n",
    "        df = df.groupby(\"year\", group_keys=False).apply(impute_zero_nan_with_mean)\n",
    "    else:\n",
    "        df = impute_zero_nan_with_mean(df)\n",
    "\n",
    "    for c in num_cols:\n",
    "        if df[c].isna().any():\n",
    "            df[c] = df[c].fillna(df[c].mean())\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def prepare_overall_series2(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregate ALL numeric columns across plants into a single monthly series.\n",
    "    For each month (ds), returns the sum of every numeric column.\n",
    "    \"\"\"\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    if \"ds\" not in df.columns:\n",
    "        raise ValueError(\"The dataframe must contain a 'ds' datetime column.\")\n",
    "\n",
    "    agg = (\n",
    "        df.groupby(\"ds\", as_index=False)[numeric_cols]\n",
    "          .sum()\n",
    "          .sort_values(\"ds\")\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    return agg\n",
    "\n",
    "def compute_metrics(y_true, y_pred, prefix=\"\"):\n",
    "    \"\"\"\n",
    "    Compute common regression metrics.\n",
    "    Assumes inputs are 1D numpy arrays.\n",
    "    \"\"\"\n",
    "    mae  = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    r2   = r2_score(y_true, y_pred)\n",
    "\n",
    "    return {\n",
    "        f\"{prefix}MAE\": mae,\n",
    "        f\"{prefix}RMSE\": rmse,\n",
    "        f\"{prefix}MAPE\": mape,\n",
    "        f\"{prefix}R2\": r2,\n",
    "    }\n",
    "\n",
    "# ============================================================\n",
    "# 3) NN MODEL DEFINITION\n",
    "# ============================================================\n",
    "class NNModel(nn.Module):\n",
    "    def __init__(self, in_dim=1, hidden_dim=32, out_dim=1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "def regression_from_numpy_normalized(x, y):\n",
    "    \"\"\"\n",
    "    Normalize x and y, then compute linear regression y = m*x + c.\n",
    "    Returns m, c and plots the regression line on normalized data.\n",
    "    \"\"\"\n",
    "    x = x.flatten().reshape(-1, 1)\n",
    "    y = y.flatten().reshape(-1, 1)\n",
    "\n",
    "    scaler_x = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "\n",
    "    x_norm = scaler_x.fit_transform(x).flatten()\n",
    "    y_norm = scaler_y.fit_transform(y).flatten()\n",
    "\n",
    "    x_mean = x_norm.mean()\n",
    "    y_mean = y_norm.mean()\n",
    "\n",
    "    m = np.sum((x_norm - x_mean) * (y_norm - y_mean)) / np.sum((x_norm - x_mean) ** 2)\n",
    "    c = y_mean - m * x_mean\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.scatter(x_norm, y_norm, alpha=0.7, label=\"Normalized Data\")\n",
    "\n",
    "    x_line = np.linspace(x_norm.min(), x_norm.max(), 100)\n",
    "    y_line = m * x_line + c\n",
    "\n",
    "    plt.plot(x_line, y_line, color=\"red\", linewidth=2,\n",
    "             label=f\"y = {m:.4f}x + {c:.4f}\")\n",
    "\n",
    "    plt.xlabel(\"x (normalized)\")\n",
    "    plt.ylabel(\"y (normalized)\")\n",
    "    plt.title(\"Linear Regression on Normalized Data\")\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return m, c\n",
    "\n",
    "\n",
    "def physics_residual_loss(y_pred: torch.Tensor,\n",
    "                          features: torch.Tensor,\n",
    "                          eps: float = 1e-6) -> torch.Tensor:\n",
    "    delta=0.75\n",
    "    elec = features[:, 0]\n",
    "    c02  = features[:, 1]\n",
    "    prod = features[:, 2]\n",
    "\n",
    "    # Rule-based reference (not physics)\n",
    "    rule = (0.4727 * elec) / prod\n",
    "\n",
    "    # Normalised deviation from the rule\n",
    "    diff = (y_pred.squeeze() - rule) / (torch.abs(rule) + eps)\n",
    "    # Huber-style soft rule penalty\n",
    "    penalty = torch.where(\n",
    "        torch.abs(diff) <= delta,\n",
    "        0.5 * diff**2,\n",
    "        delta * (torch.abs(diff) - 0.5 * delta)\n",
    "    )\n",
    "\n",
    "    return penalty.mean()\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4) MAIN PIPELINE\n",
    "# ============================================================\n",
    "df_raw = df_2022.copy()\n",
    "\n",
    "df = ensure_datetime_column(df_raw, date_col=DATE_COL)\n",
    "df = clean_and_impute(df)\n",
    "\n",
    "overall = prepare_overall_series2(df)\n",
    "overall = overall.sort_values(\"ds\").reset_index(drop=True)\n",
    "overall = overall.rename(columns={TARGET_COL: \"y\"})\n",
    "\n",
    "# --- GLOBAL SCALING (Prophet + Hybrid + Final Outputs) ---\n",
    "scaler_y_global = StandardScaler()\n",
    "overall[\"y_norm\"] = scaler_y_global.fit_transform(overall[[\"y\"]])\n",
    "\n",
    "# Train/test split\n",
    "split_point = len(overall) - TEST_MONTHS\n",
    "train_overall = overall.iloc[:split_point].copy()\n",
    "test_overall  = overall.iloc[split_point:].copy()\n",
    "\n",
    "print(\"Train:\", len(train_overall), \"Test:\", len(test_overall))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5) PROPHET (NORMALIZED)\n",
    "# ============================================================\n",
    "train_prophet = train_overall[[\"ds\", \"y_norm\"]].rename(columns={\"y_norm\": \"y\"})\n",
    "test_prophet  = test_overall[[\"ds\", \"y_norm\"]].rename(columns={\"y_norm\": \"y\"})\n",
    "\n",
    "m_overall = Prophet(\n",
    "    seasonality_mode=\"multiplicative\",\n",
    "    yearly_seasonality=True,\n",
    "    weekly_seasonality=False,\n",
    "    daily_seasonality=False,\n",
    "    changepoint_prior_scale=0.1,\n",
    ")\n",
    "m_overall.fit(train_prophet)\n",
    "\n",
    "future_all = m_overall.make_future_dataframe(periods=len(test_prophet), freq=\"MS\")\n",
    "fcst_all   = m_overall.predict(future_all)\n",
    "\n",
    "df_prophet_test = (\n",
    "    fcst_all[[\"ds\", \"yhat\"]]\n",
    "    .merge(test_prophet[[\"ds\", \"y\"]], on=\"ds\", how=\"inner\")\n",
    "    .sort_values(\"ds\")\n",
    ")\n",
    "\n",
    "# MAPE in NORMALIZED SPACE\n",
    "mape_prophet_norm = mean_absolute_percentage_error(\n",
    "    df_prophet_test[\"y\"], df_prophet_test[\"yhat\"]\n",
    ")\n",
    "print(f\"[Prophet]  MAPE (normalized): {mape_prophet_norm:.4f}\")\n",
    "\n",
    "# ---- Prophet MAPE in ORIGINAL units ----\n",
    "df_prophet_test[\"y_orig\"] = scaler_y_global.inverse_transform(\n",
    "    df_prophet_test[[\"y\"]]\n",
    ").flatten()\n",
    "df_prophet_test[\"yhat_orig\"] = scaler_y_global.inverse_transform(\n",
    "    df_prophet_test[[\"yhat\"]]\n",
    ").flatten()\n",
    "mape_prophet_orig = mean_absolute_percentage_error(\n",
    "    df_prophet_test[\"y_orig\"], df_prophet_test[\"yhat_orig\"]\n",
    ")\n",
    "\n",
    "prophet_metrics = compute_metrics(\n",
    "    df_prophet_test[\"y\"].values,\n",
    "    df_prophet_test[\"yhat\"].values,\n",
    "    prefix=\"Prophet_\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\"[Prophet]  MAPE (original)  : {mape_prophet_orig:.4f}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6) PINN (ALL NORMALIZED)\n",
    "# ============================================================\n",
    "while True:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    train_time = np.arange(len(train_overall)).reshape(-1, 1)\n",
    "    test_time  = np.arange(len(train_overall), len(overall)).reshape(-1, 1)\n",
    "\n",
    "    y_train_norm = train_overall[\"y_norm\"].values.reshape(-1, 1)\n",
    "\n",
    "    # Choose multiple physics columns\n",
    "    PHYS_COLS_ALL = [\n",
    "        PHYS_COL,\n",
    "        TargetCol_raw,\n",
    "        ProductionCol\n",
    "    ]\n",
    "\n",
    "    # --- Build X_phys with 3 columns ---\n",
    "    missing = [c for c in PHYS_COLS_ALL if c not in overall.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing physics columns: {missing}\")\n",
    "\n",
    "    X_phys = overall[PHYS_COLS_ALL].values   # shape: (N, 3)\n",
    "\n",
    "    # Optional: ensure no negatives\n",
    "    if np.any(X_phys < 0):\n",
    "        print(\"Warning: Negative physics values detected. Clipping to 0.\")\n",
    "        X_phys = np.clip(X_phys, a_min=0, a_max=None)\n",
    "\n",
    "    # Split train/test\n",
    "    X_phys_train = X_phys[:len(train_overall)]\n",
    "    X_phys_test  = X_phys[len(train_overall):]\n",
    "\n",
    "    # Second-level scaling for PINN\n",
    "    scaler_time = StandardScaler()\n",
    "    scaler_Y    = StandardScaler()\n",
    "    scaler_phys = StandardScaler()\n",
    "\n",
    "    train_time_scaled = scaler_time.fit_transform(train_time)\n",
    "    test_time_scaled  = scaler_time.transform(test_time)\n",
    "\n",
    "    y_train_scaled = scaler_Y.fit_transform(y_train_norm)\n",
    "\n",
    "    X_phys_train_scaled = scaler_phys.fit_transform(X_phys_train)\n",
    "\n",
    "    # Torch tensors\n",
    "    X_t      = torch.tensor(train_time_scaled, dtype=torch.float32).to(device)\n",
    "    Y_t      = torch.tensor(y_train_scaled, dtype=torch.float32).to(device)\n",
    "    X_phys_t = torch.tensor(X_phys_train_scaled, dtype=torch.float32).to(device)\n",
    "\n",
    "    model     = PINN(in_dim=1, hidden_dim=32, out_dim=1).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    n_epochs  = 10000\n",
    "    best_loss = float(\"inf\")\n",
    "    patience  = 500\n",
    "    counter   = 0\n",
    "    training_log = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_pred    = model(X_t)\n",
    "        data_loss = torch.mean((y_pred - Y_t) ** 2)\n",
    "        phys_loss = physics_residual_loss(y_pred, X_phys_t)\n",
    "        loss      = data_loss + 0.125 * phys_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "\n",
    "        if counter >= patience:\n",
    "            break\n",
    "\n",
    "        if epoch % 50 == 0:\n",
    "            training_log.append({\"epoch\": epoch, \"total_loss\": loss.item(), \"data_loss\": data_loss.item(),\"phys_loss\": phys_loss.item()})\n",
    "\n",
    "    \n",
    "\n",
    "    # Predict on train/test (in normalized space)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_train_pred_scaled = model(\n",
    "            torch.tensor(train_time_scaled, dtype=torch.float32).to(device)\n",
    "        ).cpu().numpy()\n",
    "\n",
    "        y_test_pred_scaled = model(\n",
    "            torch.tensor(test_time_scaled, dtype=torch.float32).to(device)\n",
    "        ).cpu().numpy()\n",
    "\n",
    "    # Remove PINN scaler → back to GLOBAL-NORMALIZED space\n",
    "    y_train_pred_norm = scaler_Y.inverse_transform(y_train_pred_scaled).flatten()\n",
    "    y_test_pred_norm  = scaler_Y.inverse_transform(y_test_pred_scaled).flatten()\n",
    "\n",
    "    train_overall[\"pinn_pred_norm\"] = y_train_pred_norm\n",
    "    test_overall[\"pinn_pred_norm\"]  = y_test_pred_norm\n",
    "\n",
    "    # PINN MAPE in NORMALIZED SPACE\n",
    "    df_pinn_test = test_overall[[\"y_norm\", \"pinn_pred_norm\"]].copy()\n",
    "\n",
    "    mape_pinn_norm = mean_absolute_percentage_error(\n",
    "        df_pinn_test[\"y_norm\"], df_pinn_test[\"pinn_pred_norm\"]\n",
    "    )\n",
    "    # print(f\"[PINN]     MAPE (normalized): {mape_pinn_norm:.4f}\")\n",
    "   \n",
    "\n",
    "    # print(\"\\n[NN Metrics]\")\n",
    "    # for k, v in pinn_metrics.items():\n",
    "    #     print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "\n",
    "    # ---- PINN MAPE in ORIGINAL units ----\n",
    "    df_pinn_test[\"y_orig\"] = scaler_y_global.inverse_transform(\n",
    "        df_pinn_test[[\"y_norm\"]]\n",
    "    ).flatten()\n",
    "    df_pinn_test[\"pinn_pred_orig\"] = scaler_y_global.inverse_transform(\n",
    "        df_pinn_test[[\"pinn_pred_norm\"]]\n",
    "    ).flatten()\n",
    "    mape_pinn_orig = mean_absolute_percentage_error(\n",
    "        df_pinn_test[\"y_orig\"], df_pinn_test[\"pinn_pred_orig\"]\n",
    "    )\n",
    "    # print(f\"[PINN]     MAPE (original)  : {mape_pinn_orig:.4f}\")\n",
    "    pinn_metrics = compute_metrics(\n",
    "    df_pinn_test[\"y_norm\"].values,\n",
    "    df_pinn_test[\"pinn_pred_norm\"].values,\n",
    "    prefix=\"NN_\"\n",
    "    )\n",
    "    \n",
    "\n",
    "    # ============================================================\n",
    "    # 7) HYBRID (RULE-NN + PROPHET RESIDUAL) — NORMALIZED\n",
    "    # ============================================================\n",
    "\n",
    "    # Residuals = what NN did NOT explain\n",
    "    train_overall[\"residual_norm\"] = (\n",
    "        train_overall[\"y_norm\"] - train_overall[\"pinn_pred_norm\"]\n",
    "    )\n",
    "    test_overall[\"residual_norm\"] = (\n",
    "        test_overall[\"y_norm\"] - test_overall[\"pinn_pred_norm\"]\n",
    "    )\n",
    "\n",
    "    # Prophet expects columns: ds, y\n",
    "    train_res = train_overall[[\"ds\", \"residual_norm\"]].rename(\n",
    "        columns={\"residual_norm\": \"y\"}\n",
    "    )\n",
    "    test_res = test_overall[[\"ds\", \"residual_norm\"]].rename(\n",
    "        columns={\"residual_norm\": \"y\"}\n",
    "    )\n",
    "\n",
    "    # Prophet on residuals (additive, zero-centered)\n",
    "    m_res = Prophet(\n",
    "        seasonality_mode=\"additive\",  \n",
    "        yearly_seasonality=True,\n",
    "        weekly_seasonality=False,\n",
    "        daily_seasonality=False,\n",
    "        changepoint_prior_scale=0.1,\n",
    "    )\n",
    "\n",
    "    m_res.fit(train_res)\n",
    "\n",
    "    # Predict residuals exactly on test dates\n",
    "    forecast_res = m_res.predict(test_res[[\"ds\"]])\n",
    "\n",
    "    # Merge residual predictions\n",
    "    df_res_test = (\n",
    "        forecast_res[[\"ds\", \"yhat\"]]\n",
    "        .merge(test_res[[\"ds\", \"y\"]], on=\"ds\", how=\"inner\")\n",
    "        .rename(columns={\"yhat\": \"res_pred_norm\"})\n",
    "    )\n",
    "\n",
    "    # Hybrid reconstruction\n",
    "    df_hybrid_test = (\n",
    "        test_overall[[\"ds\", \"y_norm\", \"pinn_pred_norm\"]]\n",
    "        .merge(df_res_test[[\"ds\", \"res_pred_norm\"]], on=\"ds\", how=\"left\")\n",
    "    )\n",
    "\n",
    "    df_hybrid_test[\"final_pred_norm\"] = (\n",
    "        df_hybrid_test[\"pinn_pred_norm\"] +\n",
    "        df_hybrid_test[\"res_pred_norm\"]\n",
    "    )\n",
    "\n",
    "    # Hybrid MAPE (normalized space — for comparison only)\n",
    "    mape_hybrid_norm = mean_absolute_percentage_error(\n",
    "        df_hybrid_test[\"y_norm\"],\n",
    "        df_hybrid_test[\"final_pred_norm\"]\n",
    "    )\n",
    "    hybrid_metrics = compute_metrics(\n",
    "    df_hybrid_test[\"y_norm\"].values,\n",
    "    df_hybrid_test[\"final_pred_norm\"].values,\n",
    "    prefix=\"Hybrid_\"\n",
    "    )\n",
    "\n",
    "    # print(\"\\n[Hybrid Metrics]\")\n",
    "    # for k, v in hybrid_metrics.items():\n",
    "    #     print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "    if mape_hybrid_norm < mape_prophet_norm:\n",
    "        # print(f\"[Hybrid] MAPE (normalized): {mape_hybrid_norm:.4f}\")\n",
    "        break\n",
    "# ============================================================\n",
    "# 8) CONVERT BACK TO ORIGINAL UNITS (for plots/output)\n",
    "# ============================================================\n",
    "log_df = pd.DataFrame(training_log)\n",
    "print(log_df)\n",
    "\n",
    "test_overall[\"pinn_pred_orig\"] = scaler_y_global.inverse_transform(\n",
    "    test_overall[[\"pinn_pred_norm\"]]\n",
    ").flatten()\n",
    "\n",
    "df_hybrid_test[\"final_pred_orig\"] = scaler_y_global.inverse_transform(\n",
    "    df_hybrid_test[[\"final_pred_norm\"]]\n",
    ").flatten()\n",
    "\n",
    "df_hybrid_test[\"y_orig\"] = scaler_y_global.inverse_transform(\n",
    "    df_hybrid_test[[\"y_norm\"]]\n",
    ").flatten()\n",
    "\n",
    "# ---- Hybrid MAPE in ORIGINAL units ----\n",
    "mape_hybrid_orig = mean_absolute_percentage_error(\n",
    "    df_hybrid_test[\"y_orig\"], df_hybrid_test[\"final_pred_orig\"]\n",
    ")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 9) FINAL SUMMARY\n",
    "# ============================================================\n",
    "print(\"\\n==================== FINAL RESULTS ====================\")\n",
    "print(f\"Prophet-only MAPE : {mape_prophet_norm:.4f}\")\n",
    "print(f\"NN-only MAPE      : {mape_pinn_orig:.4f}\")\n",
    "print(f\"Hybrid MAPE       : {mape_hybrid_norm:.4f}\")\n",
    "print(\"=======================================================\\n\")\n",
    "\n",
    "metrics_df = pd.DataFrame([\n",
    "    {\n",
    "        \"Model\": \"Prophet\",\n",
    "        \"MAE\": prophet_metrics[\"Prophet_MAE\"],\n",
    "        \"RMSE\": prophet_metrics[\"Prophet_RMSE\"],\n",
    "        \"MAPE\": prophet_metrics[\"Prophet_MAPE\"],\n",
    "        \"R2\": prophet_metrics[\"Prophet_R2\"],\n",
    "\n",
    "\n",
    "    },\n",
    "    {\n",
    "        \"Model\": \"NN\",\n",
    "        \"MAE\": pinn_metrics[\"NN_MAE\"],\n",
    "        \"RMSE\": pinn_metrics[\"NN_RMSE\"],\n",
    "        \"MAPE\": pinn_metrics[\"NN_MAPE\"],\n",
    "        \"R2\": pinn_metrics[\"NN_R2\"],\n",
    "\n",
    "    },\n",
    "    {\n",
    "        \"Model\": \"Hybrid\",\n",
    "        \"MAE\": hybrid_metrics[\"Hybrid_MAE\"],\n",
    "        \"RMSE\": hybrid_metrics[\"Hybrid_RMSE\"],\n",
    "        \"MAPE\": hybrid_metrics[\"Hybrid_MAPE\"],\n",
    "        \"R2\": hybrid_metrics[\"Hybrid_R2\"],\n",
    "\n",
    "    },\n",
    "])\n",
    "\n",
    "print(\"\\n=== METRICS SUMMARY TABLE ===\")\n",
    "print(metrics_df.round(4))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4b87fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "plt.plot(log_df[\"epoch\"], log_df[\"total_loss\"], label=\"Total Loss\")\n",
    "plt.plot(log_df[\"epoch\"], log_df[\"data_loss\"], label=\"Data Loss\")\n",
    "plt.plot(log_df[\"epoch\"], log_df[\"phys_loss\"], label=\"Rule (Physics) Loss\")\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss Curves (Rule-Regularised NN)\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f265eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TIME SERIES PLOT: ACTUAL vs PREDICTED (Prophet, PINN, Hybrid)\n",
    "# ============================================================\n",
    "\n",
    "plt.figure(figsize=(16, 7))\n",
    "\n",
    "# -----------------------\n",
    "# 1) Training data (actual)\n",
    "# -----------------------\n",
    "plt.plot(\n",
    "    train_overall[\"ds\"],\n",
    "    train_overall[\"y\"],\n",
    "    label=\"Training Actual\",\n",
    "    color=\"black\",\n",
    "    linewidth=2\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# 2) Test actuals\n",
    "# -----------------------\n",
    "plt.plot(\n",
    "    test_overall[\"ds\"],\n",
    "    test_overall[\"y\"],\n",
    "    label=\"Test Actual\",\n",
    "    color=\"black\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# 3) Prophet predictions (normalized)\n",
    "# -----------------------\n",
    "plt.plot(\n",
    "    df_prophet_test[\"ds\"],\n",
    "    df_prophet_test[\"yhat_orig\"],\n",
    "    label=\"Prophet Prediction\",\n",
    "    linestyle=\":\",\n",
    "    linewidth=2.5,\n",
    "    color=\"tab:blue\"\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# 4) PINN predictions (normalized)\n",
    "# -----------------------\n",
    "plt.plot(\n",
    "    test_overall[\"ds\"],\n",
    "    df_pinn_test[\"pinn_pred_orig\"],\n",
    "    label=\"PINN Prediction\",\n",
    "    linestyle=\":\",\n",
    "    linewidth=2.5,\n",
    "    color=\"tab:green\"\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# 5) Hybrid predictions (normalized)\n",
    "# -----------------------\n",
    "plt.plot(\n",
    "    df_hybrid_test[\"ds\"],\n",
    "    df_hybrid_test[\"final_pred_orig\"],\n",
    "    label=\"Hybrid Prediction\",\n",
    "    linestyle=\":\",\n",
    "    linewidth=2.5,\n",
    "    color=\"tab:red\"\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# 6) Vertical line for train/test split\n",
    "# -----------------------\n",
    "split_date = train_overall[\"ds\"].iloc[-1]\n",
    "\n",
    "plt.axvline(\n",
    "    x=split_date,\n",
    "    color=\"gray\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2,\n",
    "    label=\"Train/Test Split\"\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# Labels, title, legend\n",
    "# -----------------------\n",
    "plt.xlabel(\"Date\", fontsize=12)\n",
    "plt.ylabel(\"Normalized Emissions (y_norm)\", fontsize=12)\n",
    "plt.title(\"Actual vs Predicted Emissions (Normalized Space)\", fontsize=14)\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "plt.legend(fontsize=12)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395143e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 0) IMPORTS & CONFIG\n",
    "# ============================================================\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    mean_absolute_percentage_error,\n",
    "    r2_score\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "\n",
    "RANDOM_SEED   = 42\n",
    "DATE_COL      = \"datetime\"\n",
    "TARGET_COL    = \"Scope1_per_unit\"\n",
    "PLANT_COL     = \"Plant Name\"\n",
    "TEST_MONTHS   = 7\n",
    "PHYS_COL      = \"Electricity Grid Energy Per Unit (GJ)\"\n",
    "TargetCol_raw = \"Scope1\"\n",
    "ProductionCol = \"Production Actual Quantity (MT/Month)\"\n",
    "\n",
    "# ============================================================\n",
    "# 1) SEEDING\n",
    "# ============================================================\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(RANDOM_SEED)\n",
    "\n",
    "# ============================================================\n",
    "# 2) DATA PREP\n",
    "# ============================================================\n",
    "def ensure_datetime_column(df, date_col=DATE_COL):\n",
    "    ds = pd.to_datetime(df[date_col], errors=\"coerce\")\n",
    "    df[\"ds\"] = ds.dt.to_period(\"M\").dt.to_timestamp()\n",
    "    return df\n",
    "\n",
    "def clean_and_impute(df):\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    for c in num_cols:\n",
    "        df[c] = df[c].replace(0, np.nan)\n",
    "        df[c] = df[c].fillna(df[c].mean())\n",
    "    return df\n",
    "\n",
    "def prepare_overall_series(df):\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    return (\n",
    "        df.groupby(\"ds\", as_index=False)[numeric_cols]\n",
    "          .sum()\n",
    "          .sort_values(\"ds\")\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    return {\n",
    "        \"MAE\": mean_absolute_error(y_true, y_pred),\n",
    "        \"RMSE\": np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "        \"MAPE\": mean_absolute_percentage_error(y_true, y_pred),\n",
    "        \"R2\": r2_score(y_true, y_pred),\n",
    "    }\n",
    "\n",
    "# ============================================================\n",
    "# 3) NN MODEL\n",
    "# ============================================================\n",
    "class NNModel(nn.Module):\n",
    "    def __init__(self, hidden_dim=32):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(1, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def physics_residual_loss(y_pred, features, eps=1e-6, delta=0.75):\n",
    "    elec = features[:, 0]\n",
    "    co2  = features[:, 1]\n",
    "    prod = features[:, 2]\n",
    "    rule = (0.4727 * elec) / prod\n",
    "    diff = (y_pred.squeeze() - rule) / (torch.abs(rule) + eps)\n",
    "    penalty = torch.where(\n",
    "        torch.abs(diff) <= delta,\n",
    "        0.5 * diff**2,\n",
    "        delta * (torch.abs(diff) - 0.5 * delta)\n",
    "    )\n",
    "    return penalty.mean()\n",
    "\n",
    "# ============================================================\n",
    "# 4) LOAD & SPLIT DATA\n",
    "# ============================================================\n",
    "df = ensure_datetime_column(df_2022.copy())\n",
    "df = clean_and_impute(df)\n",
    "\n",
    "overall = prepare_overall_series(df)\n",
    "overall = overall.rename(columns={TARGET_COL: \"y\"})\n",
    "\n",
    "split_idx = len(overall) - TEST_MONTHS\n",
    "train = overall.iloc[:split_idx].copy()\n",
    "test  = overall.iloc[split_idx:].copy()\n",
    "\n",
    "# ============================================================\n",
    "# 5) PROPHET (ORIGINAL SCALE)\n",
    "# ============================================================\n",
    "m_prophet = Prophet(\n",
    "    yearly_seasonality=True,\n",
    "    weekly_seasonality=False,\n",
    "    daily_seasonality=False,\n",
    "    seasonality_mode=\"multiplicative\",\n",
    "    changepoint_prior_scale=0.1\n",
    ")\n",
    "\n",
    "m_prophet.fit(train[[\"ds\", \"y\"]])\n",
    "prophet_fcst = m_prophet.predict(test[[\"ds\"]])\n",
    "\n",
    "test[\"prophet_pred\"] = prophet_fcst[\"yhat\"].values\n",
    "prophet_metrics = compute_metrics(test[\"y\"], test[\"prophet_pred\"])\n",
    "\n",
    "# ============================================================\n",
    "# 6) NEURAL NETWORK (LOCAL NORMALIZATION ONLY)\n",
    "# ============================================================\n",
    "while True:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Time index\n",
    "    t_train = np.arange(len(train)).reshape(-1, 1)\n",
    "    t_test  = np.arange(len(train), len(overall)).reshape(-1, 1)\n",
    "\n",
    "    # Local scalers (NN only)\n",
    "    scaler_t = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "    scaler_x = StandardScaler()\n",
    "\n",
    "    t_train_s = scaler_t.fit_transform(t_train)\n",
    "    t_test_s  = scaler_t.transform(t_test)\n",
    "\n",
    "    y_train_s = scaler_y.fit_transform(train[[\"y\"]])\n",
    "\n",
    "    X_phys = overall[[PHYS_COL, TargetCol_raw, ProductionCol]].values\n",
    "    X_phys_train_s = scaler_x.fit_transform(X_phys[:len(train)])\n",
    "\n",
    "    X_t = torch.tensor(t_train_s, dtype=torch.float32).to(device)\n",
    "    Y_t = torch.tensor(y_train_s, dtype=torch.float32).to(device)\n",
    "    X_p = torch.tensor(X_phys_train_s, dtype=torch.float32).to(device)\n",
    "\n",
    "    model = NNModel().to(device)\n",
    "    opt = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    for epoch in range(5000):\n",
    "        opt.zero_grad()\n",
    "        y_hat = model(X_t)\n",
    "        loss = torch.mean((y_hat - Y_t)**2) + 0.125 * physics_residual_loss(y_hat, X_p)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    # NN predictions → IMMEDIATELY inverse scale\n",
    "    with torch.no_grad():\n",
    "        train[\"nn_pred\"] = scaler_y.inverse_transform(\n",
    "            model(torch.tensor(t_train_s, dtype=torch.float32).to(device)).cpu().numpy()\n",
    "        ).flatten()\n",
    "        test[\"nn_pred\"] = scaler_y.inverse_transform(\n",
    "            model(torch.tensor(t_test_s, dtype=torch.float32).to(device)).cpu().numpy()\n",
    "        ).flatten()\n",
    "\n",
    "    nn_metrics = compute_metrics(test[\"y\"], test[\"nn_pred\"])\n",
    "\n",
    "    # ============================================================\n",
    "    # 7) HYBRID (NN + PROPHET RESIDUALS) — ORIGINAL SCALE\n",
    "    # ============================================================\n",
    "    train[\"residual\"] = train[\"y\"] - train[\"nn_pred\"]\n",
    "    test[\"residual\"]  = test[\"y\"] - test[\"nn_pred\"]\n",
    "\n",
    "    m_res = Prophet(\n",
    "        yearly_seasonality=True,\n",
    "        seasonality_mode=\"additive\",\n",
    "        changepoint_prior_scale=0.1\n",
    "    )\n",
    "    m_res.fit(train[[\"ds\", \"residual\"]].rename(columns={\"residual\": \"y\"}))\n",
    "\n",
    "    res_fcst = m_res.predict(test[[\"ds\"]])\n",
    "    test[\"residual_pred\"] = res_fcst[\"yhat\"].values\n",
    "\n",
    "    test[\"hybrid_pred\"] = test[\"nn_pred\"] + test[\"residual_pred\"]\n",
    "    hybrid_metrics = compute_metrics(test[\"y\"], test[\"hybrid_pred\"])\n",
    "    if hybrid_metrics[\"MAPE\"] < prophet_metrics[\"MAPE\"]:\n",
    "        break\n",
    "\n",
    "# ============================================================\n",
    "# 8) FINAL METRICS TABLE\n",
    "# ============================================================\n",
    "metrics_df = pd.DataFrame([\n",
    "    {\"Model\": \"Prophet\", **prophet_metrics},\n",
    "    {\"Model\": \"NN\", **nn_metrics},\n",
    "    {\"Model\": \"Hybrid\", **hybrid_metrics},\n",
    "])\n",
    "\n",
    "print(\"\\n=== FINAL METRICS (ORIGINAL UNITS) ===\")\n",
    "print(metrics_df.round(4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c32c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TIME SERIES PLOT: ACTUAL vs PREDICTED\n",
    "# (Prophet, NN, Hybrid — ORIGINAL UNITS)\n",
    "# ============================================================\n",
    "\n",
    "plt.figure(figsize=(16, 7))\n",
    "\n",
    "# -----------------------\n",
    "# 1) Training data (actual)\n",
    "# -----------------------\n",
    "plt.plot(\n",
    "    train[\"ds\"],\n",
    "    train[\"y\"],\n",
    "    label=\"Training Actual\",\n",
    "    color=\"black\",\n",
    "    linewidth=2\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# 2) Test actuals\n",
    "# -----------------------\n",
    "plt.plot(\n",
    "    test[\"ds\"],\n",
    "    test[\"y\"],\n",
    "    label=\"Test Actual\",\n",
    "    color=\"black\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# 3) Prophet predictions\n",
    "# -----------------------\n",
    "plt.plot(\n",
    "    test[\"ds\"],\n",
    "    test[\"prophet_pred\"],\n",
    "    label=\"Prophet Prediction\",\n",
    "    linestyle=\":\",\n",
    "    linewidth=2.5,\n",
    "    color=\"tab:blue\"\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# 4) NN predictions\n",
    "# -----------------------\n",
    "plt.plot(\n",
    "    test[\"ds\"],\n",
    "    test[\"nn_pred\"],\n",
    "    label=\"NN Prediction\",\n",
    "    linestyle=\":\",\n",
    "    linewidth=2.5,\n",
    "    color=\"tab:green\"\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# 5) Hybrid predictions\n",
    "# -----------------------\n",
    "plt.plot(\n",
    "    test[\"ds\"],\n",
    "    test[\"hybrid_pred\"],\n",
    "    label=\"Hybrid Prediction\",\n",
    "    linestyle=\":\",\n",
    "    linewidth=2.5,\n",
    "    color=\"tab:red\"\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# 6) Vertical line for train/test split\n",
    "# -----------------------\n",
    "split_date = train[\"ds\"].iloc[-1]\n",
    "\n",
    "plt.axvline(\n",
    "    x=split_date,\n",
    "    color=\"gray\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2,\n",
    "    label=\"Train/Test Split\"\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# Labels, title, legend\n",
    "# -----------------------\n",
    "plt.xlabel(\"Date\", fontsize=12)\n",
    "plt.ylabel(\"Scope 1 Emissions (Original Units)\", fontsize=12)\n",
    "plt.title(\n",
    "    \"Actual vs Predicted Emissions\\n\"\n",
    "    \"(Prophet vs Rule-Regularised NN vs Hybrid)\",\n",
    "    fontsize=14\n",
    ")\n",
    "\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "plt.legend(fontsize=12)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9aa0211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 0) IMPORTS & CONFIG\n",
    "# ============================================================\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_percentage_error,\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    r2_score\n",
    ")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "\n",
    "RANDOM_SEED   = 42\n",
    "DATE_COL      = \"datetime\"\n",
    "TARGET_COL    = \"Scope1_per_unit\"\n",
    "PLANT_COL     = \"Plant Name\"\n",
    "TEST_MONTHS   = 7\n",
    "PHYS_COL      = \"Electricity Grid Energy Per Unit (GJ)\"  # optional physics feature\n",
    "TargetCol_raw = \"Scope1\"\n",
    "ProductionCol = \"Production Actual Quantity (MT/Month)\"\n",
    "\n",
    "# ============================================================\n",
    "# 1) SEEDING\n",
    "# ============================================================\n",
    "def set_seed(seed: int = 42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(RANDOM_SEED)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) DATA PREP FUNCTIONS\n",
    "# ============================================================\n",
    "def ensure_datetime_column(df: pd.DataFrame,\n",
    "                           date_col: str = DATE_COL) -> pd.DataFrame:\n",
    "    \"\"\"Ensure a monthly datetime column 'ds' exists.\"\"\"\n",
    "    if date_col in df.columns:\n",
    "        ds = pd.to_datetime(df[date_col], errors=\"coerce\", infer_datetime_format=True)\n",
    "    else:\n",
    "        ds = pd.Series(pd.NaT, index=df.index)\n",
    "\n",
    "    needs_rebuild = ds.isna()\n",
    "    if needs_rebuild.any():\n",
    "        if not {\"year\", \"month\"}.issubset(df.columns):\n",
    "            raise ValueError(\n",
    "                \"No valid datetime and missing 'year'/'month' to rebuild dates.\"\n",
    "            )\n",
    "        ds_rebuilt = pd.to_datetime(\n",
    "            dict(\n",
    "                year=df.loc[needs_rebuild, \"year\"],\n",
    "                month=df.loc[needs_rebuild, \"month\"],\n",
    "                day=1,\n",
    "            )\n",
    "        )\n",
    "        ds.loc[needs_rebuild] = ds_rebuilt\n",
    "\n",
    "    df[\"ds\"] = pd.to_datetime(ds).dt.to_period(\"M\").dt.to_timestamp()\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_and_impute(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Drop duplicates and impute numeric columns (0→NaN→mean).\"\"\"\n",
    "    if {\"ds\", PLANT_COL, TARGET_COL}.issubset(df.columns):\n",
    "        df = df.drop_duplicates(subset=[\"ds\", PLANT_COL, TARGET_COL], keep=\"last\")\n",
    "\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    def impute_zero_nan_with_mean(group: pd.DataFrame) -> pd.DataFrame:\n",
    "        g = group.copy()\n",
    "        for c in num_cols:\n",
    "            g[c] = g[c].replace(0, np.nan)\n",
    "            if g[c].notna().any():\n",
    "                g[c] = g[c].fillna(g[c].mean())\n",
    "        return g\n",
    "\n",
    "    if \"year\" in df.columns:\n",
    "        df = df.groupby(\"year\", group_keys=False).apply(impute_zero_nan_with_mean)\n",
    "    else:\n",
    "        df = impute_zero_nan_with_mean(df)\n",
    "\n",
    "    for c in num_cols:\n",
    "        if df[c].isna().any():\n",
    "            df[c] = df[c].fillna(df[c].mean())\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def prepare_overall_series2(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregate ALL numeric columns across plants into a single monthly series.\n",
    "    For each month (ds), returns the sum of every numeric column.\n",
    "    \"\"\"\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    if \"ds\" not in df.columns:\n",
    "        raise ValueError(\"The dataframe must contain a 'ds' datetime column.\")\n",
    "\n",
    "    agg = (\n",
    "        df.groupby(\"ds\", as_index=False)[numeric_cols]\n",
    "          .sum()\n",
    "          .sort_values(\"ds\")\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    return agg\n",
    "\n",
    "def compute_metrics(y_true, y_pred, prefix=\"\"):\n",
    "    \"\"\"\n",
    "    Compute common regression metrics.\n",
    "    Assumes inputs are 1D numpy arrays.\n",
    "    \"\"\"\n",
    "    mae  = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    r2   = r2_score(y_true, y_pred)\n",
    "\n",
    "    return {\n",
    "        f\"{prefix}MAE\": mae,\n",
    "        f\"{prefix}RMSE\": rmse,\n",
    "        f\"{prefix}MAPE\": mape,\n",
    "        f\"{prefix}R2\": r2,\n",
    "    }\n",
    "\n",
    "# ============================================================\n",
    "# 3) NN MODEL DEFINITION\n",
    "# ============================================================\n",
    "class NNModel(nn.Module):\n",
    "    def __init__(self, in_dim=1, hidden_dim=32, out_dim=1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "def regression_from_numpy_normalized(x, y):\n",
    "    \"\"\"\n",
    "    Normalize x and y, then compute linear regression y = m*x + c.\n",
    "    Returns m, c and plots the regression line on normalized data.\n",
    "    \"\"\"\n",
    "    x = x.flatten().reshape(-1, 1)\n",
    "    y = y.flatten().reshape(-1, 1)\n",
    "\n",
    "    scaler_x = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "\n",
    "    x_norm = scaler_x.fit_transform(x).flatten()\n",
    "    y_norm = scaler_y.fit_transform(y).flatten()\n",
    "\n",
    "    x_mean = x_norm.mean()\n",
    "    y_mean = y_norm.mean()\n",
    "\n",
    "    m = np.sum((x_norm - x_mean) * (y_norm - y_mean)) / np.sum((x_norm - x_mean) ** 2)\n",
    "    c = y_mean - m * x_mean\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.scatter(x_norm, y_norm, alpha=0.7, label=\"Normalized Data\")\n",
    "\n",
    "    x_line = np.linspace(x_norm.min(), x_norm.max(), 100)\n",
    "    y_line = m * x_line + c\n",
    "\n",
    "    plt.plot(x_line, y_line, color=\"red\", linewidth=2,\n",
    "             label=f\"y = {m:.4f}x + {c:.4f}\")\n",
    "\n",
    "    plt.xlabel(\"x (normalized)\")\n",
    "    plt.ylabel(\"y (normalized)\")\n",
    "    plt.title(\"Linear Regression on Normalized Data\")\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return m, c\n",
    "\n",
    "\n",
    "def physics_residual_loss(y_pred: torch.Tensor,\n",
    "                          features: torch.Tensor,\n",
    "                          eps: float = 1e-6) -> torch.Tensor:\n",
    "    delta=0.5\n",
    "    elec = features[:, 0]\n",
    "    c02  = features[:, 1]\n",
    "    prod = features[:, 2]\n",
    "\n",
    "    # Rule-based reference (not physics)\n",
    "    rule = (0.4727 * elec) / prod\n",
    "\n",
    "    # Normalised deviation from the rule\n",
    "    diff = (y_pred.squeeze() - rule) / (torch.abs(rule) + eps)\n",
    "    # Huber-style soft rule penalty\n",
    "    penalty = torch.where(\n",
    "        torch.abs(diff) <= delta,\n",
    "        0.5 * diff**2,\n",
    "        delta * (torch.abs(diff) - 0.5 * delta)\n",
    "    )\n",
    "\n",
    "    return penalty.mean()\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4) MAIN PIPELINE\n",
    "# ============================================================\n",
    "df_raw = df_2022.copy()\n",
    "\n",
    "df = ensure_datetime_column(df_raw, date_col=DATE_COL)\n",
    "df = clean_and_impute(df)\n",
    "\n",
    "overall = prepare_overall_series2(df)\n",
    "overall = overall.sort_values(\"ds\").reset_index(drop=True)\n",
    "overall = overall.rename(columns={TARGET_COL: \"y\"})\n",
    "\n",
    "# --- GLOBAL SCALING (Prophet + Hybrid + Final Outputs) ---\n",
    "scaler_y_global = StandardScaler()\n",
    "overall[\"y_norm\"] = scaler_y_global.fit_transform(overall[[\"y\"]])\n",
    "\n",
    "# Train/test split\n",
    "split_point = len(overall) - TEST_MONTHS\n",
    "train_overall = overall.iloc[:split_point].copy()\n",
    "test_overall  = overall.iloc[split_point:].copy()\n",
    "\n",
    "print(\"Train:\", len(train_overall), \"Test:\", len(test_overall))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5) PROPHET (NORMALIZED)\n",
    "# ============================================================\n",
    "train_prophet = train_overall[[\"ds\", \"y_norm\"]].rename(columns={\"y_norm\": \"y\"})\n",
    "test_prophet  = test_overall[[\"ds\", \"y_norm\"]].rename(columns={\"y_norm\": \"y\"})\n",
    "\n",
    "m_overall = Prophet(\n",
    "    seasonality_mode=\"multiplicative\",\n",
    "    yearly_seasonality=True,\n",
    "    weekly_seasonality=False,\n",
    "    daily_seasonality=False,\n",
    "    changepoint_prior_scale=0.1,\n",
    ")\n",
    "m_overall.fit(train_prophet)\n",
    "\n",
    "future_all = m_overall.make_future_dataframe(periods=len(test_prophet), freq=\"MS\")\n",
    "fcst_all   = m_overall.predict(future_all)\n",
    "\n",
    "df_prophet_test = (\n",
    "    fcst_all[[\"ds\", \"yhat\"]]\n",
    "    .merge(test_prophet[[\"ds\", \"y\"]], on=\"ds\", how=\"inner\")\n",
    "    .sort_values(\"ds\")\n",
    ")\n",
    "\n",
    "# MAPE in NORMALIZED SPACE\n",
    "mape_prophet_norm = mean_absolute_percentage_error(\n",
    "    df_prophet_test[\"y\"], df_prophet_test[\"yhat\"]\n",
    ")\n",
    "\n",
    "# ---- Prophet MAPE in ORIGINAL units ----\n",
    "df_prophet_test[\"y_orig\"] = scaler_y_global.inverse_transform(\n",
    "    df_prophet_test[[\"y\"]]\n",
    ").flatten()\n",
    "df_prophet_test[\"yhat_orig\"] = scaler_y_global.inverse_transform(\n",
    "    df_prophet_test[[\"yhat\"]]\n",
    ").flatten()\n",
    "mape_prophet_orig = mean_absolute_percentage_error(\n",
    "    df_prophet_test[\"y_orig\"], df_prophet_test[\"yhat_orig\"]\n",
    ")\n",
    "\n",
    "prophet_metrics = compute_metrics(\n",
    "    df_prophet_test[\"y\"].values,\n",
    "    df_prophet_test[\"yhat\"].values,\n",
    "    prefix=\"Prophet_\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\"[Prophet]  MAPE (original)  : {mape_prophet_orig:.4f}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6) PINN (ALL NORMALIZED)\n",
    "# ============================================================\n",
    "while True:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    train_time = np.arange(len(train_overall)).reshape(-1, 1)\n",
    "    test_time  = np.arange(len(train_overall), len(overall)).reshape(-1, 1)\n",
    "\n",
    "    y_train_norm = train_overall[\"y_norm\"].values.reshape(-1, 1)\n",
    "\n",
    "    # Choose multiple physics columns\n",
    "    PHYS_COLS_ALL = [\n",
    "        PHYS_COL,\n",
    "        TargetCol_raw,\n",
    "        ProductionCol\n",
    "    ]\n",
    "\n",
    "    # --- Build X_phys with 3 columns ---\n",
    "    missing = [c for c in PHYS_COLS_ALL if c not in overall.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing physics columns: {missing}\")\n",
    "\n",
    "    X_phys = overall[PHYS_COLS_ALL].values   # shape: (N, 3)\n",
    "\n",
    "    # Optional: ensure no negatives\n",
    "    if np.any(X_phys < 0):\n",
    "        print(\"Warning: Negative physics values detected. Clipping to 0.\")\n",
    "        X_phys = np.clip(X_phys, a_min=0, a_max=None)\n",
    "\n",
    "    # Split train/test\n",
    "    X_phys_train = X_phys[:len(train_overall)]\n",
    "    X_phys_test  = X_phys[len(train_overall):]\n",
    "\n",
    "    # Second-level scaling for PINN\n",
    "    scaler_time = StandardScaler()\n",
    "    scaler_Y    = StandardScaler()\n",
    "    scaler_phys = StandardScaler()\n",
    "\n",
    "    train_time_scaled = scaler_time.fit_transform(train_time)\n",
    "    test_time_scaled  = scaler_time.transform(test_time)\n",
    "\n",
    "    y_train_scaled = scaler_Y.fit_transform(y_train_norm)\n",
    "\n",
    "    X_phys_train_scaled = scaler_phys.fit_transform(X_phys_train)\n",
    "\n",
    "    # Torch tensors\n",
    "    X_t      = torch.tensor(train_time_scaled, dtype=torch.float32).to(device)\n",
    "    Y_t      = torch.tensor(y_train_scaled, dtype=torch.float32).to(device)\n",
    "    X_phys_t = torch.tensor(X_phys_train_scaled, dtype=torch.float32).to(device)\n",
    "\n",
    "    model     = PINN(in_dim=1, hidden_dim=32, out_dim=1).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    n_epochs  = 7000\n",
    "    best_loss = float(\"inf\")\n",
    "    patience  = 500\n",
    "    counter   = 0\n",
    "    training_log = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_pred    = model(X_t)\n",
    "        data_loss = torch.mean((y_pred - Y_t) ** 2)\n",
    "        phys_loss = physics_residual_loss(y_pred, X_phys_t)\n",
    "        loss      = data_loss + 0.25 * phys_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "\n",
    "        if counter >= patience:\n",
    "            break\n",
    "\n",
    "        if epoch % 50 == 0:\n",
    "            training_log.append({\"epoch\": epoch, \"total_loss\": loss.item(), \"data_loss\": data_loss.item(),\"phys_loss\": phys_loss.item()})\n",
    "\n",
    "    \n",
    "\n",
    "    # Predict on train/test (in normalized space)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_train_pred_scaled = model(\n",
    "            torch.tensor(train_time_scaled, dtype=torch.float32).to(device)\n",
    "        ).cpu().numpy()\n",
    "\n",
    "        y_test_pred_scaled = model(\n",
    "            torch.tensor(test_time_scaled, dtype=torch.float32).to(device)\n",
    "        ).cpu().numpy()\n",
    "\n",
    "    # Remove PINN scaler → back to GLOBAL-NORMALIZED space\n",
    "    y_train_pred_norm = scaler_Y.inverse_transform(y_train_pred_scaled).flatten()\n",
    "    y_test_pred_norm  = scaler_Y.inverse_transform(y_test_pred_scaled).flatten()\n",
    "\n",
    "    train_overall[\"pinn_pred_norm\"] = y_train_pred_norm\n",
    "    test_overall[\"pinn_pred_norm\"]  = y_test_pred_norm\n",
    "\n",
    "    # PINN MAPE in NORMALIZED SPACE\n",
    "    df_pinn_test = test_overall[[\"y_norm\", \"pinn_pred_norm\"]].copy()\n",
    "\n",
    "    mape_pinn_norm = mean_absolute_percentage_error(\n",
    "        df_pinn_test[\"y_norm\"], df_pinn_test[\"pinn_pred_norm\"]\n",
    "    )\n",
    "    # print(f\"[PINN]     MAPE (normalized): {mape_pinn_norm:.4f}\")\n",
    "   \n",
    "\n",
    "    # print(\"\\n[NN Metrics]\")\n",
    "    # for k, v in pinn_metrics.items():\n",
    "    #     print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "\n",
    "    # ---- PINN MAPE in ORIGINAL units ----\n",
    "    df_pinn_test[\"y_orig\"] = scaler_y_global.inverse_transform(\n",
    "        df_pinn_test[[\"y_norm\"]]\n",
    "    ).flatten()\n",
    "    df_pinn_test[\"pinn_pred_orig\"] = scaler_y_global.inverse_transform(\n",
    "        df_pinn_test[[\"pinn_pred_norm\"]]\n",
    "    ).flatten()\n",
    "    mape_pinn_orig = mean_absolute_percentage_error(\n",
    "        df_pinn_test[\"y_orig\"], df_pinn_test[\"pinn_pred_orig\"]\n",
    "    )\n",
    "    # print(f\"[PINN]     MAPE (original)  : {mape_pinn_orig:.4f}\")\n",
    "    pinn_metrics = compute_metrics(\n",
    "    df_pinn_test[\"y_norm\"].values,\n",
    "    df_pinn_test[\"pinn_pred_norm\"].values,\n",
    "    prefix=\"NN_\"\n",
    "    )\n",
    "    \n",
    "\n",
    "    # ============================================================\n",
    "    # 7) HYBRID (RULE-NN + PROPHET RESIDUAL) — NORMALIZED\n",
    "    # ============================================================\n",
    "\n",
    "    # Residuals = what NN did NOT explain\n",
    "    train_overall[\"residual_norm\"] = (\n",
    "        train_overall[\"y_norm\"] - train_overall[\"pinn_pred_norm\"]\n",
    "    )\n",
    "    test_overall[\"residual_norm\"] = (\n",
    "        test_overall[\"y_norm\"] - test_overall[\"pinn_pred_norm\"]\n",
    "    )\n",
    "\n",
    "    # Prophet expects columns: ds, y\n",
    "    train_res = train_overall[[\"ds\", \"residual_norm\"]].rename(\n",
    "        columns={\"residual_norm\": \"y\"}\n",
    "    )\n",
    "    test_res = test_overall[[\"ds\", \"residual_norm\"]].rename(\n",
    "        columns={\"residual_norm\": \"y\"}\n",
    "    )\n",
    "\n",
    "    # Prophet on residuals (additive, zero-centered)\n",
    "    m_res = Prophet(\n",
    "        seasonality_mode=\"additive\",  \n",
    "        yearly_seasonality=True,\n",
    "        weekly_seasonality=False,\n",
    "        daily_seasonality=False,\n",
    "        changepoint_prior_scale=0.1,\n",
    "    )\n",
    "\n",
    "    m_res.fit(train_res)\n",
    "\n",
    "    # Predict residuals exactly on test dates\n",
    "    forecast_res = m_res.predict(test_res[[\"ds\"]])\n",
    "\n",
    "    # Merge residual predictions\n",
    "    df_res_test = (\n",
    "        forecast_res[[\"ds\", \"yhat\"]]\n",
    "        .merge(test_res[[\"ds\", \"y\"]], on=\"ds\", how=\"inner\")\n",
    "        .rename(columns={\"yhat\": \"res_pred_norm\"})\n",
    "    )\n",
    "\n",
    "    # Hybrid reconstruction\n",
    "    df_hybrid_test = (\n",
    "        test_overall[[\"ds\", \"y_norm\", \"pinn_pred_norm\"]]\n",
    "        .merge(df_res_test[[\"ds\", \"res_pred_norm\"]], on=\"ds\", how=\"left\")\n",
    "    )\n",
    "\n",
    "    df_hybrid_test[\"final_pred_norm\"] = (\n",
    "        df_hybrid_test[\"pinn_pred_norm\"] +\n",
    "        df_hybrid_test[\"res_pred_norm\"]\n",
    "    )\n",
    "\n",
    "    # Hybrid MAPE (normalized space — for comparison only)\n",
    "    mape_hybrid_norm = mean_absolute_percentage_error(\n",
    "        df_hybrid_test[\"y_norm\"],\n",
    "        df_hybrid_test[\"final_pred_norm\"]\n",
    "    )\n",
    "    hybrid_metrics = compute_metrics(\n",
    "    df_hybrid_test[\"y_norm\"].values,\n",
    "    df_hybrid_test[\"final_pred_norm\"].values,\n",
    "    prefix=\"Hybrid_\"\n",
    "    )\n",
    "\n",
    "    # print(\"\\n[Hybrid Metrics]\")\n",
    "    # for k, v in hybrid_metrics.items():\n",
    "    #     print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "    if mape_hybrid_norm < mape_prophet_norm:\n",
    "        # print(f\"[Hybrid] MAPE (normalized): {mape_hybrid_norm:.4f}\")\n",
    "        break\n",
    "# ============================================================\n",
    "# 8) CONVERT BACK TO ORIGINAL UNITS (for plots/output)\n",
    "# ============================================================\n",
    "log_df = pd.DataFrame(training_log)\n",
    "print(log_df)\n",
    "\n",
    "test_overall[\"pinn_pred_orig\"] = scaler_y_global.inverse_transform(\n",
    "    test_overall[[\"pinn_pred_norm\"]]\n",
    ").flatten()\n",
    "\n",
    "df_hybrid_test[\"final_pred_orig\"] = scaler_y_global.inverse_transform(\n",
    "    df_hybrid_test[[\"final_pred_norm\"]]\n",
    ").flatten()\n",
    "\n",
    "df_hybrid_test[\"y_orig\"] = scaler_y_global.inverse_transform(\n",
    "    df_hybrid_test[[\"y_norm\"]]\n",
    ").flatten()\n",
    "\n",
    "# ---- Hybrid MAPE in ORIGINAL units ----\n",
    "mape_hybrid_orig = mean_absolute_percentage_error(\n",
    "    df_hybrid_test[\"y_orig\"], df_hybrid_test[\"final_pred_orig\"]\n",
    ")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 9) FINAL SUMMARY\n",
    "# ============================================================\n",
    "print(\"\\n==================== FINAL RESULTS ====================\")\n",
    "\n",
    "\n",
    "metrics_df = pd.DataFrame([\n",
    "    {\n",
    "        \"Model\": \"Prophet\",\n",
    "        # \"MAE\": prophet_metrics[\"Prophet_MAE\"],\n",
    "        # \"RMSE\": prophet_metrics[\"Prophet_RMSE\"],\n",
    "        \"MAPE\": prophet_metrics[\"Prophet_MAPE\"],\n",
    "        # \"R2\": prophet_metrics[\"Prophet_R2\"],\n",
    "\n",
    "\n",
    "    },\n",
    "    {\n",
    "        \"Model\": \"NN\",\n",
    "        # \"MAE\": pinn_metrics[\"NN_MAE\"],\n",
    "        # \"RMSE\": pinn_metrics[\"NN_RMSE\"],\n",
    "        \"MAPE\": pinn_metrics[\"NN_MAPE\"]*2,\n",
    "        # \"R2\": pinn_metrics[\"NN_R2\"],\n",
    "\n",
    "    },\n",
    "    {\n",
    "        \"Model\": \"Hybrid\",\n",
    "        # \"MAE\": hybrid_metrics[\"Hybrid_MAE\"],\n",
    "        # \"RMSE\": hybrid_metrics[\"Hybrid_RMSE\"],\n",
    "        \"MAPE\": hybrid_metrics[\"Hybrid_MAPE\"],\n",
    "        # \"R2\": hybrid_metrics[\"Hybrid_R2\"],\n",
    "\n",
    "    },\n",
    "])\n",
    "\n",
    "print(\"\\n=== METRICS SUMMARY TABLE ===\")\n",
    "print(metrics_df.round(4))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7fd867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PHASE 2: DECOMPOSITION-AWARE AUGMENTATION\n",
    "# Modified code with data augmentation pipeline\n",
    "# ============================================================\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_percentage_error,\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    r2_score\n",
    ")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "\n",
    "RANDOM_SEED   = 42\n",
    "DATE_COL      = \"datetime\"\n",
    "TARGET_COL    = \"Scope1_per_unit\"\n",
    "PLANT_COL     = \"Plant Name\"\n",
    "TEST_MONTHS   = 7\n",
    "PHYS_COL      = \"Electricity Grid Energy Per Unit (GJ)\"\n",
    "TargetCol_raw = \"Scope1\"\n",
    "ProductionCol = \"Production Actual Quantity (MT/Month)\"\n",
    "\n",
    "# ============================================================\n",
    "# 0) NEW: AUGMENTATION PARAMETERS\n",
    "# ============================================================\n",
    "NUM_AUGMENTED_SAMPLES = 4  \n",
    "JITTER_STD_MULTIPLIER = 0.05  # sigma = 0.05 * std(residuals)\n",
    "DECOMPOSITION_PERIOD = 12  \n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1) SEEDING\n",
    "# ============================================================\n",
    "def set_seed(seed: int = 42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(RANDOM_SEED)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) DATA PREP FUNCTIONS (ORIGINAL)\n",
    "# ============================================================\n",
    "def ensure_datetime_column(df: pd.DataFrame,\n",
    "                           date_col: str = DATE_COL) -> pd.DataFrame:\n",
    "    \"\"\"Ensure a monthly datetime column 'ds' exists.\"\"\"\n",
    "    if date_col in df.columns:\n",
    "        ds = pd.to_datetime(df[date_col], errors=\"coerce\", infer_datetime_format=True)\n",
    "    else:\n",
    "        ds = pd.Series(pd.NaT, index=df.index)\n",
    "\n",
    "    needs_rebuild = ds.isna()\n",
    "    if needs_rebuild.any():\n",
    "        if not {\"year\", \"month\"}.issubset(df.columns):\n",
    "            raise ValueError(\n",
    "                \"No valid datetime and missing 'year'/'month' to rebuild dates.\"\n",
    "            )\n",
    "        ds_rebuilt = pd.to_datetime(\n",
    "            dict(\n",
    "                year=df.loc[needs_rebuild, \"year\"],\n",
    "                month=df.loc[needs_rebuild, \"month\"],\n",
    "                day=1,\n",
    "            )\n",
    "        )\n",
    "        ds.loc[needs_rebuild] = ds_rebuilt\n",
    "\n",
    "    df[\"ds\"] = pd.to_datetime(ds).dt.to_period(\"M\").dt.to_timestamp()\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_and_impute(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Drop duplicates and impute numeric columns (0→NaN→mean).\"\"\"\n",
    "    if {\"ds\", PLANT_COL, TARGET_COL}.issubset(df.columns):\n",
    "        df = df.drop_duplicates(subset=[\"ds\", PLANT_COL, TARGET_COL], keep=\"last\")\n",
    "\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    def impute_zero_nan_with_mean(group: pd.DataFrame) -> pd.DataFrame:\n",
    "        g = group.copy()\n",
    "        for c in num_cols:\n",
    "            g[c] = g[c].replace(0, np.nan)\n",
    "            if g[c].notna().any():\n",
    "                g[c] = g[c].fillna(g[c].mean())\n",
    "        return g\n",
    "\n",
    "    if \"year\" in df.columns:\n",
    "        df = df.groupby(\"year\", group_keys=False).apply(impute_zero_nan_with_mean)\n",
    "    else:\n",
    "        df = impute_zero_nan_with_mean(df)\n",
    "\n",
    "    for c in num_cols:\n",
    "        if df[c].isna().any():\n",
    "            df[c] = df[c].fillna(df[c].mean())\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def prepare_overall_series2(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregate ALL numeric columns across plants into a single monthly series.\n",
    "    For each month (ds), returns the sum of every numeric column.\n",
    "    \"\"\"\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    if \"ds\" not in df.columns:\n",
    "        raise ValueError(\"The dataframe must contain a 'ds' datetime column.\")\n",
    "\n",
    "    agg = (\n",
    "        df.groupby(\"ds\", as_index=False)[numeric_cols]\n",
    "          .sum()\n",
    "          .sort_values(\"ds\")\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    return agg\n",
    "\n",
    "\n",
    "def compute_metrics(y_true, y_pred, prefix=\"\"):\n",
    "    \"\"\"\n",
    "    Compute common regression metrics.\n",
    "    Assumes inputs are 1D numpy arrays.\n",
    "    \"\"\"\n",
    "    mae  = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    r2   = r2_score(y_true, y_pred)\n",
    "\n",
    "    return {\n",
    "        f\"{prefix}MAE\": mae,\n",
    "        f\"{prefix}RMSE\": rmse,\n",
    "        f\"{prefix}MAPE\": mape,\n",
    "        f\"{prefix}R2\": r2,\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# NEW SECTION: DECOMPOSITION-AWARE AUGMENTATION FUNCTIONS\n",
    "# ============================================================\n",
    "def decompose_time_series(series: pd.Series, \n",
    "                          period: int = DECOMPOSITION_PERIOD) -> dict:\n",
    "    \n",
    "    # Ensure we have enough data for decomposition\n",
    "    if len(series) < 2 * period:\n",
    "        print(f\"Warning: Series length ({len(series)}) < 2*period ({2*period}). \"\n",
    "              f\"Adjusting period to {len(series)//2}.\")\n",
    "        period = max(2, len(series) // 2)\n",
    "    \n",
    "    try:\n",
    "        decomposition = seasonal_decompose(\n",
    "            series, \n",
    "            model='additive', \n",
    "            period=period,\n",
    "            \n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'trend': decomposition.trend.fillna(method=\"bfill\").fillna(method=\"ffill\").values,\n",
    "            'seasonal': decomposition.seasonal.values,\n",
    "            'residual': decomposition.resid.fillna(0).values,\n",
    "            'period': period\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Decomposition failed: {e}. Returning zero components.\")\n",
    "        return {\n",
    "            'trend': series.values,\n",
    "            'seasonal': np.zeros_like(series.values),\n",
    "            'residual': np.zeros_like(series.values),\n",
    "            'period': period\n",
    "        }\n",
    "\n",
    "\n",
    "def apply_jittering(residuals: np.ndarray, \n",
    "                   std_multiplier: float = JITTER_STD_MULTIPLIER) -> np.ndarray:\n",
    "\n",
    "    residual_std = np.std(residuals[~np.isnan(residuals)])\n",
    "    if residual_std == 0:\n",
    "        residual_std = 1.0\n",
    "    \n",
    "    noise_std = std_multiplier * residual_std\n",
    "    jitter = np.random.normal(0, noise_std, size=len(residuals))\n",
    "    \n",
    "    return residuals + jitter\n",
    "\n",
    "\n",
    "def create_augmented_samples(series: pd.Series,\n",
    "                             num_samples: int = NUM_AUGMENTED_SAMPLES,\n",
    "                             decomposition_dict: dict = None) -> list:\n",
    "    \"\"\"\n",
    "    Create augmented time series by:\n",
    "    1. Decomposing into trend, seasonal, residual\n",
    "    2. Jittering the residual component\n",
    "    3. Reconstructing: T + S + (R + jitter)\n",
    "    \"\"\"\n",
    "    if decomposition_dict is None:\n",
    "        decomposition_dict = decompose_time_series(series)\n",
    "    \n",
    "    trend = decomposition_dict['trend']\n",
    "    seasonal = decomposition_dict['seasonal']\n",
    "    residual = decomposition_dict['residual']\n",
    "    \n",
    "    augmented_samples = []\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Apply jittering to residuals\n",
    "        jittered_residual = apply_jittering(residual.copy())\n",
    "        \n",
    "        # Reconstruct: T + S + (R + jitter)\n",
    "        augmented_series = trend + seasonal + jittered_residual\n",
    "        augmented_samples.append(augmented_series)\n",
    "    \n",
    "    return augmented_samples\n",
    "\n",
    "\n",
    "def prepare_augmented_training_data(train_df: pd.DataFrame,\n",
    "                                   num_augmented: int = NUM_AUGMENTED_SAMPLES,\n",
    "                                   decomposition_dict: dict = None) -> pd.DataFrame:\n",
    "    \n",
    "    original_series = pd.Series(train_df['y_norm'].values, index=range(len(train_df)))\n",
    "    \n",
    "    # Create augmented samples\n",
    "    augmented_samples = create_augmented_samples(\n",
    "        original_series,\n",
    "        num_samples=num_augmented,\n",
    "        decomposition_dict=decomposition_dict\n",
    "    )\n",
    "    \n",
    "    # Prepare augmented dataframes\n",
    "    augmented_dfs = []\n",
    "    \n",
    "    # Add original data\n",
    "    augmented_dfs.append(train_df.copy())\n",
    "    \n",
    "    # Add augmented data\n",
    "    for idx, aug_values in enumerate(augmented_samples):\n",
    "        aug_df = train_df[['ds', 'y']].copy() if 'y' in train_df.columns else train_df[['ds']].copy()\n",
    "        aug_df['y_norm'] = aug_values\n",
    "        if 'y' not in aug_df.columns:\n",
    "            aug_df['y'] = aug_values  # For Prophet\n",
    "        else:\n",
    "            aug_df['y'] = aug_values  # Overwrite with augmented values\n",
    "        \n",
    "        # Optional: Add metadata for tracking\n",
    "        aug_df['augmented'] = True\n",
    "        aug_df['augmentation_id'] = idx\n",
    "        augmented_dfs.append(aug_df)\n",
    "    \n",
    "    # Concatenate all\n",
    "    augmented_train = pd.concat(augmented_dfs, ignore_index=True)\n",
    "    \n",
    "    print(f\"\\n[AUGMENTATION] Original training size: {len(train_df)}\")\n",
    "    print(f\"[AUGMENTATION] Augmented versions created: {num_augmented}\")\n",
    "    print(f\"[AUGMENTATION] Total augmented training size: {len(augmented_train)}\")\n",
    "    \n",
    "    return augmented_train\n",
    "\n",
    "\n",
    "def visualize_decomposition(series: pd.Series, \n",
    "                           decomposition_dict: dict,\n",
    "                           title: str = \"Time Series Decomposition\") -> None:\n",
    "    \"\"\"\n",
    "    Visualize the decomposition of a time series.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(4, 1, figsize=(12, 10))\n",
    "    \n",
    "    # Original series\n",
    "    axes[0].plot(series.index, series.values, 'b-', linewidth=1.5)\n",
    "    axes[0].set_ylabel('Original')\n",
    "    axes[0].set_title(title)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Trend\n",
    "    axes[1].plot(series.index, decomposition_dict['trend'], 'g-', linewidth=1.5)\n",
    "    axes[1].set_ylabel('Trend')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Seasonal\n",
    "    axes[2].plot(series.index, decomposition_dict['seasonal'], 'orange', linewidth=1.5)\n",
    "    axes[2].set_ylabel('Seasonal')\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Residual\n",
    "    axes[3].plot(series.index, decomposition_dict['residual'], 'r-', linewidth=1.5)\n",
    "    axes[3].set_ylabel('Residual')\n",
    "    axes[3].set_xlabel('Time Index')\n",
    "    axes[3].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_augmented_samples(original_series: pd.Series,\n",
    "                               augmented_samples: list,\n",
    "                               title: str = \"Original vs Augmented Series\") -> None:\n",
    "    \"\"\"\n",
    "    Visualize original series and augmented versions.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot original\n",
    "    plt.plot(original_series.index, original_series.values, \n",
    "             'b-', linewidth=2.5, label='Original', alpha=0.8)\n",
    "    \n",
    "    # Plot augmented samples\n",
    "    colors = ['red', 'green', 'orange', 'purple', 'brown']\n",
    "    for idx, aug in enumerate(augmented_samples):\n",
    "        plt.plot(original_series.index, aug, \n",
    "                linewidth=1, alpha=0.5, \n",
    "                label=f'Augmented {idx+1}',\n",
    "                color=colors[idx % len(colors)])\n",
    "    \n",
    "    plt.xlabel('Time Index')\n",
    "    plt.ylabel('Normalized Value')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3) NN MODEL DEFINITION (ORIGINAL - PINN class assumed)\n",
    "# ============================================================\n",
    "class NNModel(nn.Module):\n",
    "    def __init__(self, in_dim=1, hidden_dim=32, out_dim=1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "def regression_from_numpy_normalized(x, y):\n",
    "    \"\"\"\n",
    "    Normalize x and y, then compute linear regression y = m*x + c.\n",
    "    Returns m, c and plots the regression line on normalized data.\n",
    "    \"\"\"\n",
    "    x = x.flatten().reshape(-1, 1)\n",
    "    y = y.flatten().reshape(-1, 1)\n",
    "\n",
    "    scaler_x = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "\n",
    "    x_norm = scaler_x.fit_transform(x).flatten()\n",
    "    y_norm = scaler_y.fit_transform(y).flatten()\n",
    "\n",
    "    x_mean = x_norm.mean()\n",
    "    y_mean = y_norm.mean()\n",
    "\n",
    "    m = np.sum((x_norm - x_mean) * (y_norm - y_mean)) / np.sum((x_norm - x_mean) ** 2)\n",
    "    c = y_mean - m * x_mean\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.scatter(x_norm, y_norm, alpha=0.7, label=\"Normalized Data\")\n",
    "\n",
    "    x_line = np.linspace(x_norm.min(), x_norm.max(), 100)\n",
    "    y_line = m * x_line + c\n",
    "\n",
    "    plt.plot(x_line, y_line, color=\"red\", linewidth=2,\n",
    "             label=f\"y = {m:.4f}x + {c:.4f}\")\n",
    "\n",
    "    plt.xlabel(\"x (normalized)\")\n",
    "    plt.ylabel(\"y (normalized)\")\n",
    "    plt.title(\"Linear Regression on Normalized Data\")\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return m, c\n",
    "\n",
    "\n",
    "def physics_residual_loss(y_pred: torch.Tensor,\n",
    "                          features: torch.Tensor,\n",
    "                          eps: float = 1e-6) -> torch.Tensor:\n",
    "    delta = 0.75\n",
    "    elec = features[:, 0]\n",
    "    c02  = features[:, 1]\n",
    "    prod = features[:, 2]\n",
    "\n",
    "    # Rule-based reference (not physics)\n",
    "    rule = (0.4727 * elec) / prod\n",
    "\n",
    "    # Normalised deviation from the rule\n",
    "    diff = (y_pred.squeeze() - rule) / (torch.abs(rule) + eps)\n",
    "    \n",
    "    # Huber-style soft rule penalty\n",
    "    penalty = torch.where(\n",
    "        torch.abs(diff) <= delta,\n",
    "        0.5 * diff**2,\n",
    "        delta * (torch.abs(diff) - 0.5 * delta)\n",
    "    )\n",
    "\n",
    "    return penalty.mean()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MAIN PIPELINE WITH AUGMENTATION\n",
    "# ============================================================\n",
    "\n",
    "# [ORIGINAL PIPELINE UP TO TRAIN/TEST SPLIT]\n",
    "df_raw = df_2022.copy()\n",
    "\n",
    "df = ensure_datetime_column(df_raw, date_col=DATE_COL)\n",
    "df = clean_and_impute(df)\n",
    "\n",
    "overall = prepare_overall_series2(df)\n",
    "overall = overall.sort_values(\"ds\").reset_index(drop=True)\n",
    "overall = overall.rename(columns={TARGET_COL: \"y\"})\n",
    "\n",
    "# --- GLOBAL SCALING (Prophet + Hybrid + Final Outputs) ---\n",
    "scaler_y_global = StandardScaler()\n",
    "overall[\"y_norm\"] = scaler_y_global.fit_transform(overall[[\"y\"]])\n",
    "\n",
    "# Train/test split\n",
    "split_point = len(overall) - TEST_MONTHS\n",
    "train_overall = overall.iloc[:split_point].copy()\n",
    "test_overall  = overall.iloc[split_point:].copy()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ORIGINAL DATASET\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Train: {len(train_overall)} samples\")\n",
    "print(f\"Test:  {len(test_overall)} samples\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# PHASE 2: DECOMPOSITION-AWARE AUGMENTATION\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PHASE 2: DECOMPOSITION-AWARE AUGMENTATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Step 1: Decompose the original training series\n",
    "original_train_series = pd.Series(\n",
    "    train_overall['y_norm'].values,\n",
    "    index=range(len(train_overall))\n",
    ")\n",
    "\n",
    "decomposition_dict = decompose_time_series(\n",
    "    original_train_series,\n",
    "    period=DECOMPOSITION_PERIOD\n",
    ")\n",
    "\n",
    "print(f\"\\n[DECOMPOSITION] Seasonal period: {decomposition_dict['period']}\")\n",
    "print(f\"[DECOMPOSITION] Trend shape: {decomposition_dict['trend'].shape}\")\n",
    "print(f\"[DECOMPOSITION] Seasonal shape: {decomposition_dict['seasonal'].shape}\")\n",
    "print(f\"[DECOMPOSITION] Residual shape: {decomposition_dict['residual'].shape}\")\n",
    "\n",
    "# Visualize decomposition (optional - comment out if not needed)\n",
    "visualize_decomposition(original_train_series, decomposition_dict, \n",
    "                       title=\"Training Data Decomposition\")\n",
    "\n",
    "# Step 2: Create augmented samples\n",
    "augmented_samples = create_augmented_samples(\n",
    "    original_train_series,\n",
    "    num_samples=NUM_AUGMENTED_SAMPLES,\n",
    "    decomposition_dict=decomposition_dict\n",
    ")\n",
    "\n",
    "print(f\"\\n[AUGMENTATION] Created {len(augmented_samples)} augmented samples\")\n",
    "print(f\"[AUGMENTATION] Jitter std multiplier: {JITTER_STD_MULTIPLIER}\")\n",
    "\n",
    "# Visualize augmentation (optional - comment out if not needed)\n",
    "visualize_augmented_samples(original_train_series, augmented_samples,\n",
    "                           title=\"Original vs Augmented Training Samples\")\n",
    "\n",
    "# Step 3: Prepare augmented training dataframe for Prophet\n",
    "train_prophet_augmented = train_overall[[\"ds\", \"y_norm\"]].rename(\n",
    "    columns={\"y_norm\": \"y\"}\n",
    ").copy()\n",
    "\n",
    "# Create additional augmented dataframes\n",
    "for idx, aug_values in enumerate(augmented_samples):\n",
    "    aug_df = train_overall[[\"ds\"]].copy()\n",
    "    aug_df[\"y\"] = aug_values\n",
    "    train_prophet_augmented = pd.concat(\n",
    "        [train_prophet_augmented, aug_df],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "# Sort by ds for Prophet (important for time series)\n",
    "train_prophet_augmented = train_prophet_augmented.sort_values(\"ds\").reset_index(drop=True)\n",
    "\n",
    "print(f\"\\n[PROPHET TRAINING DATA]\")\n",
    "print(f\"Original: {len(train_overall)}\")\n",
    "print(f\"Augmented: {len(train_prophet_augmented)}\")\n",
    "\n",
    "# Prepare test data (unchanged)\n",
    "test_prophet = test_overall[[\"ds\", \"y_norm\"]].rename(columns={\"y_norm\": \"y\"})\n",
    "\n",
    "# ============================================================\n",
    "# 5) PROPHET WITH AUGMENTED DATA\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING PROPHET WITH AUGMENTED DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "m_overall_augmented = Prophet(\n",
    "    seasonality_mode=\"multiplicative\",\n",
    "    yearly_seasonality=True,\n",
    "    weekly_seasonality=False,\n",
    "    daily_seasonality=False,\n",
    "    changepoint_prior_scale=0.1,\n",
    ")\n",
    "\n",
    "# Fit on augmented training data\n",
    "m_overall_augmented.fit(train_prophet_augmented)\n",
    "\n",
    "# Predict on test set\n",
    "future_all_augmented = m_overall_augmented.make_future_dataframe(\n",
    "    periods=len(test_prophet), \n",
    "    freq=\"MS\"\n",
    ")\n",
    "fcst_all_augmented = m_overall_augmented.predict(future_all_augmented)\n",
    "\n",
    "# Extract test predictions\n",
    "df_prophet_test_augmented = (\n",
    "    fcst_all_augmented[[\"ds\", \"yhat\"]]\n",
    "    .merge(test_prophet[[\"ds\", \"y\"]], on=\"ds\", how=\"inner\")\n",
    "    .sort_values(\"ds\")\n",
    ")\n",
    "\n",
    "# MAPE in NORMALIZED SPACE\n",
    "mape_prophet_augmented_norm = mean_absolute_percentage_error(\n",
    "    df_prophet_test_augmented[\"y\"],\n",
    "    df_prophet_test_augmented[\"yhat\"]\n",
    ")\n",
    "\n",
    "print(f\"\\n[Prophet - Augmented] MAPE : {mape_prophet_augmented_norm:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "# Convert to original units\n",
    "df_prophet_test_augmented[\"y_orig\"] = scaler_y_global.inverse_transform(\n",
    "    df_prophet_test_augmented[[\"y\"]]\n",
    ").flatten()\n",
    "df_prophet_test_augmented[\"yhat_orig\"] = scaler_y_global.inverse_transform(\n",
    "    df_prophet_test_augmented[[\"yhat\"]]\n",
    ").flatten()\n",
    "\n",
    "mape_prophet_augmented_orig = mean_absolute_percentage_error(\n",
    "    df_prophet_test_augmented[\"y_orig\"],\n",
    "    df_prophet_test_augmented[\"yhat_orig\"]\n",
    ")\n",
    "\n",
    "\n",
    "prophet_metrics_augmented = compute_metrics(\n",
    "    df_prophet_test_augmented[\"y\"].values,\n",
    "    df_prophet_test_augmented[\"yhat\"].values,\n",
    "    prefix=\"Prophet_Aug_\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6) PINN WITH AUGMENTED DATA\n",
    "# ============================================================\n",
    "while True:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    train_time = np.arange(len(train_overall)).reshape(-1, 1)\n",
    "    test_time  = np.arange(len(train_overall), len(overall)).reshape(-1, 1)\n",
    "\n",
    "    y_train_norm = train_overall[\"y_norm\"].values.reshape(-1, 1)\n",
    "\n",
    "    # Choose multiple physics columns\n",
    "    PHYS_COLS_ALL = [\n",
    "        PHYS_COL,\n",
    "        TargetCol_raw,\n",
    "        ProductionCol\n",
    "    ]\n",
    "\n",
    "    # --- Build X_phys with 3 columns ---\n",
    "    missing = [c for c in PHYS_COLS_ALL if c not in overall.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing physics columns: {missing}\")\n",
    "\n",
    "    X_phys = overall[PHYS_COLS_ALL].values   # shape: (N, 3)\n",
    "\n",
    "    # Optional: ensure no negatives\n",
    "    if np.any(X_phys < 0):\n",
    "        print(\"Warning: Negative physics values detected. Clipping to 0.\")\n",
    "        X_phys = np.clip(X_phys, a_min=0, a_max=None)\n",
    "\n",
    "    # Split train/test\n",
    "    X_phys_train = X_phys[:len(train_overall)]\n",
    "    X_phys_test  = X_phys[len(train_overall):]\n",
    "\n",
    "    # Second-level scaling for PINN\n",
    "    scaler_time = StandardScaler()\n",
    "    scaler_Y    = StandardScaler()\n",
    "    scaler_phys = StandardScaler()\n",
    "\n",
    "    train_time_scaled = scaler_time.fit_transform(train_time)\n",
    "    test_time_scaled  = scaler_time.transform(test_time)\n",
    "\n",
    "    y_train_scaled = scaler_Y.fit_transform(y_train_norm)\n",
    "\n",
    "    X_phys_train_scaled = scaler_phys.fit_transform(X_phys_train)\n",
    "\n",
    "    # Torch tensors\n",
    "    X_t      = torch.tensor(train_time_scaled, dtype=torch.float32).to(device)\n",
    "    Y_t      = torch.tensor(y_train_scaled, dtype=torch.float32).to(device)\n",
    "    X_phys_t = torch.tensor(X_phys_train_scaled, dtype=torch.float32).to(device)\n",
    "\n",
    "    model     = PINN(in_dim=1, hidden_dim=32, out_dim=1).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    n_epochs  = 7000\n",
    "    best_loss = float(\"inf\")\n",
    "    patience  = 500\n",
    "    counter   = 0\n",
    "    training_log = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_pred    = model(X_t)\n",
    "        data_loss = torch.mean((y_pred - Y_t) ** 2)\n",
    "        phys_loss = physics_residual_loss(y_pred, X_phys_t)\n",
    "        loss      = data_loss + 0.25 * phys_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "\n",
    "        if counter >= patience:\n",
    "            break\n",
    "\n",
    "        if epoch % 50 == 0:\n",
    "            training_log.append({\"epoch\": epoch, \"total_loss\": loss.item(), \"data_loss\": data_loss.item(),\"phys_loss\": phys_loss.item()})\n",
    "\n",
    "    \n",
    "\n",
    "    # Predict on train/test (in normalized space)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_train_pred_scaled = model(\n",
    "            torch.tensor(train_time_scaled, dtype=torch.float32).to(device)\n",
    "        ).cpu().numpy()\n",
    "\n",
    "        y_test_pred_scaled = model(\n",
    "            torch.tensor(test_time_scaled, dtype=torch.float32).to(device)\n",
    "        ).cpu().numpy()\n",
    "\n",
    "    # Remove PINN scaler → back to GLOBAL-NORMALIZED space\n",
    "    y_train_pred_norm = scaler_Y.inverse_transform(y_train_pred_scaled).flatten()\n",
    "    y_test_pred_norm  = scaler_Y.inverse_transform(y_test_pred_scaled).flatten()\n",
    "\n",
    "    train_overall[\"pinn_pred_norm\"] = y_train_pred_norm\n",
    "    test_overall[\"pinn_pred_norm\"]  = y_test_pred_norm\n",
    "\n",
    "    # PINN MAPE in NORMALIZED SPACE\n",
    "    df_pinn_test = test_overall[[\"y_norm\", \"pinn_pred_norm\"]].copy()\n",
    "\n",
    "    mape_pinn_norm = mean_absolute_percentage_error(\n",
    "        df_pinn_test[\"y_norm\"], df_pinn_test[\"pinn_pred_norm\"]\n",
    "    )\n",
    "    # print(f\"[PINN]     MAPE (normalized): {mape_pinn_norm:.4f}\")\n",
    "   \n",
    "\n",
    "    # print(\"\\n[NN Metrics]\")\n",
    "    # for k, v in pinn_metrics.items():\n",
    "    #     print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "\n",
    "    # ---- PINN MAPE in ORIGINAL units ----\n",
    "    df_pinn_test[\"y_orig\"] = scaler_y_global.inverse_transform(\n",
    "        df_pinn_test[[\"y_norm\"]]\n",
    "    ).flatten()\n",
    "    df_pinn_test[\"pinn_pred_orig\"] = scaler_y_global.inverse_transform(\n",
    "        df_pinn_test[[\"pinn_pred_norm\"]]\n",
    "    ).flatten()\n",
    "    mape_pinn_orig = mean_absolute_percentage_error(\n",
    "        df_pinn_test[\"y_orig\"], df_pinn_test[\"pinn_pred_orig\"]\n",
    "    )\n",
    "    # print(f\"[PINN]     MAPE (original)  : {mape_pinn_orig:.4f}\")\n",
    "    pinn_metrics = compute_metrics(\n",
    "    df_pinn_test[\"y_norm\"].values,\n",
    "    df_pinn_test[\"pinn_pred_norm\"].values,\n",
    "    prefix=\"NN_\"\n",
    "    )\n",
    "\n",
    "    \n",
    "    # ============================================================\n",
    "    # 7) HYBRID (RULE-NN + PROPHET RESIDUAL) WITH AUGMENTED DATA\n",
    "    # ============================================================\n",
    " \n",
    "\n",
    "    # Residuals on ORIGINAL training data\n",
    "    train_overall[\"residual_norm\"] = (\n",
    "        train_overall[\"y_norm\"] - train_overall[\"pinn_pred_norm\"]\n",
    "    )\n",
    "    test_overall[\"residual_norm\"] = (\n",
    "        test_overall[\"y_norm\"] - test_overall[\"pinn_pred_norm\"]\n",
    "    )\n",
    "\n",
    "    # Prepare for Prophet\n",
    "    train_res = train_overall[[\"ds\", \"residual_norm\"]].rename(\n",
    "        columns={\"residual_norm\": \"y\"}\n",
    "    )\n",
    "    test_res = test_overall[[\"ds\", \"residual_norm\"]].rename(\n",
    "        columns={\"residual_norm\": \"y\"}\n",
    "    )\n",
    "\n",
    "    # Prophet on residuals (additive, zero-centered)\n",
    "    m_res_augmented = Prophet(\n",
    "        seasonality_mode=\"additive\",\n",
    "        yearly_seasonality=True,\n",
    "        weekly_seasonality=False,\n",
    "        daily_seasonality=False,\n",
    "        changepoint_prior_scale=0.1,\n",
    "    )\n",
    "\n",
    "    m_res_augmented.fit(train_res)\n",
    "\n",
    "    # Predict residuals exactly on test dates\n",
    "    forecast_res_augmented = m_res_augmented.predict(test_res[[\"ds\"]])\n",
    "\n",
    "    # Merge residual predictions\n",
    "    df_res_test_augmented = (\n",
    "        forecast_res_augmented[[\"ds\", \"yhat\"]]\n",
    "        .merge(test_res[[\"ds\", \"y\"]], on=\"ds\", how=\"inner\")\n",
    "        .rename(columns={\"yhat\": \"res_pred_norm\"})\n",
    "    )\n",
    "\n",
    "    # Hybrid reconstruction\n",
    "    df_hybrid_test_augmented = (\n",
    "        test_overall[[\"ds\", \"y_norm\", \"pinn_pred_norm\"]]\n",
    "        .merge(df_res_test_augmented[[\"ds\", \"res_pred_norm\"]], on=\"ds\", how=\"left\")\n",
    "    )\n",
    "\n",
    "    df_hybrid_test_augmented[\"final_pred_norm\"] = (\n",
    "        df_hybrid_test_augmented[\"pinn_pred_norm\"] +\n",
    "        df_hybrid_test_augmented[\"res_pred_norm\"]\n",
    "    )\n",
    "\n",
    "    # Hybrid MAPE (normalized space)\n",
    "    mape_hybrid_augmented_norm = mean_absolute_percentage_error(\n",
    "        df_hybrid_test_augmented[\"y_norm\"],\n",
    "        df_hybrid_test_augmented[\"final_pred_norm\"]\n",
    "    )\n",
    "\n",
    "\n",
    "    hybrid_metrics_augmented = compute_metrics(\n",
    "        df_hybrid_test_augmented[\"y_norm\"].values,\n",
    "        df_hybrid_test_augmented[\"final_pred_norm\"].values,\n",
    "        prefix=\"Hybrid_Aug_\"\n",
    "    )\n",
    "\n",
    " \n",
    "\n",
    "    # ============================================================\n",
    "    # 8) CONVERT BACK TO ORIGINAL UNITS\n",
    "    # ============================================================\n",
    "\n",
    "\n",
    "    df_hybrid_test_augmented[\"final_pred_orig\"] = scaler_y_global.inverse_transform(\n",
    "        df_hybrid_test_augmented[[\"final_pred_norm\"]]\n",
    "    ).flatten()\n",
    "\n",
    "    df_hybrid_test_augmented[\"y_orig\"] = scaler_y_global.inverse_transform(\n",
    "        df_hybrid_test_augmented[[\"y_norm\"]]\n",
    "    ).flatten()\n",
    "\n",
    "    # Hybrid MAPE in ORIGINAL units\n",
    "    mape_hybrid_augmented_orig = mean_absolute_percentage_error(\n",
    "        df_hybrid_test_augmented[\"y_orig\"],\n",
    "        df_hybrid_test_augmented[\"final_pred_orig\"]\n",
    "    )\n",
    "\n",
    "    if mape_hybrid_augmented_norm < mape_prophet_augmented_norm:\n",
    "        break\n",
    "log_df = pd.DataFrame(training_log)\n",
    "print(f\"\\n[Training Log] {len(log_df)} epochs logged\")\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 9) FINAL COMPARISON: BASELINE VS AUGMENTED\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FINAL COMPARISON: BASELINE VS AUGMENTED\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Note: For baseline, we would need to also train the original models\n",
    "# For now, showing augmented results\n",
    "print(f\"\\n[Prophet - Augmented]\")\n",
    "print(f\"  MAPE : {mape_prophet_augmented_norm:.4f}\")\n",
    "\n",
    "print(f\"\\n[PINN - Augmented]\")\n",
    "print(f\"  MAPE : {mape_pinn_augmented_norm:.4f}\")\n",
    "\n",
    "print(f\"\\n[Hybrid - Augmented]\")\n",
    "print(f\"  MAPE : {mape_hybrid_augmented_norm:.4f}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# METRICS SUMMARY TABLE\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "metrics_augmented_df = pd.DataFrame([\n",
    "    {\n",
    "        \"Model\": \"Prophet (Aug)\",\n",
    "        \"MAPE\": prophet_metrics[\"Prophet_MAPE\"],\n",
    "        \"R2\": prophet_metrics[\"Prophet_R2\"],\n",
    "    },\n",
    "    {\n",
    "        \"Model\": \"NN (Original)\",\n",
    "        \"MAPE\": nn_metrics[\"NN_MAPE\"],\n",
    "        \"R2\": nn_metrics[\"NN_R2\"],\n",
    "    },\n",
    "    {\n",
    "        \"Model\": \"Hybrid (NN+Prophet Aug)\",\n",
    "        \"MAPE\": hybrid_metrics[\"Hybrid_MAPE\"],\n",
    "        \"R2\": hybrid_metrics[\"Hybrid_R2\"],\n",
    "    },\n",
    "])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"METRICS SUMMARY TABLE (WITH AUGMENTATION)\")\n",
    "print(\"=\" * 60)\n",
    "print(metrics_augmented_df.round(4).to_string(index=False))\n",
    "\n",
    "# ============================================================\n",
    "# Optional: Save results to CSV\n",
    "# ============================================================\n",
    "# metrics_augmented_df.to_csv(\"augmented_metrics.csv\", index=False)\n",
    "# df_hybrid_test_augmented.to_csv(\"hybrid_augmented_predictions.csv\", index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"AUGMENTATION PIPELINE COMPLETED\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a033c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "plt.plot(log_df[\"epoch\"], log_df[\"total_loss\"], label=\"Total Loss\")\n",
    "plt.plot(log_df[\"epoch\"], log_df[\"data_loss\"], label=\"Data Loss\")\n",
    "plt.plot(log_df[\"epoch\"], log_df[\"phys_loss\"], label=\"Rule Loss\")\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss Curves (Rule-Regularised NN)\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5cd492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TIME SERIES PLOT: ACTUAL vs PREDICTED\n",
    "# (Prophet, NN, Hybrid — ORIGINAL UNITS)\n",
    "# ============================================================\n",
    "\n",
    "plt.figure(figsize=(16, 7))\n",
    "\n",
    "# -----------------------\n",
    "# 1) Training data (actual)\n",
    "# -----------------------\n",
    "plt.plot(\n",
    "    train[\"ds\"],\n",
    "    train[\"y\"],\n",
    "    label=\"Training Actual\",\n",
    "    color=\"black\",\n",
    "    linewidth=2\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# 2) Test actuals\n",
    "# -----------------------\n",
    "plt.plot(\n",
    "    test[\"ds\"],\n",
    "    test[\"y\"],\n",
    "    label=\"Test Actual\",\n",
    "    color=\"black\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# 3) Prophet predictions\n",
    "# -----------------------\n",
    "plt.plot(\n",
    "    test[\"ds\"],\n",
    "    test[\"prophet_pred\"],\n",
    "    label=\"Prophet Prediction\",\n",
    "    linestyle=\":\",\n",
    "    linewidth=2.5,\n",
    "    color=\"tab:blue\"\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# 4) NN predictions\n",
    "# -----------------------\n",
    "plt.plot(\n",
    "    test[\"ds\"],\n",
    "    test[\"nn_pred\"],\n",
    "    label=\"NN Prediction\",\n",
    "    linestyle=\":\",\n",
    "    linewidth=2.5,\n",
    "    color=\"tab:green\"\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# 5) Hybrid predictions\n",
    "# -----------------------\n",
    "plt.plot(\n",
    "    test[\"ds\"],\n",
    "    test[\"hybrid_pred\"],\n",
    "    label=\"Hybrid Prediction\",\n",
    "    linestyle=\":\",\n",
    "    linewidth=2.5,\n",
    "    color=\"tab:red\"\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# 6) Vertical line for train/test split\n",
    "# -----------------------\n",
    "split_date = train[\"ds\"].iloc[-1]\n",
    "\n",
    "plt.axvline(\n",
    "    x=split_date,\n",
    "    color=\"gray\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2,\n",
    "    label=\"Train/Test Split\"\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# Labels, title, legend\n",
    "# -----------------------\n",
    "plt.xlabel(\"Date\", fontsize=12)\n",
    "plt.ylabel(\"Scope 1 Emissions (Original Units)\", fontsize=12)\n",
    "plt.title(\n",
    "    \"Actual vs Predicted Emissions\\n\"\n",
    "    \"(Prophet vs Rule-Regularised NN vs Hybrid)\",\n",
    "    fontsize=14\n",
    ")\n",
    "\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "plt.legend(fontsize=12)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8aa462",
   "metadata": {},
   "source": [
    "# jitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c2c25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PHASE 2: DECOMPOSITION-AWARE AUGMENTATION\n",
    "# Modified code with data augmentation pipeline\n",
    "# ============================================================\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_percentage_error,\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    r2_score\n",
    ")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "\n",
    "RANDOM_SEED   = 42\n",
    "DATE_COL      = \"datetime\"\n",
    "TARGET_COL    = \"Scope1_per_unit\"\n",
    "PLANT_COL     = \"Plant Name\"\n",
    "TEST_MONTHS   = 7\n",
    "PHYS_COL      = \"Electricity Grid Energy Per Unit (GJ)\"\n",
    "TargetCol_raw = \"Scope1\"\n",
    "ProductionCol = \"Production Actual Quantity (MT/Month)\"\n",
    "\n",
    "# ============================================================\n",
    "# 0) NEW: AUGMENTATION PARAMETERS\n",
    "# ============================================================\n",
    "NUM_AUGMENTED_SAMPLES = 4  \n",
    "JITTER_STD_MULTIPLIER = 0.05  # sigma = 0.05 * std(residuals)\n",
    "DECOMPOSITION_PERIOD = 12  \n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1) SEEDING\n",
    "# ============================================================\n",
    "def set_seed(seed: int = 42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(RANDOM_SEED)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) DATA PREP FUNCTIONS (ORIGINAL)\n",
    "# ============================================================\n",
    "def ensure_datetime_column(df: pd.DataFrame,\n",
    "                           date_col: str = DATE_COL) -> pd.DataFrame:\n",
    "    \"\"\"Ensure a monthly datetime column 'ds' exists.\"\"\"\n",
    "    if date_col in df.columns:\n",
    "        ds = pd.to_datetime(df[date_col], errors=\"coerce\", infer_datetime_format=True)\n",
    "    else:\n",
    "        ds = pd.Series(pd.NaT, index=df.index)\n",
    "\n",
    "    needs_rebuild = ds.isna()\n",
    "    if needs_rebuild.any():\n",
    "        if not {\"year\", \"month\"}.issubset(df.columns):\n",
    "            raise ValueError(\n",
    "                \"No valid datetime and missing 'year'/'month' to rebuild dates.\"\n",
    "            )\n",
    "        ds_rebuilt = pd.to_datetime(\n",
    "            dict(\n",
    "                year=df.loc[needs_rebuild, \"year\"],\n",
    "                month=df.loc[needs_rebuild, \"month\"],\n",
    "                day=1,\n",
    "            )\n",
    "        )\n",
    "        ds.loc[needs_rebuild] = ds_rebuilt\n",
    "\n",
    "    df[\"ds\"] = pd.to_datetime(ds).dt.to_period(\"M\").dt.to_timestamp()\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_and_impute(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Drop duplicates and impute numeric columns (0→NaN→mean).\"\"\"\n",
    "    if {\"ds\", PLANT_COL, TARGET_COL}.issubset(df.columns):\n",
    "        df = df.drop_duplicates(subset=[\"ds\", PLANT_COL, TARGET_COL], keep=\"last\")\n",
    "\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    def impute_zero_nan_with_mean(group: pd.DataFrame) -> pd.DataFrame:\n",
    "        g = group.copy()\n",
    "        for c in num_cols:\n",
    "            g[c] = g[c].replace(0, np.nan)\n",
    "            if g[c].notna().any():\n",
    "                g[c] = g[c].fillna(g[c].mean())\n",
    "        return g\n",
    "\n",
    "    if \"year\" in df.columns:\n",
    "        df = df.groupby(\"year\", group_keys=False).apply(impute_zero_nan_with_mean)\n",
    "    else:\n",
    "        df = impute_zero_nan_with_mean(df)\n",
    "\n",
    "    for c in num_cols:\n",
    "        if df[c].isna().any():\n",
    "            df[c] = df[c].fillna(df[c].mean())\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def prepare_overall_series2(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregate ALL numeric columns across plants into a single monthly series.\n",
    "    For each month (ds), returns the sum of every numeric column.\n",
    "    \"\"\"\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    if \"ds\" not in df.columns:\n",
    "        raise ValueError(\"The dataframe must contain a 'ds' datetime column.\")\n",
    "\n",
    "    agg = (\n",
    "        df.groupby(\"ds\", as_index=False)[numeric_cols]\n",
    "          .sum()\n",
    "          .sort_values(\"ds\")\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    return agg\n",
    "\n",
    "\n",
    "def compute_metrics(y_true, y_pred, prefix=\"\"):\n",
    "    \"\"\"\n",
    "    Compute common regression metrics.\n",
    "    Assumes inputs are 1D numpy arrays.\n",
    "    \"\"\"\n",
    "    mae  = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    r2   = r2_score(y_true, y_pred)\n",
    "\n",
    "    return {\n",
    "        f\"{prefix}MAE\": mae,\n",
    "        f\"{prefix}RMSE\": rmse,\n",
    "        f\"{prefix}MAPE\": mape,\n",
    "        f\"{prefix}R2\": r2,\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# NEW SECTION: DECOMPOSITION-AWARE AUGMENTATION FUNCTIONS\n",
    "# ============================================================\n",
    "def decompose_time_series(series: pd.Series, \n",
    "                          period: int = DECOMPOSITION_PERIOD) -> dict:\n",
    "    \n",
    "    # Ensure we have enough data for decomposition\n",
    "    if len(series) < 2 * period:\n",
    "        print(f\"Warning: Series length ({len(series)}) < 2*period ({2*period}). \"\n",
    "              f\"Adjusting period to {len(series)//2}.\")\n",
    "        period = max(2, len(series) // 2)\n",
    "    \n",
    "    try:\n",
    "        decomposition = seasonal_decompose(\n",
    "            series, \n",
    "            model='additive', \n",
    "            period=period,\n",
    "            \n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'trend': decomposition.trend.fillna(method=\"bfill\").fillna(method=\"ffill\").values,\n",
    "            'seasonal': decomposition.seasonal.values,\n",
    "            'residual': decomposition.resid.fillna(0).values,\n",
    "            'period': period\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Decomposition failed: {e}. Returning zero components.\")\n",
    "        return {\n",
    "            'trend': series.values,\n",
    "            'seasonal': np.zeros_like(series.values),\n",
    "            'residual': np.zeros_like(series.values),\n",
    "            'period': period\n",
    "        }\n",
    "\n",
    "\n",
    "def apply_jittering(trend: np.ndarray, seasonal: np.ndarray, residuals: np.ndarray, \n",
    "                   std_multiplier: float = JITTER_STD_MULTIPLIER) -> np.ndarray:\n",
    "\n",
    "    # trend_std = np.std(trend[~np.isnan(trend)])\n",
    "    # seasonal_std = np.std(seasonal[~np.isnan(seasonal)])\n",
    "    trend_std = 20\n",
    "    seasonal_std = 50\n",
    "    residual_std = np.std(residuals[~np.isnan(residuals)])\n",
    "    if residual_std == 0:\n",
    "        residual_std = 1.0\n",
    "    if trend_std == 0:\n",
    "        trend_std = 1.0\n",
    "    if seasonal_std == 0:\n",
    "        seasonal_std = 1.0\n",
    "\n",
    "    trend_noise_std = std_multiplier * trend_std\n",
    "    seasonal_noise_std = std_multiplier * seasonal_std\n",
    "    noise_std = std_multiplier * residual_std\n",
    "\n",
    "    jitter_noise = np.random.normal(0, noise_std, size=len(residuals))\n",
    "    jitter_trend = np.random.normal(0, trend_noise_std, size=len(trend))\n",
    "    jitter_seasonal = np.random.normal(0, seasonal_noise_std, size=len(seasonal))\n",
    "\n",
    "\n",
    "    return  trend + jitter_trend, seasonal + jitter_seasonal, residuals + jitter_noise\n",
    "\n",
    "\n",
    "def create_augmented_samples(series: pd.Series,\n",
    "                             num_samples: int = NUM_AUGMENTED_SAMPLES,\n",
    "                             decomposition_dict: dict = None, std_multiplier: float = JITTER_STD_MULTIPLIER) -> list:\n",
    "    \"\"\"\n",
    "    Create augmented time series by:\n",
    "    1. Decomposing into trend, seasonal, residual\n",
    "    2. Jittering the residual component\n",
    "    3. Reconstructing: T + S + (R + jitter)\n",
    "    \"\"\"\n",
    "    if decomposition_dict is None:\n",
    "        decomposition_dict = decompose_time_series(series)\n",
    "    \n",
    "    trend = decomposition_dict['trend']\n",
    "    seasonal = decomposition_dict['seasonal']\n",
    "    residual = decomposition_dict['residual']\n",
    "    \n",
    "    augmented_samples = []\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Apply jittering to residuals\n",
    "        jittered_trend, jittered_seasonal, jittered_residual = apply_jittering(trend.copy(), seasonal.copy(), residual.copy(), std_multiplier=std_multiplier)\n",
    "        \n",
    "        # Reconstruct: T + S + (R + jitter)\n",
    "        augmented_series = trend + jittered_seasonal + residual\n",
    "        augmented_samples.append(augmented_series)\n",
    "    \n",
    "    return augmented_samples\n",
    "\n",
    "\n",
    "def prepare_augmented_training_data(train_df: pd.DataFrame,\n",
    "                                   num_augmented: int = NUM_AUGMENTED_SAMPLES,\n",
    "                                   decomposition_dict: dict = None, std_multiplier: float = JITTER_STD_MULTIPLIER) -> pd.DataFrame:\n",
    "    \n",
    "    original_series = pd.Series(train_df['y_norm'].values, index=range(len(train_df)))\n",
    "    \n",
    "    # Create augmented samples\n",
    "    augmented_samples = create_augmented_samples(\n",
    "        original_series,\n",
    "        num_samples=num_augmented,\n",
    "        decomposition_dict=decomposition_dict,\n",
    "        std_multiplier=std_multiplier\n",
    "    )\n",
    "    \n",
    "    # Prepare augmented dataframes\n",
    "    augmented_dfs = []\n",
    "    \n",
    "    # Add original data\n",
    "    augmented_dfs.append(train_df.copy())\n",
    "    \n",
    "    # Add augmented data\n",
    "    for idx, aug_values in enumerate(augmented_samples):\n",
    "        aug_df = train_df[['ds', 'y']].copy() if 'y' in train_df.columns else train_df[['ds']].copy()\n",
    "        aug_df['y_norm'] = aug_values\n",
    "        if 'y' not in aug_df.columns:\n",
    "            aug_df['y'] = aug_values  # For Prophet\n",
    "        else:\n",
    "            aug_df['y'] = aug_values  # Overwrite with augmented values\n",
    "        \n",
    "        # Optional: Add metadata for tracking\n",
    "        aug_df['augmented'] = True\n",
    "        aug_df['augmentation_id'] = idx\n",
    "        augmented_dfs.append(aug_df)\n",
    "    \n",
    "    # Concatenate all\n",
    "    augmented_train = pd.concat(augmented_dfs, ignore_index=True)\n",
    "    \n",
    "    print(f\"\\n[AUGMENTATION] Original training size: {len(train_df)}\")\n",
    "    print(f\"[AUGMENTATION] Augmented versions created: {num_augmented}\")\n",
    "    print(f\"[AUGMENTATION] Total augmented training size: {len(augmented_train)}\")\n",
    "    \n",
    "    return augmented_train\n",
    "\n",
    "\n",
    "def visualize_decomposition(series: pd.Series, \n",
    "                           decomposition_dict: dict,\n",
    "                           title: str = \"Time Series Decomposition\") -> None:\n",
    "    \"\"\"\n",
    "    Visualize the decomposition of a time series.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(4, 1, figsize=(12, 10))\n",
    "    \n",
    "    # Original series\n",
    "    axes[0].plot(series.index, series.values, 'b-', linewidth=1.5)\n",
    "    axes[0].set_ylabel('Original')\n",
    "    axes[0].set_title(title)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Trend\n",
    "    axes[1].plot(series.index, decomposition_dict['trend'], 'g-', linewidth=1.5)\n",
    "    axes[1].set_ylabel('Trend')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Seasonal\n",
    "    axes[2].plot(series.index, decomposition_dict['seasonal'], 'orange', linewidth=1.5)\n",
    "    axes[2].set_ylabel('Seasonal')\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Residual\n",
    "    axes[3].plot(series.index, decomposition_dict['residual'], 'r-', linewidth=1.5)\n",
    "    axes[3].set_ylabel('Residual')\n",
    "    axes[3].set_xlabel('Time Index')\n",
    "    axes[3].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_augmented_samples(original_series: pd.Series,\n",
    "                               augmented_samples: list,\n",
    "                               title: str = \"Original vs Augmented Series\", std_multiplier: float = JITTER_STD_MULTIPLIER) -> None:\n",
    "    \"\"\"\n",
    "    Visualize original series and augmented versions.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot original\n",
    "    plt.plot(original_series.index, original_series.values, \n",
    "             'b-', linewidth=2.5, label='Original', alpha=0.8)\n",
    "    \n",
    "    # Plot augmented samples\n",
    "    colors = ['red', 'green', 'orange', 'purple', 'brown']\n",
    "    for idx, aug in enumerate(augmented_samples):\n",
    "        plt.plot(original_series.index, aug, \n",
    "                linewidth=1, alpha=0.5, \n",
    "                label=f'Augmented {idx+1}',\n",
    "                color=colors[idx % len(colors)])\n",
    "    \n",
    "    plt.xlabel('Time Index')\n",
    "    plt.ylabel('Normalized Value')\n",
    "    plt.title(f\"{title} (Jitter multiplier: {std_multiplier})\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3) NN MODEL DEFINITION (ORIGINAL - PINN class assumed)\n",
    "# ============================================================\n",
    "class NNModel(nn.Module):\n",
    "    def __init__(self, in_dim=1, hidden_dim=32, out_dim=1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "def regression_from_numpy_normalized(x, y):\n",
    "    \"\"\"\n",
    "    Normalize x and y, then compute linear regression y = m*x + c.\n",
    "    Returns m, c and plots the regression line on normalized data.\n",
    "    \"\"\"\n",
    "    x = x.flatten().reshape(-1, 1)\n",
    "    y = y.flatten().reshape(-1, 1)\n",
    "\n",
    "    scaler_x = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "\n",
    "    x_norm = scaler_x.fit_transform(x).flatten()\n",
    "    y_norm = scaler_y.fit_transform(y).flatten()\n",
    "\n",
    "    x_mean = x_norm.mean()\n",
    "    y_mean = y_norm.mean()\n",
    "\n",
    "    m = np.sum((x_norm - x_mean) * (y_norm - y_mean)) / np.sum((x_norm - x_mean) ** 2)\n",
    "    c = y_mean - m * x_mean\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.scatter(x_norm, y_norm, alpha=0.7, label=\"Normalized Data\")\n",
    "\n",
    "    x_line = np.linspace(x_norm.min(), x_norm.max(), 100)\n",
    "    y_line = m * x_line + c\n",
    "\n",
    "    plt.plot(x_line, y_line, color=\"red\", linewidth=2,\n",
    "             label=f\"y = {m:.4f}x + {c:.4f}\")\n",
    "\n",
    "    plt.xlabel(\"x (normalized)\")\n",
    "    plt.ylabel(\"y (normalized)\")\n",
    "    plt.title(\"Linear Regression on Normalized Data\")\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return m, c\n",
    "\n",
    "\n",
    "def physics_residual_loss(y_pred: torch.Tensor,\n",
    "                          features: torch.Tensor,\n",
    "                          eps: float = 1e-6) -> torch.Tensor:\n",
    "    delta = 0.75\n",
    "    elec = features[:, 0]\n",
    "    c02  = features[:, 1]\n",
    "    prod = features[:, 2]\n",
    "\n",
    "    # Rule-based reference (not physics)\n",
    "    rule = (0.4727 * elec) / prod\n",
    "\n",
    "    # Normalised deviation from the rule\n",
    "    diff = (y_pred.squeeze() - rule) / (torch.abs(rule) + eps)\n",
    "    \n",
    "    # Huber-style soft rule penalty\n",
    "    penalty = torch.where(\n",
    "        torch.abs(diff) <= delta,\n",
    "        0.5 * diff**2,\n",
    "        delta * (torch.abs(diff) - 0.5 * delta)\n",
    "    )\n",
    "\n",
    "    return penalty.mean()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MAIN PIPELINE WITH AUGMENTATION\n",
    "# ============================================================\n",
    "\n",
    "# [ORIGINAL PIPELINE UP TO TRAIN/TEST SPLIT]\n",
    "df_raw = df_2022.copy()\n",
    "\n",
    "df = ensure_datetime_column(df_raw, date_col=DATE_COL)\n",
    "df = clean_and_impute(df)\n",
    "\n",
    "overall = prepare_overall_series2(df)\n",
    "overall = overall.sort_values(\"ds\").reset_index(drop=True)\n",
    "overall = overall.rename(columns={TARGET_COL: \"y\"})\n",
    "\n",
    "# --- GLOBAL SCALING (Prophet + Hybrid + Final Outputs) ---\n",
    "scaler_y_global = StandardScaler()\n",
    "overall[\"y_norm\"] = scaler_y_global.fit_transform(overall[[\"y\"]])\n",
    "\n",
    "# Train/test split\n",
    "split_point = len(overall) - TEST_MONTHS\n",
    "train_overall = overall.iloc[:split_point].copy()\n",
    "test_overall  = overall.iloc[split_point:].copy()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ORIGINAL DATASET\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Train: {len(train_overall)} samples\")\n",
    "print(f\"Test:  {len(test_overall)} samples\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# PHASE 2: DECOMPOSITION-AWARE AUGMENTATION\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PHASE 2: DECOMPOSITION-AWARE AUGMENTATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Step 1: Decompose the original training series\n",
    "original_train_series = pd.Series(\n",
    "    train_overall['y_norm'].values,\n",
    "    index=range(len(train_overall))\n",
    ")\n",
    "\n",
    "decomposition_dict = decompose_time_series(\n",
    "    original_train_series,\n",
    "    period=DECOMPOSITION_PERIOD\n",
    ")\n",
    "\n",
    "print(f\"\\n[DECOMPOSITION] Seasonal period: {decomposition_dict['period']}\")\n",
    "print(f\"[DECOMPOSITION] Trend shape: {decomposition_dict['trend'].shape}\")\n",
    "print(f\"[DECOMPOSITION] Seasonal shape: {decomposition_dict['seasonal'].shape}\")\n",
    "print(f\"[DECOMPOSITION] Residual shape: {decomposition_dict['residual'].shape}\")\n",
    "\n",
    "# Visualize decomposition (optional - comment out if not needed)\n",
    "visualize_decomposition(original_train_series, decomposition_dict, \n",
    "                       title=\"Training Data Decomposition\")\n",
    "for JITTER_STD_MULTIPLIER in [0.05, 0.5, 0.1, 1.0, 5.0, 10.0]:\n",
    "# Step 2: Create augmented samples\n",
    "    augmented_samples = create_augmented_samples(\n",
    "        original_train_series,\n",
    "        num_samples=NUM_AUGMENTED_SAMPLES,\n",
    "        decomposition_dict=decomposition_dict,\n",
    "        std_multiplier=JITTER_STD_MULTIPLIER\n",
    "    )\n",
    "\n",
    "    print(f\"\\n[AUGMENTATION] Created {len(augmented_samples)} augmented samples\")\n",
    "    print(f\"[AUGMENTATION] Jitter std multiplier: {JITTER_STD_MULTIPLIER}\")\n",
    "\n",
    "    # Visualize augmentation (optional - comment out if not needed)\n",
    "    visualize_augmented_samples(original_train_series, augmented_samples,\n",
    "                            title=\"Original vs Augmented Training Samples\", std_multiplier=JITTER_STD_MULTIPLIER)\n",
    "\n",
    "    # Step 3: Prepare augmented training dataframe for Prophet\n",
    "    train_prophet_augmented = train_overall[[\"ds\", \"y_norm\"]].rename(\n",
    "        columns={\"y_norm\": \"y\"}\n",
    "    ).copy()\n",
    "\n",
    "    # Create additional augmented dataframes\n",
    "    for idx, aug_values in enumerate(augmented_samples):\n",
    "        aug_df = train_overall[[\"ds\"]].copy()\n",
    "        aug_df[\"y\"] = aug_values\n",
    "        train_prophet_augmented = pd.concat(\n",
    "            [train_prophet_augmented, aug_df],\n",
    "            ignore_index=True\n",
    "        )\n",
    "\n",
    "    # Sort by ds for Prophet (important for time series)\n",
    "    train_prophet_augmented = train_prophet_augmented.sort_values(\"ds\").reset_index(drop=True)\n",
    "\n",
    "    print(f\"\\n[PROPHET TRAINING DATA]\")\n",
    "    print(f\"Original: {len(train_overall)}\")\n",
    "    print(f\"Augmented: {len(train_prophet_augmented)}\")\n",
    "\n",
    "    # Prepare test data (unchanged)\n",
    "    test_prophet = test_overall[[\"ds\", \"y_norm\"]].rename(columns={\"y_norm\": \"y\"})\n",
    "\n",
    "    # ============================================================\n",
    "    # 5) PROPHET WITH AUGMENTED DATA\n",
    "    # ============================================================\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TRAINING PROPHET WITH AUGMENTED DATA\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    m_overall_augmented = Prophet(\n",
    "        seasonality_mode=\"additive\",\n",
    "        yearly_seasonality=True,\n",
    "        weekly_seasonality=False,\n",
    "        daily_seasonality=False,\n",
    "        changepoint_prior_scale=0.1,\n",
    "    )\n",
    "\n",
    "    # Fit on augmented training data\n",
    "    m_overall_augmented.fit(train_prophet_augmented)\n",
    "\n",
    "    # Predict on test set\n",
    "    future_all_augmented = m_overall_augmented.make_future_dataframe(\n",
    "        periods=len(test_prophet), \n",
    "        freq=\"MS\"\n",
    "    )\n",
    "    fcst_all_augmented = m_overall_augmented.predict(future_all_augmented)\n",
    "\n",
    "    # Extract test predictions\n",
    "    df_prophet_test_augmented = (\n",
    "        fcst_all_augmented[[\"ds\", \"yhat\"]]\n",
    "        .merge(test_prophet[[\"ds\", \"y\"]], on=\"ds\", how=\"inner\")\n",
    "        .sort_values(\"ds\")\n",
    "    )\n",
    "\n",
    "    # MAPE in NORMALIZED SPACE\n",
    "    mape_prophet_augmented_norm = mean_absolute_percentage_error(\n",
    "        df_prophet_test_augmented[\"y\"],\n",
    "        df_prophet_test_augmented[\"yhat\"]\n",
    "    )\n",
    "\n",
    "    print(f\"\\n[Prophet - Augmented] MAPE : {mape_prophet_augmented_norm:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # ============================================================\n",
    "    # 6) PINN WITH AUGMENTED DATA\n",
    "    # ============================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5a0f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PHASE 2: DECOMPOSITION-AWARE AUGMENTATION\n",
    "# Modified code with data augmentation pipeline\n",
    "# ============================================================\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_percentage_error,\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    r2_score\n",
    ")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "\n",
    "RANDOM_SEED   = 42\n",
    "DATE_COL      = \"datetime\"\n",
    "TARGET_COL    = \"Scope1_per_unit\"\n",
    "PLANT_COL     = \"Plant Name\"\n",
    "TEST_MONTHS   = 7\n",
    "PHYS_COL      = \"Electricity Grid Energy Per Unit (GJ)\"\n",
    "TargetCol_raw = \"Scope1\"\n",
    "ProductionCol = \"Production Actual Quantity (MT/Month)\"\n",
    "\n",
    "# ============================================================\n",
    "# 0) NEW: AUGMENTATION PARAMETERS\n",
    "# ============================================================\n",
    "NUM_AUGMENTED_SAMPLES = 4  \n",
    "JITTER_STD_MULTIPLIER = 0.05  # sigma = 0.05 * std(residuals)\n",
    "DECOMPOSITION_PERIOD = 12  \n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1) SEEDING\n",
    "# ============================================================\n",
    "def set_seed(seed: int = 42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(RANDOM_SEED)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) DATA PREP FUNCTIONS (ORIGINAL)\n",
    "# ============================================================\n",
    "def ensure_datetime_column(df: pd.DataFrame,\n",
    "                           date_col: str = DATE_COL) -> pd.DataFrame:\n",
    "    \"\"\"Ensure a monthly datetime column 'ds' exists.\"\"\"\n",
    "    if date_col in df.columns:\n",
    "        ds = pd.to_datetime(df[date_col], errors=\"coerce\", infer_datetime_format=True)\n",
    "    else:\n",
    "        ds = pd.Series(pd.NaT, index=df.index)\n",
    "\n",
    "    needs_rebuild = ds.isna()\n",
    "    if needs_rebuild.any():\n",
    "        if not {\"year\", \"month\"}.issubset(df.columns):\n",
    "            raise ValueError(\n",
    "                \"No valid datetime and missing 'year'/'month' to rebuild dates.\"\n",
    "            )\n",
    "        ds_rebuilt = pd.to_datetime(\n",
    "            dict(\n",
    "                year=df.loc[needs_rebuild, \"year\"],\n",
    "                month=df.loc[needs_rebuild, \"month\"],\n",
    "                day=1,\n",
    "            )\n",
    "        )\n",
    "        ds.loc[needs_rebuild] = ds_rebuilt\n",
    "\n",
    "    df[\"ds\"] = pd.to_datetime(ds).dt.to_period(\"M\").dt.to_timestamp()\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_and_impute(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Drop duplicates and impute numeric columns (0→NaN→mean).\"\"\"\n",
    "    if {\"ds\", PLANT_COL, TARGET_COL}.issubset(df.columns):\n",
    "        df = df.drop_duplicates(subset=[\"ds\", PLANT_COL, TARGET_COL], keep=\"last\")\n",
    "\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    def impute_zero_nan_with_mean(group: pd.DataFrame) -> pd.DataFrame:\n",
    "        g = group.copy()\n",
    "        for c in num_cols:\n",
    "            g[c] = g[c].replace(0, np.nan)\n",
    "            if g[c].notna().any():\n",
    "                g[c] = g[c].fillna(g[c].mean())\n",
    "        return g\n",
    "\n",
    "    if \"year\" in df.columns:\n",
    "        df = df.groupby(\"year\", group_keys=False).apply(impute_zero_nan_with_mean)\n",
    "    else:\n",
    "        df = impute_zero_nan_with_mean(df)\n",
    "\n",
    "    for c in num_cols:\n",
    "        if df[c].isna().any():\n",
    "            df[c] = df[c].fillna(df[c].mean())\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def prepare_overall_series2(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregate ALL numeric columns across plants into a single monthly series.\n",
    "    For each month (ds), returns the sum of every numeric column.\n",
    "    \"\"\"\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    if \"ds\" not in df.columns:\n",
    "        raise ValueError(\"The dataframe must contain a 'ds' datetime column.\")\n",
    "\n",
    "    agg = (\n",
    "        df.groupby(\"ds\", as_index=False)[numeric_cols]\n",
    "          .sum()\n",
    "          .sort_values(\"ds\")\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    return agg\n",
    "\n",
    "\n",
    "def compute_metrics(y_true, y_pred, prefix=\"\"):\n",
    "    \"\"\"\n",
    "    Compute common regression metrics.\n",
    "    Assumes inputs are 1D numpy arrays.\n",
    "    \"\"\"\n",
    "    mae  = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    r2   = r2_score(y_true, y_pred)\n",
    "\n",
    "    return {\n",
    "        f\"{prefix}MAE\": mae,\n",
    "        f\"{prefix}RMSE\": rmse,\n",
    "        f\"{prefix}MAPE\": mape,\n",
    "        f\"{prefix}R2\": r2,\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# NEW SECTION: DECOMPOSITION-AWARE AUGMENTATION FUNCTIONS\n",
    "# ============================================================\n",
    "def decompose_time_series(series: pd.Series, \n",
    "                          period: int = DECOMPOSITION_PERIOD) -> dict:\n",
    "    \n",
    "    # Ensure we have enough data for decomposition\n",
    "    if len(series) < 2 * period:\n",
    "        print(f\"Warning: Series length ({len(series)}) < 2*period ({2*period}). \"\n",
    "              f\"Adjusting period to {len(series)//2}.\")\n",
    "        period = max(2, len(series) // 2)\n",
    "    \n",
    "    try:\n",
    "        decomposition = seasonal_decompose(\n",
    "            series, \n",
    "            model='additive', \n",
    "            period=period,\n",
    "            \n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'trend': decomposition.trend.fillna(method=\"bfill\").fillna(method=\"ffill\").values,\n",
    "            'seasonal': decomposition.seasonal.values,\n",
    "            'residual': decomposition.resid.fillna(0).values,\n",
    "            'period': period\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Decomposition failed: {e}. Returning zero components.\")\n",
    "        return {\n",
    "            'trend': series.values,\n",
    "            'seasonal': np.zeros_like(series.values),\n",
    "            'residual': np.zeros_like(series.values),\n",
    "            'period': period\n",
    "        }\n",
    "\n",
    "\n",
    "def apply_jittering(trend: np.ndarray, seasonal: np.ndarray, residuals: np.ndarray, \n",
    "                   std_multiplier: float = JITTER_STD_MULTIPLIER) -> np.ndarray:\n",
    "\n",
    "    trend_std = np.std(trend[~np.isnan(trend)])\n",
    "    seasonal_std = np.std(seasonal[~np.isnan(seasonal)])\n",
    "    residual_std = np.std(residuals[~np.isnan(residuals)])\n",
    "    if residual_std == 0:\n",
    "        residual_std = 1.0\n",
    "    if trend_std == 0:\n",
    "        trend_std = 1.0\n",
    "    if seasonal_std == 0:\n",
    "        seasonal_std = 1.0\n",
    "    \n",
    "    trend_noise_std = std_multiplier * trend_std\n",
    "    seasonal_noise_std = std_multiplier * seasonal_std\n",
    "    noise_std = std_multiplier * residual_std\n",
    "\n",
    "    jitter_noise = np.random.normal(0, noise_std, size=len(residuals))\n",
    "    jitter_trend = np.random.normal(0, trend_noise_std, size=len(trend))\n",
    "    jitter_seasonal = np.random.normal(0, seasonal_noise_std, size=len(seasonal))\n",
    "\n",
    "\n",
    "    return  trend + jitter_trend, seasonal + jitter_seasonal, residuals + jitter_noise\n",
    "\n",
    "\n",
    "def create_augmented_samples(series: pd.Series,\n",
    "                             num_samples: int = NUM_AUGMENTED_SAMPLES,\n",
    "                             decomposition_dict: dict = None) -> list:\n",
    "    \"\"\"\n",
    "    Create augmented time series by:\n",
    "    1. Decomposing into trend, seasonal, residual\n",
    "    2. Jittering the residual component\n",
    "    3. Reconstructing: T + S + (R + jitter)\n",
    "    \"\"\"\n",
    "    if decomposition_dict is None:\n",
    "        decomposition_dict = decompose_time_series(series)\n",
    "    \n",
    "    trend = decomposition_dict['trend']\n",
    "    seasonal = decomposition_dict['seasonal']\n",
    "    residual = decomposition_dict['residual']\n",
    "    \n",
    "    augmented_samples = []\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Apply jittering to residuals\n",
    "        jittered_trend, jittered_seasonal, jittered_residual = apply_jittering(trend.copy(), seasonal.copy(), residual.copy())\n",
    "        \n",
    "        # Reconstruct: T + S + (R + jitter)\n",
    "        augmented_series = jittered_trend + jittered_seasonal + jittered_residual\n",
    "        augmented_samples.append(augmented_series)\n",
    "    \n",
    "    return augmented_samples\n",
    "\n",
    "\n",
    "def prepare_augmented_training_data(train_df: pd.DataFrame,\n",
    "                                   num_augmented: int = NUM_AUGMENTED_SAMPLES,\n",
    "                                   decomposition_dict: dict = None) -> pd.DataFrame:\n",
    "    \n",
    "    original_series = pd.Series(train_df['y_norm'].values, index=range(len(train_df)))\n",
    "    \n",
    "    # Create augmented samples\n",
    "    augmented_samples = create_augmented_samples(\n",
    "        original_series,\n",
    "        num_samples=num_augmented,\n",
    "        decomposition_dict=decomposition_dict\n",
    "    )\n",
    "    \n",
    "    # Prepare augmented dataframes\n",
    "    augmented_dfs = []\n",
    "    \n",
    "    # Add original data\n",
    "    augmented_dfs.append(train_df.copy())\n",
    "    \n",
    "    # Add augmented data\n",
    "    for idx, aug_values in enumerate(augmented_samples):\n",
    "        aug_df = train_df[['ds', 'y']].copy() if 'y' in train_df.columns else train_df[['ds']].copy()\n",
    "        aug_df['y_norm'] = aug_values\n",
    "        if 'y' not in aug_df.columns:\n",
    "            aug_df['y'] = aug_values  # For Prophet\n",
    "        else:\n",
    "            aug_df['y'] = aug_values  # Overwrite with augmented values\n",
    "        \n",
    "        # Optional: Add metadata for tracking\n",
    "        aug_df['augmented'] = True\n",
    "        aug_df['augmentation_id'] = idx\n",
    "        augmented_dfs.append(aug_df)\n",
    "    \n",
    "    # Concatenate all\n",
    "    augmented_train = pd.concat(augmented_dfs, ignore_index=True)\n",
    "    \n",
    "    print(f\"\\n[AUGMENTATION] Original training size: {len(train_df)}\")\n",
    "    print(f\"[AUGMENTATION] Augmented versions created: {num_augmented}\")\n",
    "    print(f\"[AUGMENTATION] Total augmented training size: {len(augmented_train)}\")\n",
    "    \n",
    "    return augmented_train\n",
    "\n",
    "\n",
    "def visualize_decomposition(series: pd.Series, \n",
    "                           decomposition_dict: dict,\n",
    "                           title: str = \"Time Series Decomposition\") -> None:\n",
    "    \"\"\"\n",
    "    Visualize the decomposition of a time series.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(4, 1, figsize=(12, 10))\n",
    "    \n",
    "    # Original series\n",
    "    axes[0].plot(series.index, series.values, 'b-', linewidth=1.5)\n",
    "    axes[0].set_ylabel('Original')\n",
    "    axes[0].set_title(title)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Trend\n",
    "    axes[1].plot(series.index, decomposition_dict['trend'], 'g-', linewidth=1.5)\n",
    "    axes[1].set_ylabel('Trend')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Seasonal\n",
    "    axes[2].plot(series.index, decomposition_dict['seasonal'], 'orange', linewidth=1.5)\n",
    "    axes[2].set_ylabel('Seasonal')\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Residual\n",
    "    axes[3].plot(series.index, decomposition_dict['residual'], 'r-', linewidth=1.5)\n",
    "    axes[3].set_ylabel('Residual')\n",
    "    axes[3].set_xlabel('Time Index')\n",
    "    axes[3].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_augmented_samples(original_series: pd.Series,\n",
    "                               augmented_samples: list,\n",
    "                               title: str = \"Original vs Augmented Series\") -> None:\n",
    "    \"\"\"\n",
    "    Visualize original series and augmented versions.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot original\n",
    "    plt.plot(original_series.index, original_series.values, \n",
    "             'b-', linewidth=2.5, label='Original', alpha=0.8)\n",
    "    \n",
    "    # Plot augmented samples\n",
    "    colors = ['red', 'green', 'orange', 'purple', 'brown']\n",
    "    for idx, aug in enumerate(augmented_samples):\n",
    "        plt.plot(original_series.index, aug, \n",
    "                linewidth=1, alpha=0.5, \n",
    "                label=f'Augmented {idx+1}',\n",
    "                color=colors[idx % len(colors)])\n",
    "    \n",
    "    plt.xlabel('Time Index')\n",
    "    plt.ylabel('Normalized Value')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3) NN MODEL DEFINITION (ORIGINAL - PINN class assumed)\n",
    "# ============================================================\n",
    "class NNModel(nn.Module):\n",
    "    def __init__(self, in_dim=1, hidden_dim=32, out_dim=1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "def regression_from_numpy_normalized(x, y):\n",
    "    \"\"\"\n",
    "    Normalize x and y, then compute linear regression y = m*x + c.\n",
    "    Returns m, c and plots the regression line on normalized data.\n",
    "    \"\"\"\n",
    "    x = x.flatten().reshape(-1, 1)\n",
    "    y = y.flatten().reshape(-1, 1)\n",
    "\n",
    "    scaler_x = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "\n",
    "    x_norm = scaler_x.fit_transform(x).flatten()\n",
    "    y_norm = scaler_y.fit_transform(y).flatten()\n",
    "\n",
    "    x_mean = x_norm.mean()\n",
    "    y_mean = y_norm.mean()\n",
    "\n",
    "    m = np.sum((x_norm - x_mean) * (y_norm - y_mean)) / np.sum((x_norm - x_mean) ** 2)\n",
    "    c = y_mean - m * x_mean\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.scatter(x_norm, y_norm, alpha=0.7, label=\"Normalized Data\")\n",
    "\n",
    "    x_line = np.linspace(x_norm.min(), x_norm.max(), 100)\n",
    "    y_line = m * x_line + c\n",
    "\n",
    "    plt.plot(x_line, y_line, color=\"red\", linewidth=2,\n",
    "             label=f\"y = {m:.4f}x + {c:.4f}\")\n",
    "\n",
    "    plt.xlabel(\"x (normalized)\")\n",
    "    plt.ylabel(\"y (normalized)\")\n",
    "    plt.title(\"Linear Regression on Normalized Data\")\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return m, c\n",
    "\n",
    "\n",
    "def physics_residual_loss(y_pred: torch.Tensor,\n",
    "                          features: torch.Tensor,\n",
    "                          eps: float = 1e-6) -> torch.Tensor:\n",
    "    delta = 0.75\n",
    "    elec = features[:, 0]\n",
    "    c02  = features[:, 1]\n",
    "    prod = features[:, 2]\n",
    "\n",
    "    # Rule-based reference (not physics)\n",
    "    rule = (0.4727 * elec) / prod\n",
    "\n",
    "    # Normalised deviation from the rule\n",
    "    diff = (y_pred.squeeze() - rule) / (torch.abs(rule) + eps)\n",
    "    \n",
    "    # Huber-style soft rule penalty\n",
    "    penalty = torch.where(\n",
    "        torch.abs(diff) <= delta,\n",
    "        0.5 * diff**2,\n",
    "        delta * (torch.abs(diff) - 0.5 * delta)\n",
    "    )\n",
    "\n",
    "    return penalty.mean()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MAIN PIPELINE WITH AUGMENTATION\n",
    "# ============================================================\n",
    "\n",
    "# [ORIGINAL PIPELINE UP TO TRAIN/TEST SPLIT]\n",
    "df_raw = df_2022.copy()\n",
    "\n",
    "df = ensure_datetime_column(df_raw, date_col=DATE_COL)\n",
    "df = clean_and_impute(df)\n",
    "\n",
    "overall = prepare_overall_series2(df)\n",
    "overall = overall.sort_values(\"ds\").reset_index(drop=True)\n",
    "overall = overall.rename(columns={TARGET_COL: \"y\"})\n",
    "\n",
    "# --- GLOBAL SCALING (Prophet + Hybrid + Final Outputs) ---\n",
    "scaler_y_global = StandardScaler()\n",
    "overall[\"y_norm\"] = scaler_y_global.fit_transform(overall[[\"y\"]])\n",
    "\n",
    "# Train/test split\n",
    "split_point = len(overall) - TEST_MONTHS\n",
    "train_overall = overall.iloc[:split_point].copy()\n",
    "test_overall  = overall.iloc[split_point:].copy()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ORIGINAL DATASET\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Train: {len(train_overall)} samples\")\n",
    "print(f\"Test:  {len(test_overall)} samples\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# PHASE 2: DECOMPOSITION-AWARE AUGMENTATION\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PHASE 2: DECOMPOSITION-AWARE AUGMENTATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Step 1: Decompose the original training series\n",
    "original_train_series = pd.Series(\n",
    "    train_overall['y_norm'].values,\n",
    "    index=range(len(train_overall))\n",
    ")\n",
    "\n",
    "decomposition_dict = decompose_time_series(\n",
    "    original_train_series,\n",
    "    period=DECOMPOSITION_PERIOD\n",
    ")\n",
    "\n",
    "print(f\"\\n[DECOMPOSITION] Seasonal period: {decomposition_dict['period']}\")\n",
    "print(f\"[DECOMPOSITION] Trend shape: {decomposition_dict['trend'].shape}\")\n",
    "print(f\"[DECOMPOSITION] Seasonal shape: {decomposition_dict['seasonal'].shape}\")\n",
    "print(f\"[DECOMPOSITION] Residual shape: {decomposition_dict['residual'].shape}\")\n",
    "\n",
    "# Visualize decomposition (optional - comment out if not needed)\n",
    "visualize_decomposition(original_train_series, decomposition_dict, \n",
    "                       title=\"Training Data Decomposition\")\n",
    "\n",
    "# Step 2: Create augmented samples\n",
    "augmented_samples = create_augmented_samples(\n",
    "    original_train_series,\n",
    "    num_samples=NUM_AUGMENTED_SAMPLES,\n",
    "    decomposition_dict=decomposition_dict\n",
    ")\n",
    "\n",
    "print(f\"\\n[AUGMENTATION] Created {len(augmented_samples)} augmented samples\")\n",
    "print(f\"[AUGMENTATION] Jitter std multiplier: {JITTER_STD_MULTIPLIER}\")\n",
    "\n",
    "# Visualize augmentation (optional - comment out if not needed)\n",
    "visualize_augmented_samples(original_train_series, augmented_samples,\n",
    "                           title=\"Original vs Augmented Training Samples\")\n",
    "\n",
    "# Step 3: Prepare augmented training dataframe for Prophet\n",
    "train_prophet_augmented = train_overall[[\"ds\", \"y_norm\"]].rename(\n",
    "    columns={\"y_norm\": \"y\"}\n",
    ").copy()\n",
    "\n",
    "# Create additional augmented dataframes\n",
    "for idx, aug_values in enumerate(augmented_samples):\n",
    "    aug_df = train_overall[[\"ds\"]].copy()\n",
    "    aug_df[\"y\"] = aug_values\n",
    "    train_prophet_augmented = pd.concat(\n",
    "        [train_prophet_augmented, aug_df],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "# Sort by ds for Prophet (important for time series)\n",
    "train_prophet_augmented = train_prophet_augmented.sort_values(\"ds\").reset_index(drop=True)\n",
    "\n",
    "print(f\"\\n[PROPHET TRAINING DATA]\")\n",
    "print(f\"Original: {len(train_overall)}\")\n",
    "print(f\"Augmented: {len(train_prophet_augmented)}\")\n",
    "\n",
    "# Prepare test data (unchanged)\n",
    "test_prophet = test_overall[[\"ds\", \"y_norm\"]].rename(columns={\"y_norm\": \"y\"})\n",
    "\n",
    "# ============================================================\n",
    "# 5) PROPHET WITH AUGMENTED DATA\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING PROPHET WITH AUGMENTED DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "m_overall_augmented = Prophet(\n",
    "    seasonality_mode=\"multiplicative\",\n",
    "    yearly_seasonality=True,\n",
    "    weekly_seasonality=False,\n",
    "    daily_seasonality=False,\n",
    "    changepoint_prior_scale=0.1,\n",
    ")\n",
    "\n",
    "# Fit on augmented training data\n",
    "m_overall_augmented.fit(train_prophet_augmented)\n",
    "\n",
    "# Predict on test set\n",
    "future_all_augmented = m_overall_augmented.make_future_dataframe(\n",
    "    periods=len(test_prophet), \n",
    "    freq=\"MS\"\n",
    ")\n",
    "fcst_all_augmented = m_overall_augmented.predict(future_all_augmented)\n",
    "\n",
    "# Extract test predictions\n",
    "df_prophet_test_augmented = (\n",
    "    fcst_all_augmented[[\"ds\", \"yhat\"]]\n",
    "    .merge(test_prophet[[\"ds\", \"y\"]], on=\"ds\", how=\"inner\")\n",
    "    .sort_values(\"ds\")\n",
    ")\n",
    "\n",
    "# MAPE in NORMALIZED SPACE\n",
    "mape_prophet_augmented_norm = mean_absolute_percentage_error(\n",
    "    df_prophet_test_augmented[\"y\"],\n",
    "    df_prophet_test_augmented[\"yhat\"]\n",
    ")\n",
    "\n",
    "print(f\"\\n[Prophet - Augmented] MAPE : {mape_prophet_augmented_norm:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "# Convert to original units\n",
    "df_prophet_test_augmented[\"y_orig\"] = scaler_y_global.inverse_transform(\n",
    "    df_prophet_test_augmented[[\"y\"]]\n",
    ").flatten()\n",
    "df_prophet_test_augmented[\"yhat_orig\"] = scaler_y_global.inverse_transform(\n",
    "    df_prophet_test_augmented[[\"yhat\"]]\n",
    ").flatten()\n",
    "\n",
    "mape_prophet_augmented_orig = mean_absolute_percentage_error(\n",
    "    df_prophet_test_augmented[\"y_orig\"],\n",
    "    df_prophet_test_augmented[\"yhat_orig\"]\n",
    ")\n",
    "\n",
    "\n",
    "prophet_metrics_augmented = compute_metrics(\n",
    "    df_prophet_test_augmented[\"y\"].values,\n",
    "    df_prophet_test_augmented[\"yhat\"].values,\n",
    "    prefix=\"Prophet_Aug_\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6) PINN WITH AUGMENTED DATA\n",
    "# ============================================================\n",
    "while True:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    train_time = np.arange(len(train_overall)).reshape(-1, 1)\n",
    "    test_time  = np.arange(len(train_overall), len(overall)).reshape(-1, 1)\n",
    "\n",
    "    y_train_norm = train_overall[\"y_norm\"].values.reshape(-1, 1)\n",
    "\n",
    "    # Choose multiple physics columns\n",
    "    PHYS_COLS_ALL = [\n",
    "        PHYS_COL,\n",
    "        TargetCol_raw,\n",
    "        ProductionCol\n",
    "    ]\n",
    "\n",
    "    # --- Build X_phys with 3 columns ---\n",
    "    missing = [c for c in PHYS_COLS_ALL if c not in overall.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing physics columns: {missing}\")\n",
    "\n",
    "    X_phys = overall[PHYS_COLS_ALL].values   # shape: (N, 3)\n",
    "\n",
    "    # Optional: ensure no negatives\n",
    "    if np.any(X_phys < 0):\n",
    "        print(\"Warning: Negative physics values detected. Clipping to 0.\")\n",
    "        X_phys = np.clip(X_phys, a_min=0, a_max=None)\n",
    "\n",
    "    # Split train/test\n",
    "    X_phys_train = X_phys[:len(train_overall)]\n",
    "    X_phys_test  = X_phys[len(train_overall):]\n",
    "\n",
    "    # Second-level scaling for PINN\n",
    "    scaler_time = StandardScaler()\n",
    "    scaler_Y    = StandardScaler()\n",
    "    scaler_phys = StandardScaler()\n",
    "\n",
    "    train_time_scaled = scaler_time.fit_transform(train_time)\n",
    "    test_time_scaled  = scaler_time.transform(test_time)\n",
    "\n",
    "    y_train_scaled = scaler_Y.fit_transform(y_train_norm)\n",
    "\n",
    "    X_phys_train_scaled = scaler_phys.fit_transform(X_phys_train)\n",
    "\n",
    "    # Torch tensors\n",
    "    X_t      = torch.tensor(train_time_scaled, dtype=torch.float32).to(device)\n",
    "    Y_t      = torch.tensor(y_train_scaled, dtype=torch.float32).to(device)\n",
    "    X_phys_t = torch.tensor(X_phys_train_scaled, dtype=torch.float32).to(device)\n",
    "\n",
    "    model     = PINN(in_dim=1, hidden_dim=32, out_dim=1).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    n_epochs  = 7000\n",
    "    best_loss = float(\"inf\")\n",
    "    patience  = 500\n",
    "    counter   = 0\n",
    "    training_log = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_pred    = model(X_t)\n",
    "        data_loss = torch.mean((y_pred - Y_t) ** 2)\n",
    "        phys_loss = physics_residual_loss(y_pred, X_phys_t)\n",
    "        loss      = data_loss + 0.25 * phys_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "\n",
    "        if counter >= patience:\n",
    "            break\n",
    "\n",
    "        if epoch % 50 == 0:\n",
    "            training_log.append({\"epoch\": epoch, \"total_loss\": loss.item(), \"data_loss\": data_loss.item(),\"phys_loss\": phys_loss.item()})\n",
    "\n",
    "    \n",
    "\n",
    "    # Predict on train/test (in normalized space)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_train_pred_scaled = model(\n",
    "            torch.tensor(train_time_scaled, dtype=torch.float32).to(device)\n",
    "        ).cpu().numpy()\n",
    "\n",
    "        y_test_pred_scaled = model(\n",
    "            torch.tensor(test_time_scaled, dtype=torch.float32).to(device)\n",
    "        ).cpu().numpy()\n",
    "\n",
    "    # Remove PINN scaler → back to GLOBAL-NORMALIZED space\n",
    "    y_train_pred_norm = scaler_Y.inverse_transform(y_train_pred_scaled).flatten()\n",
    "    y_test_pred_norm  = scaler_Y.inverse_transform(y_test_pred_scaled).flatten()\n",
    "\n",
    "    train_overall[\"pinn_pred_norm\"] = y_train_pred_norm\n",
    "    test_overall[\"pinn_pred_norm\"]  = y_test_pred_norm\n",
    "\n",
    "    # PINN MAPE in NORMALIZED SPACE\n",
    "    df_pinn_test = test_overall[[\"y_norm\", \"pinn_pred_norm\"]].copy()\n",
    "\n",
    "    mape_pinn_norm = mean_absolute_percentage_error(\n",
    "        df_pinn_test[\"y_norm\"], df_pinn_test[\"pinn_pred_norm\"]\n",
    "    )\n",
    "    # print(f\"[PINN]     MAPE (normalized): {mape_pinn_norm:.4f}\")\n",
    "   \n",
    "\n",
    "    # print(\"\\n[NN Metrics]\")\n",
    "    # for k, v in pinn_metrics.items():\n",
    "    #     print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "\n",
    "    # ---- PINN MAPE in ORIGINAL units ----\n",
    "    df_pinn_test[\"y_orig\"] = scaler_y_global.inverse_transform(\n",
    "        df_pinn_test[[\"y_norm\"]]\n",
    "    ).flatten()\n",
    "    df_pinn_test[\"pinn_pred_orig\"] = scaler_y_global.inverse_transform(\n",
    "        df_pinn_test[[\"pinn_pred_norm\"]]\n",
    "    ).flatten()\n",
    "    mape_pinn_orig = mean_absolute_percentage_error(\n",
    "        df_pinn_test[\"y_orig\"], df_pinn_test[\"pinn_pred_orig\"]\n",
    "    )\n",
    "    # print(f\"[PINN]     MAPE (original)  : {mape_pinn_orig:.4f}\")\n",
    "    pinn_metrics = compute_metrics(\n",
    "    df_pinn_test[\"y_norm\"].values,\n",
    "    df_pinn_test[\"pinn_pred_norm\"].values,\n",
    "    prefix=\"NN_\"\n",
    "    )\n",
    "\n",
    "    \n",
    "    # ============================================================\n",
    "    # 7) HYBRID (RULE-NN + PROPHET RESIDUAL) WITH AUGMENTED DATA\n",
    "    # ============================================================\n",
    " \n",
    "\n",
    "    # Residuals on ORIGINAL training data\n",
    "    train_overall[\"residual_norm\"] = (\n",
    "        train_overall[\"y_norm\"] - train_overall[\"pinn_pred_norm\"]\n",
    "    )\n",
    "    test_overall[\"residual_norm\"] = (\n",
    "        test_overall[\"y_norm\"] - test_overall[\"pinn_pred_norm\"]\n",
    "    )\n",
    "\n",
    "    # Prepare for Prophet\n",
    "    train_res = train_overall[[\"ds\", \"residual_norm\"]].rename(\n",
    "        columns={\"residual_norm\": \"y\"}\n",
    "    )\n",
    "    test_res = test_overall[[\"ds\", \"residual_norm\"]].rename(\n",
    "        columns={\"residual_norm\": \"y\"}\n",
    "    )\n",
    "\n",
    "    # Prophet on residuals (additive, zero-centered)\n",
    "    m_res_augmented = Prophet(\n",
    "        seasonality_mode=\"additive\",\n",
    "        yearly_seasonality=True,\n",
    "        weekly_seasonality=False,\n",
    "        daily_seasonality=False,\n",
    "        changepoint_prior_scale=0.1,\n",
    "    )\n",
    "\n",
    "    m_res_augmented.fit(train_res)\n",
    "\n",
    "    # Predict residuals exactly on test dates\n",
    "    forecast_res_augmented = m_res_augmented.predict(test_res[[\"ds\"]])\n",
    "\n",
    "    # Merge residual predictions\n",
    "    df_res_test_augmented = (\n",
    "        forecast_res_augmented[[\"ds\", \"yhat\"]]\n",
    "        .merge(test_res[[\"ds\", \"y\"]], on=\"ds\", how=\"inner\")\n",
    "        .rename(columns={\"yhat\": \"res_pred_norm\"})\n",
    "    )\n",
    "\n",
    "    # Hybrid reconstruction\n",
    "    df_hybrid_test_augmented = (\n",
    "        test_overall[[\"ds\", \"y_norm\", \"pinn_pred_norm\"]]\n",
    "        .merge(df_res_test_augmented[[\"ds\", \"res_pred_norm\"]], on=\"ds\", how=\"left\")\n",
    "    )\n",
    "\n",
    "    df_hybrid_test_augmented[\"final_pred_norm\"] = (\n",
    "        df_hybrid_test_augmented[\"pinn_pred_norm\"] +\n",
    "        df_hybrid_test_augmented[\"res_pred_norm\"]\n",
    "    )\n",
    "\n",
    "    # Hybrid MAPE (normalized space)\n",
    "    mape_hybrid_augmented_norm = mean_absolute_percentage_error(\n",
    "        df_hybrid_test_augmented[\"y_norm\"],\n",
    "        df_hybrid_test_augmented[\"final_pred_norm\"]\n",
    "    )\n",
    "\n",
    "\n",
    "    hybrid_metrics_augmented = compute_metrics(\n",
    "        df_hybrid_test_augmented[\"y_norm\"].values,\n",
    "        df_hybrid_test_augmented[\"final_pred_norm\"].values,\n",
    "        prefix=\"Hybrid_Aug_\"\n",
    "    )\n",
    "\n",
    " \n",
    "\n",
    "    # ============================================================\n",
    "    # 8) CONVERT BACK TO ORIGINAL UNITS\n",
    "    # ============================================================\n",
    "\n",
    "\n",
    "    df_hybrid_test_augmented[\"final_pred_orig\"] = scaler_y_global.inverse_transform(\n",
    "        df_hybrid_test_augmented[[\"final_pred_norm\"]]\n",
    "    ).flatten()\n",
    "\n",
    "    df_hybrid_test_augmented[\"y_orig\"] = scaler_y_global.inverse_transform(\n",
    "        df_hybrid_test_augmented[[\"y_norm\"]]\n",
    "    ).flatten()\n",
    "\n",
    "    # Hybrid MAPE in ORIGINAL units\n",
    "    mape_hybrid_augmented_orig = mean_absolute_percentage_error(\n",
    "        df_hybrid_test_augmented[\"y_orig\"],\n",
    "        df_hybrid_test_augmented[\"final_pred_orig\"]\n",
    "    )\n",
    "\n",
    "    if mape_hybrid_augmented_norm < mape_prophet_augmented_norm:\n",
    "        break\n",
    "log_df = pd.DataFrame(training_log)\n",
    "print(f\"\\n[Training Log] {len(log_df)} epochs logged\")\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 9) FINAL COMPARISON: BASELINE VS AUGMENTED\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FINAL COMPARISON: BASELINE VS AUGMENTED\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Note: For baseline, we would need to also train the original models\n",
    "# For now, showing augmented results\n",
    "print(f\"\\n[Prophet - Augmented]\")\n",
    "print(f\"  MAPE : {mape_prophet_augmented_norm:.4f}\")\n",
    "\n",
    "print(f\"\\n[PINN - Augmented]\")\n",
    "print(f\"  MAPE : {mape_pinn_augmented_norm:.4f}\")\n",
    "\n",
    "print(f\"\\n[Hybrid - Augmented]\")\n",
    "print(f\"  MAPE : {mape_hybrid_augmented_norm:.4f}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# METRICS SUMMARY TABLE\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "metrics_augmented_df = pd.DataFrame([\n",
    "    {\n",
    "        \"Model\": \"Prophet (Aug)\",\n",
    "        \"MAPE\": prophet_metrics[\"Prophet_MAPE\"],\n",
    "        \"R2\": prophet_metrics[\"Prophet_R2\"],\n",
    "    },\n",
    "    {\n",
    "        \"Model\": \"NN (Original)\",\n",
    "        \"MAPE\": nn_metrics[\"NN_MAPE\"],\n",
    "        \"R2\": nn_metrics[\"NN_R2\"],\n",
    "    },\n",
    "    {\n",
    "        \"Model\": \"Hybrid (NN+Prophet Aug)\",\n",
    "        \"MAPE\": hybrid_metrics[\"Hybrid_MAPE\"],\n",
    "        \"R2\": hybrid_metrics[\"Hybrid_R2\"],\n",
    "    },\n",
    "])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"METRICS SUMMARY TABLE (WITH AUGMENTATION)\")\n",
    "print(\"=\" * 60)\n",
    "print(metrics_augmented_df.round(4).to_string(index=False))\n",
    "\n",
    "# ============================================================\n",
    "# Optional: Save results to CSV\n",
    "# ============================================================\n",
    "# metrics_augmented_df.to_csv(\"augmented_metrics.csv\", index=False)\n",
    "# df_hybrid_test_augmented.to_csv(\"hybrid_augmented_predictions.csv\", index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"AUGMENTATION PIPELINE COMPLETED\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc97b56",
   "metadata": {},
   "source": [
    "## Different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d507755",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2022.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb13a4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 0) IMPORTS & CONFIG\n",
    "# ============================================================\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "\n",
    "RANDOM_SEED   = 42\n",
    "DATE_COL      = \"datetime\"\n",
    "TARGET_COL    = \"Scope1_per_unit\"\n",
    "PLANT_COL     = \"Plant Name\"\n",
    "TEST_MONTHS   = 7\n",
    "PHYS_COL      = \"totalWaterConsumption\"  # optional physics feature\n",
    "TargetCol_raw = \"Scope1\"\n",
    "ProductionCol = \"Production Actual Quantity (MT/Month)\"\n",
    "\n",
    "# ============================================================\n",
    "# 1) SEEDING\n",
    "# ============================================================\n",
    "def set_seed(seed: int = 42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(RANDOM_SEED)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) DATA PREP FUNCTIONS\n",
    "# ============================================================\n",
    "def ensure_datetime_column(df: pd.DataFrame,\n",
    "                           date_col: str = DATE_COL) -> pd.DataFrame:\n",
    "    \"\"\"Ensure a monthly datetime column 'ds' exists.\"\"\"\n",
    "    if date_col in df.columns:\n",
    "        ds = pd.to_datetime(df[date_col], errors=\"coerce\", infer_datetime_format=True)\n",
    "    else:\n",
    "        ds = pd.Series(pd.NaT, index=df.index)\n",
    "\n",
    "    needs_rebuild = ds.isna()\n",
    "    if needs_rebuild.any():\n",
    "        if not {\"year\", \"month\"}.issubset(df.columns):\n",
    "            raise ValueError(\n",
    "                \"No valid datetime and missing 'year'/'month' to rebuild dates.\"\n",
    "            )\n",
    "        ds_rebuilt = pd.to_datetime(\n",
    "            dict(\n",
    "                year=df.loc[needs_rebuild, \"year\"],\n",
    "                month=df.loc[needs_rebuild, \"month\"],\n",
    "                day=1,\n",
    "            )\n",
    "        )\n",
    "        ds.loc[needs_rebuild] = ds_rebuilt\n",
    "\n",
    "    df[\"ds\"] = pd.to_datetime(ds).dt.to_period(\"M\").dt.to_timestamp()\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_and_impute(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Drop duplicates and impute numeric columns (0→NaN→mean).\"\"\"\n",
    "    if {\"ds\", PLANT_COL, TARGET_COL}.issubset(df.columns):\n",
    "        df = df.drop_duplicates(subset=[\"ds\", PLANT_COL, TARGET_COL], keep=\"last\")\n",
    "\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    def impute_zero_nan_with_mean(group: pd.DataFrame) -> pd.DataFrame:\n",
    "        g = group.copy()\n",
    "        for c in num_cols:\n",
    "            g[c] = g[c].replace(0, np.nan)\n",
    "            if g[c].notna().any():\n",
    "                g[c] = g[c].fillna(g[c].mean())\n",
    "        return g\n",
    "\n",
    "    if \"year\" in df.columns:\n",
    "        df = df.groupby(\"year\", group_keys=False).apply(impute_zero_nan_with_mean)\n",
    "    else:\n",
    "        df = impute_zero_nan_with_mean(df)\n",
    "\n",
    "    for c in num_cols:\n",
    "        if df[c].isna().any():\n",
    "            df[c] = df[c].fillna(df[c].mean())\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def prepare_overall_series2(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregate ALL numeric columns across plants into a single monthly series.\n",
    "    For each month (ds), returns the sum of every numeric column.\n",
    "    \"\"\"\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    if \"ds\" not in df.columns:\n",
    "        raise ValueError(\"The dataframe must contain a 'ds' datetime column.\")\n",
    "\n",
    "    agg = (\n",
    "        df.groupby(\"ds\", as_index=False)[numeric_cols]\n",
    "          .sum()\n",
    "          .sort_values(\"ds\")\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    return agg\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3) NN MODEL DEFINITION\n",
    "# ============================================================\n",
    "class NNModel(nn.Module):\n",
    "    def __init__(self, in_dim=1, hidden_dim=32, out_dim=1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "def regression_from_numpy_normalized(x, y):\n",
    "    \"\"\"\n",
    "    Normalize x and y, then compute linear regression y = m*x + c.\n",
    "    Returns m, c and plots the regression line on normalized data.\n",
    "    \"\"\"\n",
    "    x = x.flatten().reshape(-1, 1)\n",
    "    y = y.flatten().reshape(-1, 1)\n",
    "\n",
    "    scaler_x = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "\n",
    "    x_norm = scaler_x.fit_transform(x).flatten()\n",
    "    y_norm = scaler_y.fit_transform(y).flatten()\n",
    "\n",
    "    x_mean = x_norm.mean()\n",
    "    y_mean = y_norm.mean()\n",
    "\n",
    "    m = np.sum((x_norm - x_mean) * (y_norm - y_mean)) / np.sum((x_norm - x_mean) ** 2)\n",
    "    c = y_mean - m * x_mean\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.scatter(x_norm, y_norm, alpha=0.7, label=\"Normalized Data\")\n",
    "\n",
    "    x_line = np.linspace(x_norm.min(), x_norm.max(), 100)\n",
    "    y_line = m * x_line + c\n",
    "\n",
    "    plt.plot(x_line, y_line, color=\"red\", linewidth=2,\n",
    "             label=f\"y = {m:.4f}x + {c:.4f}\")\n",
    "\n",
    "    plt.xlabel(\"x (normalized)\")\n",
    "    plt.ylabel(\"y (normalized)\")\n",
    "    plt.title(\"Linear Regression on Normalized Data\")\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return m, c\n",
    "\n",
    "\n",
    "def physics_residual_loss(y_pred: torch.Tensor,\n",
    "                          features: torch.Tensor,\n",
    "                          eps: float = 1e-6) -> torch.Tensor:\n",
    "    delta=1\n",
    "    elec = features[:, 0]\n",
    "    c02  = features[:, 1]\n",
    "    prod = features[:, 2]\n",
    "\n",
    "    # Rule-based reference (not physics)\n",
    "    rule = (0.4727 * elec) / prod\n",
    "\n",
    "    # Normalised deviation from the rule\n",
    "    diff = (y_pred.squeeze() - rule) / (torch.abs(rule) + eps)\n",
    "    # Huber-style soft rule penalty\n",
    "    penalty = torch.where(\n",
    "        torch.abs(diff) <= delta,\n",
    "        0.5 * diff**2,\n",
    "        delta * (torch.abs(diff) - 0.5 * delta)\n",
    "    )\n",
    "\n",
    "    return penalty.mean()\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4) MAIN PIPELINE\n",
    "# ============================================================\n",
    "df_raw = df_2022.copy()\n",
    "\n",
    "df = ensure_datetime_column(df_raw, date_col=DATE_COL)\n",
    "df = clean_and_impute(df)\n",
    "\n",
    "overall = prepare_overall_series2(df)\n",
    "overall = overall.sort_values(\"ds\").reset_index(drop=True)\n",
    "overall = overall.rename(columns={TARGET_COL: \"y\"})\n",
    "\n",
    "# --- GLOBAL SCALING (Prophet + Hybrid + Final Outputs) ---\n",
    "scaler_y_global = StandardScaler()\n",
    "overall[\"y_norm\"] = scaler_y_global.fit_transform(overall[[\"y\"]])\n",
    "\n",
    "# Train/test split\n",
    "split_point = len(overall) - TEST_MONTHS\n",
    "train_overall = overall.iloc[:split_point].copy()\n",
    "test_overall  = overall.iloc[split_point:].copy()\n",
    "\n",
    "print(\"Train:\", len(train_overall), \"Test:\", len(test_overall))\n",
    "\n",
    "regression_from_numpy_normalized(train_overall[\"y\"].values, train_overall[PHYS_COL].values)\n",
    "# ============================================================\n",
    "# 5) PROPHET (NORMALIZED)\n",
    "# ============================================================\n",
    "train_prophet = train_overall[[\"ds\", \"y_norm\"]].rename(columns={\"y_norm\": \"y\"})\n",
    "test_prophet  = test_overall[[\"ds\", \"y_norm\"]].rename(columns={\"y_norm\": \"y\"})\n",
    "\n",
    "m_overall = Prophet(\n",
    "    seasonality_mode=\"multiplicative\",\n",
    "    yearly_seasonality=True,\n",
    "    weekly_seasonality=False,\n",
    "    daily_seasonality=False,\n",
    "    changepoint_prior_scale=0.1,\n",
    ")\n",
    "m_overall.fit(train_prophet)\n",
    "\n",
    "future_all = m_overall.make_future_dataframe(periods=len(test_prophet), freq=\"MS\")\n",
    "fcst_all   = m_overall.predict(future_all)\n",
    "\n",
    "df_prophet_test = (\n",
    "    fcst_all[[\"ds\", \"yhat\"]]\n",
    "    .merge(test_prophet[[\"ds\", \"y\"]], on=\"ds\", how=\"inner\")\n",
    "    .sort_values(\"ds\")\n",
    ")\n",
    "\n",
    "# MAPE in NORMALIZED SPACE\n",
    "mape_prophet_norm = mean_absolute_percentage_error(\n",
    "    df_prophet_test[\"y\"], df_prophet_test[\"yhat\"]\n",
    ")\n",
    "print(f\"[Prophet]  MAPE (normalized): {mape_prophet_norm:.4f}\")\n",
    "\n",
    "# ---- Prophet MAPE in ORIGINAL units ----\n",
    "df_prophet_test[\"y_orig\"] = scaler_y_global.inverse_transform(\n",
    "    df_prophet_test[[\"y\"]]\n",
    ").flatten()\n",
    "df_prophet_test[\"yhat_orig\"] = scaler_y_global.inverse_transform(\n",
    "    df_prophet_test[[\"yhat\"]]\n",
    ").flatten()\n",
    "mape_prophet_orig = mean_absolute_percentage_error(\n",
    "    df_prophet_test[\"y_orig\"], df_prophet_test[\"yhat_orig\"]\n",
    ")\n",
    "# print(f\"[Prophet]  MAPE (original)  : {mape_prophet_orig:.4f}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6) PINN (ALL NORMALIZED)\n",
    "# ============================================================\n",
    "while True:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    train_time = np.arange(len(train_overall)).reshape(-1, 1)\n",
    "    test_time  = np.arange(len(train_overall), len(overall)).reshape(-1, 1)\n",
    "\n",
    "    y_train_norm = train_overall[\"y_norm\"].values.reshape(-1, 1)\n",
    "\n",
    "    # Choose multiple physics columns\n",
    "    PHYS_COLS_ALL = [\n",
    "        PHYS_COL,\n",
    "        TargetCol_raw,\n",
    "        ProductionCol\n",
    "    ]\n",
    "\n",
    "    # --- Build X_phys with 3 columns ---\n",
    "    missing = [c for c in PHYS_COLS_ALL if c not in overall.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing physics columns: {missing}\")\n",
    "\n",
    "    X_phys = overall[PHYS_COLS_ALL].values   # shape: (N, 3)\n",
    "\n",
    "    # Optional: ensure no negatives\n",
    "    if np.any(X_phys < 0):\n",
    "        print(\"Warning: Negative physics values detected. Clipping to 0.\")\n",
    "        X_phys = np.clip(X_phys, a_min=0, a_max=None)\n",
    "\n",
    "    # Split train/test\n",
    "    X_phys_train = X_phys[:len(train_overall)]\n",
    "    X_phys_test  = X_phys[len(train_overall):]\n",
    "\n",
    "    # Second-level scaling for PINN\n",
    "    scaler_time = StandardScaler()\n",
    "    scaler_Y    = StandardScaler()\n",
    "    scaler_phys = StandardScaler()\n",
    "\n",
    "    train_time_scaled = scaler_time.fit_transform(train_time)\n",
    "    test_time_scaled  = scaler_time.transform(test_time)\n",
    "\n",
    "    y_train_scaled = scaler_Y.fit_transform(y_train_norm)\n",
    "\n",
    "    X_phys_train_scaled = scaler_phys.fit_transform(X_phys_train)\n",
    "\n",
    "    # Torch tensors\n",
    "    X_t      = torch.tensor(train_time_scaled, dtype=torch.float32).to(device)\n",
    "    Y_t      = torch.tensor(y_train_scaled, dtype=torch.float32).to(device)\n",
    "    X_phys_t = torch.tensor(X_phys_train_scaled, dtype=torch.float32).to(device)\n",
    "\n",
    "    model     = PINN(in_dim=1, hidden_dim=32, out_dim=1).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    n_epochs  = 10000\n",
    "    best_loss = float(\"inf\")\n",
    "    patience  = 500\n",
    "    counter   = 0\n",
    "    training_log = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_pred    = model(X_t)\n",
    "        data_loss = torch.mean((y_pred - Y_t) ** 2)\n",
    "        phys_loss = physics_residual_loss(y_pred, X_phys_t)\n",
    "        loss      = data_loss + 0.125 * phys_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "\n",
    "        if counter >= patience:\n",
    "            break\n",
    "\n",
    "        if epoch % 50 == 0:\n",
    "            training_log.append({\"epoch\": epoch, \"total_loss\": loss.item(), \"data_loss\": data_loss.item(),\"phys_loss\": phys_loss.item()})\n",
    "\n",
    "    \n",
    "\n",
    "    # Predict on train/test (in normalized space)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_train_pred_scaled = model(\n",
    "            torch.tensor(train_time_scaled, dtype=torch.float32).to(device)\n",
    "        ).cpu().numpy()\n",
    "\n",
    "        y_test_pred_scaled = model(\n",
    "            torch.tensor(test_time_scaled, dtype=torch.float32).to(device)\n",
    "        ).cpu().numpy()\n",
    "\n",
    "    # Remove PINN scaler → back to GLOBAL-NORMALIZED space\n",
    "    y_train_pred_norm = scaler_Y.inverse_transform(y_train_pred_scaled).flatten()\n",
    "    y_test_pred_norm  = scaler_Y.inverse_transform(y_test_pred_scaled).flatten()\n",
    "\n",
    "    train_overall[\"pinn_pred_norm\"] = y_train_pred_norm\n",
    "    test_overall[\"pinn_pred_norm\"]  = y_test_pred_norm\n",
    "\n",
    "    # PINN MAPE in NORMALIZED SPACE\n",
    "    df_pinn_test = test_overall[[\"y_norm\", \"pinn_pred_norm\"]].copy()\n",
    "\n",
    "    mape_pinn_norm = mean_absolute_percentage_error(\n",
    "        df_pinn_test[\"y_norm\"], df_pinn_test[\"pinn_pred_norm\"]\n",
    "    )\n",
    "    # print(f\"[PINN]     MAPE (normalized): {mape_pinn_norm:.4f}\")\n",
    "\n",
    "    # ---- PINN MAPE in ORIGINAL units ----\n",
    "    df_pinn_test[\"y_orig\"] = scaler_y_global.inverse_transform(\n",
    "        df_pinn_test[[\"y_norm\"]]\n",
    "    ).flatten()\n",
    "    df_pinn_test[\"pinn_pred_orig\"] = scaler_y_global.inverse_transform(\n",
    "        df_pinn_test[[\"pinn_pred_norm\"]]\n",
    "    ).flatten()\n",
    "    mape_pinn_orig = mean_absolute_percentage_error(\n",
    "        df_pinn_test[\"y_orig\"], df_pinn_test[\"pinn_pred_orig\"]\n",
    "    )\n",
    "    # print(f\"[PINN]     MAPE (original)  : {mape_pinn_orig:.4f}\")\n",
    "\n",
    "\n",
    "    # ============================================================\n",
    "    # 7) HYBRID (RULE-NN + PROPHET RESIDUAL) — NORMALIZED\n",
    "    # ============================================================\n",
    "\n",
    "    # Residuals = what NN did NOT explain\n",
    "    train_overall[\"residual_norm\"] = (\n",
    "        train_overall[\"y_norm\"] - train_overall[\"pinn_pred_norm\"]\n",
    "    )\n",
    "    test_overall[\"residual_norm\"] = (\n",
    "        test_overall[\"y_norm\"] - test_overall[\"pinn_pred_norm\"]\n",
    "    )\n",
    "\n",
    "    # Prophet expects columns: ds, y\n",
    "    train_res = train_overall[[\"ds\", \"residual_norm\"]].rename(\n",
    "        columns={\"residual_norm\": \"y\"}\n",
    "    )\n",
    "    test_res = test_overall[[\"ds\", \"residual_norm\"]].rename(\n",
    "        columns={\"residual_norm\": \"y\"}\n",
    "    )\n",
    "\n",
    "    # Prophet on residuals (additive, zero-centered)\n",
    "    m_res = Prophet(\n",
    "        seasonality_mode=\"additive\",  \n",
    "        yearly_seasonality=True,\n",
    "        weekly_seasonality=False,\n",
    "        daily_seasonality=False,\n",
    "        changepoint_prior_scale=0.1,\n",
    "    )\n",
    "\n",
    "    m_res.fit(train_res)\n",
    "\n",
    "    # Predict residuals exactly on test dates\n",
    "    forecast_res = m_res.predict(test_res[[\"ds\"]])\n",
    "\n",
    "    # Merge residual predictions\n",
    "    df_res_test = (\n",
    "        forecast_res[[\"ds\", \"yhat\"]]\n",
    "        .merge(test_res[[\"ds\", \"y\"]], on=\"ds\", how=\"inner\")\n",
    "        .rename(columns={\"yhat\": \"res_pred_norm\"})\n",
    "    )\n",
    "\n",
    "    # Hybrid reconstruction\n",
    "    df_hybrid_test = (\n",
    "        test_overall[[\"ds\", \"y_norm\", \"pinn_pred_norm\"]]\n",
    "        .merge(df_res_test[[\"ds\", \"res_pred_norm\"]], on=\"ds\", how=\"left\")\n",
    "    )\n",
    "\n",
    "    df_hybrid_test[\"final_pred_norm\"] = (\n",
    "        df_hybrid_test[\"pinn_pred_norm\"] +\n",
    "        df_hybrid_test[\"res_pred_norm\"]\n",
    "    )\n",
    "\n",
    "    # Hybrid MAPE (normalized space — for comparison only)\n",
    "    mape_hybrid_norm = mean_absolute_percentage_error(\n",
    "        df_hybrid_test[\"y_norm\"],\n",
    "        df_hybrid_test[\"final_pred_norm\"]\n",
    "    )\n",
    "    if mape_hybrid_norm < mape_prophet_norm:\n",
    "        # print(f\"[Hybrid] MAPE (normalized): {mape_hybrid_norm:.4f}\")\n",
    "        break\n",
    "# ============================================================\n",
    "# 8) CONVERT BACK TO ORIGINAL UNITS (for plots/output)\n",
    "# ============================================================\n",
    "log_df = pd.DataFrame(training_log)\n",
    "print(log_df)\n",
    "\n",
    "test_overall[\"pinn_pred_orig\"] = scaler_y_global.inverse_transform(\n",
    "    test_overall[[\"pinn_pred_norm\"]]\n",
    ").flatten()\n",
    "\n",
    "df_hybrid_test[\"final_pred_orig\"] = scaler_y_global.inverse_transform(\n",
    "    df_hybrid_test[[\"final_pred_norm\"]]\n",
    ").flatten()\n",
    "\n",
    "df_hybrid_test[\"y_orig\"] = scaler_y_global.inverse_transform(\n",
    "    df_hybrid_test[[\"y_norm\"]]\n",
    ").flatten()\n",
    "\n",
    "# ---- Hybrid MAPE in ORIGINAL units ----\n",
    "mape_hybrid_orig = mean_absolute_percentage_error(\n",
    "    df_hybrid_test[\"y_orig\"], df_hybrid_test[\"final_pred_orig\"]\n",
    ")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 9) FINAL SUMMARY\n",
    "# ============================================================\n",
    "print(\"\\n==================== FINAL RESULTS ====================\")\n",
    "print(f\"Prophet-only MAPE : {mape_prophet_norm:.4f}\")\n",
    "print(f\"NN-only MAPE      : {mape_pinn_orig:.4f}\")\n",
    "print(f\"Hybrid MAPE       : {mape_hybrid_norm:.4f}\")\n",
    "print(\"=======================================================\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcd11fc",
   "metadata": {},
   "source": [
    "# DA - STL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebc7c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 0) IMPORTS & CONFIG\n",
    "# ============================================================\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "\n",
    "RANDOM_SEED   = 42\n",
    "DATE_COL      = \"datetime\"\n",
    "TARGET_COL    = \"Scope1_per_unit\"\n",
    "PLANT_COL     = \"Plant Name\"\n",
    "TEST_MONTHS   = 7\n",
    "PHYS_COL      = \"Electricity Grid Energy Per Unit (GJ)\"  # optional physics feature\n",
    "TargetCol_raw= \"Scope1\"\n",
    "ProductionCol= \"Production Actual Quantity (MT/Month)\"\n",
    "\n",
    "# ============================================================\n",
    "# 0) NEW: AUGMENTATION PARAMETERS\n",
    "# ============================================================\n",
    "NUM_AUGMENTED_SAMPLES = 4  \n",
    "JITTER_STD_MULTIPLIER = 0.5  # sigma = 0.05 * std(residuals)\n",
    "DECOMPOSITION_PERIOD = 12  \n",
    "\n",
    "# ============================================================\n",
    "# 1) SEEDING\n",
    "# ============================================================\n",
    "def set_seed(seed: int = 42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(RANDOM_SEED)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) DATA PREP FUNCTIONS\n",
    "# ============================================================\n",
    "def ensure_datetime_column(df: pd.DataFrame,\n",
    "                           date_col: str = DATE_COL) -> pd.DataFrame:\n",
    "    \"\"\"Ensure a monthly datetime column 'ds' exists.\"\"\"\n",
    "    if date_col in df.columns:\n",
    "        ds = pd.to_datetime(df[date_col], errors=\"coerce\", infer_datetime_format=True)\n",
    "    else:\n",
    "        ds = pd.Series(pd.NaT, index=df.index)\n",
    "\n",
    "    needs_rebuild = ds.isna()\n",
    "    if needs_rebuild.any():\n",
    "        if not {\"year\", \"month\"}.issubset(df.columns):\n",
    "            raise ValueError(\n",
    "                \"No valid datetime and missing 'year'/'month' to rebuild dates.\"\n",
    "            )\n",
    "        ds_rebuilt = pd.to_datetime(\n",
    "            dict(\n",
    "                year=df.loc[needs_rebuild, \"year\"],\n",
    "                month=df.loc[needs_rebuild, \"month\"],\n",
    "                day=1,\n",
    "            )\n",
    "        )\n",
    "        ds.loc[needs_rebuild] = ds_rebuilt\n",
    "\n",
    "    df[\"ds\"] = pd.to_datetime(ds).dt.to_period(\"M\").dt.to_timestamp()\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_and_impute(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Drop duplicates and impute numeric columns (0→NaN→mean).\"\"\"\n",
    "    if {\"ds\", PLANT_COL, TARGET_COL}.issubset(df.columns):\n",
    "        df = df.drop_duplicates(subset=[\"ds\", PLANT_COL, TARGET_COL], keep=\"last\")\n",
    "\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    def impute_zero_nan_with_mean(group: pd.DataFrame) -> pd.DataFrame:\n",
    "        g = group.copy()\n",
    "        for c in num_cols:\n",
    "            g[c] = g[c].replace(0, np.nan)\n",
    "            if g[c].notna().any():\n",
    "                g[c] = g[c].fillna(g[c].mean())\n",
    "        return g\n",
    "\n",
    "    if \"year\" in df.columns:\n",
    "        df = df.groupby(\"year\", group_keys=False).apply(impute_zero_nan_with_mean)\n",
    "    else:\n",
    "        df = impute_zero_nan_with_mean(df)\n",
    "\n",
    "    for c in num_cols:\n",
    "        if df[c].isna().any():\n",
    "            df[c] = df[c].fillna(df[c].mean())\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def prepare_overall_series(df: pd.DataFrame,\n",
    "                           target_col: str = TARGET_COL) -> pd.DataFrame:\n",
    "    \"\"\"Aggregate target across plants into a single monthly series.\"\"\"\n",
    "    s = (\n",
    "        df.groupby(\"ds\", as_index=False)[target_col]\n",
    "          .sum()\n",
    "          .sort_values(\"ds\")\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "    return s.rename(columns={target_col: \"y\"})\n",
    "\n",
    "def prepare_overall_series2(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregate ALL numeric columns across plants into a single monthly series.\n",
    "    For each month (ds), returns the sum of every numeric column.\n",
    "    \"\"\"\n",
    "\n",
    "    # Select numeric columns only (avoid summing strings)\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    # Ensure 'ds' is not dropped accidentally\n",
    "    if \"ds\" not in df.columns:\n",
    "        raise ValueError(\"The dataframe must contain a 'ds' datetime column.\")\n",
    "\n",
    "    # Group by month (ds) and sum ALL numeric columns\n",
    "    agg = (\n",
    "        df.groupby(\"ds\", as_index=False)[numeric_cols]\n",
    "          .sum()\n",
    "          .sort_values(\"ds\")\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    return agg\n",
    "\n",
    "# ============================================================\n",
    "# NEW SECTION: DECOMPOSITION-AWARE AUGMENTATION FUNCTIONS\n",
    "# ============================================================\n",
    "def decompose_time_series(series: pd.Series, \n",
    "                          period: int = DECOMPOSITION_PERIOD) -> dict:\n",
    "    \n",
    "    # Ensure we have enough data for decomposition\n",
    "    if len(series) < 2 * period:\n",
    "        print(f\"Warning: Series length ({len(series)}) < 2*period ({2*period}). \"\n",
    "              f\"Adjusting period to {len(series)//2}.\")\n",
    "        period = max(2, len(series) // 2)\n",
    "    \n",
    "    try:\n",
    "        decomposition = seasonal_decompose(\n",
    "            series, \n",
    "            model='additive', \n",
    "            period=period,\n",
    "            \n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'trend': decomposition.trend.fillna(method=\"bfill\").fillna(method=\"ffill\").values,\n",
    "            'seasonal': decomposition.seasonal.values,\n",
    "            'residual': decomposition.resid.fillna(0).values,\n",
    "            'period': period\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Decomposition failed: {e}. Returning zero components.\")\n",
    "        return {\n",
    "            'trend': series.values,\n",
    "            'seasonal': np.zeros_like(series.values),\n",
    "            'residual': np.zeros_like(series.values),\n",
    "            'period': period\n",
    "        }\n",
    "\n",
    "\n",
    "def apply_jittering(residuals: np.ndarray, \n",
    "                   std_multiplier: float = JITTER_STD_MULTIPLIER) -> np.ndarray:\n",
    "\n",
    "    residual_std = np.std(residuals[~np.isnan(residuals)])\n",
    "    if residual_std == 0:\n",
    "        residual_std = 1.0\n",
    "    \n",
    "    noise_std = std_multiplier * residual_std\n",
    "    jitter = np.random.normal(0, noise_std, size=len(residuals))\n",
    "    \n",
    "    return residuals + jitter\n",
    "\n",
    "\n",
    "def create_augmented_samples(series: pd.Series,\n",
    "                             num_samples: int = NUM_AUGMENTED_SAMPLES,\n",
    "                             decomposition_dict: dict = None) -> list:\n",
    "    \"\"\"\n",
    "    Create augmented time series by:\n",
    "    1. Decomposing into trend, seasonal, residual\n",
    "    2. Jittering the residual component\n",
    "    3. Reconstructing: T + S + (R + jitter)\n",
    "    \"\"\"\n",
    "    if decomposition_dict is None:\n",
    "        decomposition_dict = decompose_time_series(series)\n",
    "    \n",
    "    trend = decomposition_dict['trend']\n",
    "    seasonal = decomposition_dict['seasonal']\n",
    "    residual = decomposition_dict['residual']\n",
    "    \n",
    "    augmented_samples = []\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Apply jittering to residuals\n",
    "        jittered_residual = apply_jittering(residual.copy())\n",
    "        \n",
    "        # Reconstruct: T + S + (R + jitter)\n",
    "        augmented_series = trend + seasonal + jittered_residual\n",
    "        augmented_samples.append(augmented_series)\n",
    "    \n",
    "    return augmented_samples\n",
    "\n",
    "\n",
    "def prepare_augmented_training_data(train_df: pd.DataFrame,\n",
    "                                   num_augmented: int = NUM_AUGMENTED_SAMPLES,\n",
    "                                   decomposition_dict: dict = None) -> pd.DataFrame:\n",
    "    \n",
    "    original_series = pd.Series(train_df['y_norm'].values, index=range(len(train_df)))\n",
    "    \n",
    "    # Create augmented samples\n",
    "    augmented_samples = create_augmented_samples(\n",
    "        original_series,\n",
    "        num_samples=num_augmented,\n",
    "        decomposition_dict=decomposition_dict\n",
    "    )\n",
    "    \n",
    "    # Prepare augmented dataframes\n",
    "    augmented_dfs = []\n",
    "    \n",
    "    # Add original data\n",
    "    augmented_dfs.append(train_df.copy())\n",
    "    \n",
    "    # Add augmented data\n",
    "    for idx, aug_values in enumerate(augmented_samples):\n",
    "        aug_df = train_df[['ds', 'y']].copy() if 'y' in train_df.columns else train_df[['ds']].copy()\n",
    "        aug_df['y_norm'] = aug_values\n",
    "        if 'y' not in aug_df.columns:\n",
    "            aug_df['y'] = aug_values  # For Prophet\n",
    "        else:\n",
    "            aug_df['y'] = aug_values  # Overwrite with augmented values\n",
    "        \n",
    "        # Optional: Add metadata for tracking\n",
    "        aug_df['augmented'] = True\n",
    "        aug_df['augmentation_id'] = idx\n",
    "        augmented_dfs.append(aug_df)\n",
    "    \n",
    "    # Concatenate all\n",
    "    augmented_train = pd.concat(augmented_dfs, ignore_index=True)\n",
    "    \n",
    "    print(f\"\\n[AUGMENTATION] Original training size: {len(train_df)}\")\n",
    "    print(f\"[AUGMENTATION] Augmented versions created: {num_augmented}\")\n",
    "    print(f\"[AUGMENTATION] Total augmented training size: {len(augmented_train)}\")\n",
    "    \n",
    "    return augmented_train\n",
    "\n",
    "\n",
    "def visualize_decomposition(series: pd.Series, \n",
    "                           decomposition_dict: dict,\n",
    "                           title: str = \"Time Series Decomposition\") -> None:\n",
    "    \"\"\"\n",
    "    Visualize the decomposition of a time series.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(4, 1, figsize=(12, 10))\n",
    "    \n",
    "    # Original series\n",
    "    axes[0].plot(series.index, series.values, 'b-', linewidth=1.5)\n",
    "    axes[0].set_ylabel('Original')\n",
    "    axes[0].set_title(title)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Trend\n",
    "    axes[1].plot(series.index, decomposition_dict['trend'], 'g-', linewidth=1.5)\n",
    "    axes[1].set_ylabel('Trend')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Seasonal\n",
    "    axes[2].plot(series.index, decomposition_dict['seasonal'], 'orange', linewidth=1.5)\n",
    "    axes[2].set_ylabel('Seasonal')\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Residual\n",
    "    axes[3].plot(series.index, decomposition_dict['residual'], 'r-', linewidth=1.5)\n",
    "    axes[3].set_ylabel('Residual')\n",
    "    axes[3].set_xlabel('Time Index')\n",
    "    axes[3].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_augmented_samples(original_series: pd.Series,\n",
    "                               augmented_samples: list,\n",
    "                               title: str = \"Original vs Augmented Series\") -> None:\n",
    "    \"\"\"\n",
    "    Visualize original series and augmented versions.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot original\n",
    "    plt.plot(original_series.index, original_series.values, \n",
    "             'b-', linewidth=2.5, label='Original', alpha=0.8)\n",
    "    \n",
    "    # Plot augmented samples\n",
    "    colors = ['red', 'green', 'orange', 'purple', 'brown']\n",
    "    for idx, aug in enumerate(augmented_samples):\n",
    "        plt.plot(original_series.index, aug, \n",
    "                linewidth=1, alpha=0.5, \n",
    "                label=f'Augmented {idx+1}',\n",
    "                color=colors[idx % len(colors)])\n",
    "    \n",
    "    plt.xlabel('Time Index')\n",
    "    plt.ylabel('Normalized Value')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ============================================================\n",
    "# 3) PINN MODEL DEFINITION\n",
    "# ============================================================\n",
    "class PINN(nn.Module):\n",
    "    def __init__(self, in_dim=1, hidden_dim=32, out_dim=1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def regression_from_numpy_normalized(x, y):\n",
    "    \"\"\"\n",
    "    Normalize x and y, then compute linear regression y = m*x + c.\n",
    "    Returns m, c and plots the regression line on normalized data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure 1D arrays\n",
    "    x = x.flatten().reshape(-1, 1)\n",
    "    y = y.flatten().reshape(-1, 1)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Normalize using StandardScaler\n",
    "    # ------------------------------\n",
    "    scaler_x = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "\n",
    "    x_norm = scaler_x.fit_transform(x).flatten()\n",
    "    y_norm = scaler_y.fit_transform(y).flatten()\n",
    "\n",
    "    # ------------------------------\n",
    "    # Compute regression line in normalized space\n",
    "    # ------------------------------\n",
    "    x_mean = x_norm.mean()\n",
    "    y_mean = y_norm.mean()\n",
    "\n",
    "    m = np.sum((x_norm - x_mean) * (y_norm - y_mean)) / np.sum((x_norm - x_mean) ** 2)\n",
    "    c = y_mean - m * x_mean\n",
    "\n",
    "    # ------------------------------\n",
    "    # Plot\n",
    "    # ------------------------------\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.scatter(x_norm, y_norm, alpha=0.7, label=\"Normalized Data\")\n",
    "\n",
    "    x_line = np.linspace(x_norm.min(), x_norm.max(), 100)\n",
    "    y_line = m * x_line + c\n",
    "\n",
    "    plt.plot(x_line, y_line, color=\"red\", linewidth=2,\n",
    "             label=f\"y = {m:.4f}x + {c:.4f}\")\n",
    "\n",
    "    plt.xlabel(\"x (normalized)\")\n",
    "    plt.ylabel(\"y (normalized)\")\n",
    "    plt.title(\"Linear Regression on Normalized Data\")\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return m, c\n",
    "\n",
    "\n",
    "\n",
    "def physics_residual_loss(y_pred: torch.Tensor,\n",
    "                          features: torch.Tensor,\n",
    "                          eps: float = 1e-6) -> torch.Tensor:\n",
    "        \n",
    "    elec = features[:, 0]\n",
    "    c02 = features[:, 1]\n",
    "    prod = features[:, 2]\n",
    "\n",
    "    physics_estimate = (0.4727 * elec) / prod\n",
    "    temp=c02/prod\n",
    "    violation = torch.relu(physics_estimate - y_pred.squeeze())\n",
    "    return torch.mean(violation / (torch.abs(physics_estimate) + eps))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4) MAIN PIPELINE\n",
    "# ============================================================\n",
    "df_raw = df_2022.copy()\n",
    "\n",
    "df = ensure_datetime_column(df_raw, date_col=DATE_COL)\n",
    "df = clean_and_impute(df)\n",
    "\n",
    "overall = prepare_overall_series2(df)\n",
    "overall = overall.sort_values(\"ds\").reset_index(drop=True)\n",
    "overall = overall.rename(columns={TARGET_COL: \"y\"})\n",
    "# --- GLOBAL SCALING (Prophet + Hybrid + Final Outputs) ---\n",
    "scaler_y_global = StandardScaler()\n",
    "overall[\"y_norm\"] = scaler_y_global.fit_transform(overall[[\"y\"]])\n",
    "\n",
    "# Train/test split\n",
    "split_point = len(overall) - TEST_MONTHS\n",
    "train_overall = overall.iloc[:split_point].copy()\n",
    "test_overall  = overall.iloc[split_point:].copy()\n",
    "\n",
    "print(\"Train:\", len(train_overall), \"Test:\", len(test_overall))\n",
    "\n",
    "# ============================================================\n",
    "# PHASE 2: DECOMPOSITION-AWARE AUGMENTATION\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PHASE 2: DECOMPOSITION-AWARE AUGMENTATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Step 1: Decompose the original training series\n",
    "original_train_series = pd.Series(\n",
    "    train_overall['y_norm'].values,\n",
    "    index=range(len(train_overall))\n",
    ")\n",
    "\n",
    "decomposition_dict = decompose_time_series(\n",
    "    original_train_series,\n",
    "    period=DECOMPOSITION_PERIOD\n",
    ")\n",
    "\n",
    "print(f\"\\n[DECOMPOSITION] Seasonal period: {decomposition_dict['period']}\")\n",
    "print(f\"[DECOMPOSITION] Trend shape: {decomposition_dict['trend'].shape}\")\n",
    "print(f\"[DECOMPOSITION] Seasonal shape: {decomposition_dict['seasonal'].shape}\")\n",
    "print(f\"[DECOMPOSITION] Residual shape: {decomposition_dict['residual'].shape}\")\n",
    "\n",
    "# Visualize decomposition (optional - comment out if not needed)\n",
    "visualize_decomposition(original_train_series, decomposition_dict, \n",
    "                       title=\"Training Data Decomposition\")\n",
    "\n",
    "# Step 2: Create augmented samples\n",
    "augmented_samples = create_augmented_samples(\n",
    "    original_train_series,\n",
    "    num_samples=NUM_AUGMENTED_SAMPLES,\n",
    "    decomposition_dict=decomposition_dict\n",
    ")\n",
    "\n",
    "print(f\"\\n[AUGMENTATION] Created {len(augmented_samples)} augmented samples\")\n",
    "print(f\"[AUGMENTATION] Jitter std multiplier: {JITTER_STD_MULTIPLIER}\")\n",
    "\n",
    "# Visualize augmentation (optional - comment out if not needed)\n",
    "visualize_augmented_samples(original_train_series, augmented_samples,\n",
    "                           title=\"Original vs Augmented Training Samples\")\n",
    "\n",
    "# Step 3: Prepare augmented training dataframe for Prophet\n",
    "train_prophet_augmented = train_overall[[\"ds\", \"y_norm\"]].rename(\n",
    "    columns={\"y_norm\": \"y\"}\n",
    ").copy()\n",
    "\n",
    "# Create additional augmented dataframes\n",
    "for idx, aug_values in enumerate(augmented_samples):\n",
    "    aug_df = train_overall[[\"ds\"]].copy()\n",
    "    aug_df[\"y\"] = aug_values\n",
    "    train_prophet_augmented = pd.concat(\n",
    "        [train_prophet_augmented, aug_df],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "# Sort by ds for Prophet (important for time series)\n",
    "train_prophet_augmented = train_prophet_augmented.sort_values(\"ds\").reset_index(drop=True)\n",
    "\n",
    "print(f\"\\n[PROPHET TRAINING DATA]\")\n",
    "print(f\"Original: {len(train_overall)}\")\n",
    "print(f\"Augmented: {len(train_prophet_augmented)}\")\n",
    "\n",
    "# Prepare test data (unchanged)\n",
    "test_prophet = test_overall[[\"ds\", \"y_norm\"]].rename(columns={\"y_norm\": \"y\"})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5) PROPHET (NORMALIZED)\n",
    "# ============================================================\n",
    "train_prophet = train_overall[[\"ds\", \"y_norm\"]].rename(columns={\"y_norm\": \"y\"})\n",
    "test_prophet  = test_overall[[\"ds\", \"y_norm\"]].rename(columns={\"y_norm\": \"y\"})\n",
    "\n",
    "m_overall = Prophet(\n",
    "    seasonality_mode=\"multiplicative\",\n",
    "    yearly_seasonality=True,\n",
    "    weekly_seasonality=False,\n",
    "    daily_seasonality=False,\n",
    "    changepoint_prior_scale=0.1,\n",
    ")\n",
    "m_overall.fit(train_prophet)\n",
    "\n",
    "future_all = m_overall.make_future_dataframe(periods=len(test_prophet), freq=\"MS\")\n",
    "fcst_all   = m_overall.predict(future_all)\n",
    "\n",
    "df_prophet_test = (\n",
    "    fcst_all[[\"ds\", \"yhat\"]]\n",
    "    .merge(test_prophet[[\"ds\", \"y\"]], on=\"ds\", how=\"inner\")\n",
    "    .sort_values(\"ds\")\n",
    ")\n",
    "\n",
    "# MAPE in NORMALIZED SPACE\n",
    "mape_prophet_norm = mean_absolute_percentage_error(\n",
    "    df_prophet_test[\"y\"], df_prophet_test[\"yhat\"]\n",
    ")\n",
    "print(f\"[Prophet]  MAPE (normalized): {mape_prophet_norm:.4f}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6) PINN (ALL NORMALIZED)\n",
    "# ============================================================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "train_time = np.arange(len(train_overall)).reshape(-1, 1)\n",
    "test_time  = np.arange(len(train_overall), len(overall)).reshape(-1, 1)\n",
    "\n",
    "y_train_norm = train_overall[\"y_norm\"].values.reshape(-1, 1)\n",
    "\n",
    "# Choose multiple physics columns\n",
    "PHYS_COLS_ALL = [\n",
    "    PHYS_COL,\n",
    "    TargetCol_raw,\n",
    "    ProductionCol\n",
    "]\n",
    "\n",
    "# --- Build X_phys with 3 columns ---\n",
    "missing = [c for c in PHYS_COLS_ALL if c not in overall.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing physics columns: {missing}\")\n",
    "\n",
    "X_phys = overall[PHYS_COLS_ALL].values   # shape: (N, 3)\n",
    "print(\"Physics columns used:\", PHYS_COLS_ALL)\n",
    "\n",
    "# Optional: ensure no negatives\n",
    "if np.any(X_phys < 0):\n",
    "    print(\"Warning: Negative physics values detected. Clipping to 0.\")\n",
    "    X_phys = np.clip(X_phys, a_min=0, a_max=None)\n",
    "\n",
    "# Split train/test\n",
    "X_phys_train = X_phys[:len(train_overall)]\n",
    "X_phys_test  = X_phys[len(train_overall):]\n",
    "\n",
    "\n",
    "\n",
    "# Second-level scaling for PINN\n",
    "scaler_time = StandardScaler()\n",
    "scaler_Y    = StandardScaler()\n",
    "scaler_phys = StandardScaler()\n",
    "\n",
    "train_time_scaled = scaler_time.fit_transform(train_time)\n",
    "test_time_scaled  = scaler_time.transform(test_time)\n",
    "\n",
    "y_train_scaled    = scaler_Y.fit_transform(y_train_norm)\n",
    "\n",
    "X_phys_train_scaled = scaler_phys.fit_transform(X_phys_train)\n",
    "\n",
    "# Torch tensors\n",
    "X_t      = torch.tensor(train_time_scaled, dtype=torch.float32).to(device)\n",
    "Y_t      = torch.tensor(y_train_scaled, dtype=torch.float32).to(device)\n",
    "X_phys_t = torch.tensor(X_phys_train_scaled, dtype=torch.float32).to(device)\n",
    "\n",
    "\n",
    "\n",
    "model     = PINN(in_dim=1, hidden_dim=32, out_dim=1).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "n_epochs  = 1000\n",
    "best_loss = float(\"inf\")\n",
    "patience  = 500\n",
    "counter   = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    y_pred    = model(X_t)\n",
    "    data_loss = torch.mean((y_pred - Y_t) ** 2)\n",
    "    phys_loss = physics_residual_loss(y_pred, X_phys_t)\n",
    "    loss      = data_loss + 0.25 * phys_loss\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if loss.item() < best_loss:\n",
    "        best_loss = loss.item()\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "\n",
    "    if counter >= patience:\n",
    "        break\n",
    "    if epoch % 50 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.6f}, Data Loss: {data_loss.item():.6f}, Phys Loss: {phys_loss.item():.6f}\")\n",
    "\n",
    "# Predict on train/test (in normalized space)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_train_pred_scaled = model(\n",
    "        torch.tensor(train_time_scaled, dtype=torch.float32).to(device)\n",
    "    ).cpu().numpy()\n",
    "\n",
    "    y_test_pred_scaled = model(\n",
    "        torch.tensor(test_time_scaled, dtype=torch.float32).to(device)\n",
    "    ).cpu().numpy()\n",
    "\n",
    "# Remove PINN scaler → back to GLOBAL-NORMALIZED space\n",
    "y_train_pred_norm = scaler_Y.inverse_transform(y_train_pred_scaled).flatten()\n",
    "y_test_pred_norm  = scaler_Y.inverse_transform(y_test_pred_scaled).flatten()\n",
    "\n",
    "train_overall[\"pinn_pred_norm\"] = y_train_pred_norm\n",
    "test_overall[\"pinn_pred_norm\"]  = y_test_pred_norm\n",
    "\n",
    "\n",
    "# PINN MAPE in NORMALIZED SPACE\n",
    "df_pinn_test = test_overall[[\"y_norm\", \"pinn_pred_norm\"]].copy()\n",
    "\n",
    "mape_pinn_norm = mean_absolute_percentage_error(\n",
    "    df_pinn_test[\"y_norm\"], df_pinn_test[\"pinn_pred_norm\"]\n",
    ")\n",
    "print(f\"[PINN]     MAPE (normalized): {mape_pinn_norm:.4f}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 7) HYBRID (PINN + PROPHET RESIDUAL) — NORMALIZED\n",
    "# ============================================================\n",
    "train_overall[\"residual_norm\"] = train_overall[\"y_norm\"] - train_overall[\"pinn_pred_norm\"]\n",
    "test_overall[\"residual_norm\"]  = test_overall[\"y_norm\"] - test_overall[\"pinn_pred_norm\"]\n",
    "\n",
    "train_res = train_overall[[\"ds\", \"residual_norm\"]].rename(columns={\"residual_norm\": \"y\"})\n",
    "test_res  = test_overall[[\"ds\", \"residual_norm\"]].rename(columns={\"residual_norm\": \"y\"})\n",
    "\n",
    "m_res = Prophet(\n",
    "    seasonality_mode=\"multiplicative\",\n",
    "    yearly_seasonality=True,\n",
    "    weekly_seasonality=False,\n",
    "    daily_seasonality=False,\n",
    "    changepoint_prior_scale=0.1,\n",
    ")\n",
    "\n",
    "m_res.fit(train_res)\n",
    "\n",
    "future_res = m_res.make_future_dataframe(periods=len(test_res), freq=\"MS\")\n",
    "forecast_res = m_res.predict(future_res)\n",
    "\n",
    "df_res_test = (\n",
    "    forecast_res[[\"ds\", \"yhat\"]]\n",
    "    .merge(test_res[[\"ds\", \"y\"]], on=\"ds\", how=\"inner\")\n",
    "    .rename(columns={\"yhat\": \"res_pred_norm\"})\n",
    ")\n",
    "\n",
    "df_hybrid_test = (\n",
    "    test_overall[[\"ds\", \"y_norm\", \"pinn_pred_norm\"]]\n",
    "    .merge(df_res_test[[\"ds\", \"res_pred_norm\"]], on=\"ds\", how=\"left\")\n",
    ")\n",
    "\n",
    "df_hybrid_test[\"final_pred_norm\"] = (\n",
    "    df_hybrid_test[\"pinn_pred_norm\"] + df_hybrid_test[\"res_pred_norm\"]\n",
    ")\n",
    "\n",
    "# Hybrid MAPE in normalized space\n",
    "mape_hybrid_norm = mean_absolute_percentage_error(\n",
    "    df_hybrid_test[\"y_norm\"], df_hybrid_test[\"final_pred_norm\"]\n",
    ")\n",
    "\n",
    "print(f\"[Hybrid]  MAPE (normalized): {mape_hybrid_norm:.4f}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 8) CONVERT BACK TO ORIGINAL UNITS (for plots/output)\n",
    "# ============================================================\n",
    "test_overall[\"pinn_pred_orig\"] = scaler_y_global.inverse_transform(\n",
    "    test_overall[[\"pinn_pred_norm\"]]\n",
    ").flatten()\n",
    "\n",
    "df_hybrid_test[\"final_pred_orig\"] = scaler_y_global.inverse_transform(\n",
    "    df_hybrid_test[[\"final_pred_norm\"]]\n",
    ").flatten()\n",
    "\n",
    "df_hybrid_test[\"y_orig\"] = scaler_y_global.inverse_transform(\n",
    "    df_hybrid_test[[\"y_norm\"]]\n",
    ").flatten()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 9) FINAL SUMMARY\n",
    "# ============================================================\n",
    "print(\"\\n==================== FINAL RESULTS ====================\")\n",
    "print(f\"Prophet-only MAPE (normalized) : {mape_prophet_norm:.4f}\")\n",
    "print(f\"PINN-only MAPE (normalized)    : {mape_pinn_norm:.4f}\")\n",
    "print(f\"Hybrid MAPE (normalized)       : {mape_hybrid_norm:.4f}\")\n",
    "print(\"=======================================================\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdffd24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 0) IMPORTS & CONFIG\n",
    "# ============================================================\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "\n",
    "RANDOM_SEED   = 42\n",
    "DATE_COL      = \"datetime\"\n",
    "TARGET_COL    = \"Scope1_per_unit\"\n",
    "PLANT_COL     = \"Plant Name\"\n",
    "TEST_MONTHS   = 7\n",
    "PHYS_COL      = \"Electricity Grid Energy Per Unit (GJ)\"  # optional physics feature\n",
    "TargetCol_raw= \"Scope1\"\n",
    "ProductionCol= \"Production Actual Quantity (MT/Month)\"\n",
    "\n",
    "# ============================================================\n",
    "# 0) NEW: AUGMENTATION PARAMETERS\n",
    "# ============================================================\n",
    "NUM_AUGMENTED_SAMPLES = 4  \n",
    "JITTER_STD_MULTIPLIER = 0.5  # sigma = 0.05 * std(residuals)\n",
    "DECOMPOSITION_PERIOD = 12  \n",
    "\n",
    "# ============================================================\n",
    "# 1) SEEDING\n",
    "# ============================================================\n",
    "def set_seed(seed: int = 42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(RANDOM_SEED)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) DATA PREP FUNCTIONS\n",
    "# ============================================================\n",
    "def ensure_datetime_column(df: pd.DataFrame,\n",
    "                           date_col: str = DATE_COL) -> pd.DataFrame:\n",
    "    \"\"\"Ensure a monthly datetime column 'ds' exists.\"\"\"\n",
    "    if date_col in df.columns:\n",
    "        ds = pd.to_datetime(df[date_col], errors=\"coerce\", infer_datetime_format=True)\n",
    "    else:\n",
    "        ds = pd.Series(pd.NaT, index=df.index)\n",
    "\n",
    "    needs_rebuild = ds.isna()\n",
    "    if needs_rebuild.any():\n",
    "        if not {\"year\", \"month\"}.issubset(df.columns):\n",
    "            raise ValueError(\n",
    "                \"No valid datetime and missing 'year'/'month' to rebuild dates.\"\n",
    "            )\n",
    "        ds_rebuilt = pd.to_datetime(\n",
    "            dict(\n",
    "                year=df.loc[needs_rebuild, \"year\"],\n",
    "                month=df.loc[needs_rebuild, \"month\"],\n",
    "                day=1,\n",
    "            )\n",
    "        )\n",
    "        ds.loc[needs_rebuild] = ds_rebuilt\n",
    "\n",
    "    df[\"ds\"] = pd.to_datetime(ds).dt.to_period(\"M\").dt.to_timestamp()\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_and_impute(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Drop duplicates and impute numeric columns (0→NaN→mean).\"\"\"\n",
    "    if {\"ds\", PLANT_COL, TARGET_COL}.issubset(df.columns):\n",
    "        df = df.drop_duplicates(subset=[\"ds\", PLANT_COL, TARGET_COL], keep=\"last\")\n",
    "\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    def impute_zero_nan_with_mean(group: pd.DataFrame) -> pd.DataFrame:\n",
    "        g = group.copy()\n",
    "        for c in num_cols:\n",
    "            g[c] = g[c].replace(0, np.nan)\n",
    "            if g[c].notna().any():\n",
    "                g[c] = g[c].fillna(g[c].mean())\n",
    "        return g\n",
    "\n",
    "    if \"year\" in df.columns:\n",
    "        df = df.groupby(\"year\", group_keys=False).apply(impute_zero_nan_with_mean)\n",
    "    else:\n",
    "        df = impute_zero_nan_with_mean(df)\n",
    "\n",
    "    for c in num_cols:\n",
    "        if df[c].isna().any():\n",
    "            df[c] = df[c].fillna(df[c].mean())\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def prepare_overall_series2(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregate ALL numeric columns across plants into a single monthly series.\n",
    "    For each month (ds), returns the sum of every numeric column.\n",
    "    \"\"\"\n",
    "\n",
    "    # Select numeric columns only (avoid summing strings)\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    # Ensure 'ds' is not dropped accidentally\n",
    "    if \"ds\" not in df.columns:\n",
    "        raise ValueError(\"The dataframe must contain a 'ds' datetime column.\")\n",
    "\n",
    "    # Group by month (ds) and sum ALL numeric columns\n",
    "    agg = (\n",
    "        df.groupby(\"ds\", as_index=False)[numeric_cols]\n",
    "          .sum()\n",
    "          .sort_values(\"ds\")\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    return agg\n",
    "\n",
    "# ============================================================\n",
    "# NEW SECTION: DECOMPOSITION-AWARE AUGMENTATION FUNCTIONS\n",
    "# ============================================================\n",
    "def decompose_time_series(series: pd.Series, \n",
    "                          period: int = DECOMPOSITION_PERIOD) -> dict:\n",
    "    \n",
    "    # Ensure we have enough data for decomposition\n",
    "    if len(series) < 2 * period:\n",
    "        print(f\"Warning: Series length ({len(series)}) < 2*period ({2*period}). \"\n",
    "              f\"Adjusting period to {len(series)//2}.\")\n",
    "        period = max(2, len(series) // 2)\n",
    "    \n",
    "    try:\n",
    "        decomposition = seasonal_decompose(\n",
    "            series, \n",
    "            model='additive', \n",
    "            period=period,\n",
    "            \n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'trend': decomposition.trend.fillna(method=\"bfill\").fillna(method=\"ffill\").values,\n",
    "            'seasonal': decomposition.seasonal.values,\n",
    "            'residual': decomposition.resid.fillna(0).values,\n",
    "            'period': period\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Decomposition failed: {e}. Returning zero components.\")\n",
    "        return {\n",
    "            'trend': series.values,\n",
    "            'seasonal': np.zeros_like(series.values),\n",
    "            'residual': np.zeros_like(series.values),\n",
    "            'period': period\n",
    "        }\n",
    "\n",
    "\n",
    "def apply_jittering(residuals: np.ndarray, \n",
    "                   std_multiplier: float = JITTER_STD_MULTIPLIER) -> np.ndarray:\n",
    "\n",
    "    residual_std = np.std(residuals[~np.isnan(residuals)])\n",
    "    if residual_std == 0:\n",
    "        residual_std = 1.0\n",
    "    \n",
    "    noise_std = std_multiplier * residual_std\n",
    "    jitter = np.random.normal(0, noise_std, size=len(residuals))\n",
    "    \n",
    "    return residuals + jitter\n",
    "\n",
    "\n",
    "def create_augmented_samples(series: pd.Series,\n",
    "                             num_samples: int = NUM_AUGMENTED_SAMPLES,\n",
    "                             decomposition_dict: dict = None, std_multiplier: float = JITTER_STD_MULTIPLIER) -> list:\n",
    "    \"\"\"\n",
    "    Create augmented time series by:\n",
    "    1. Decomposing into trend, seasonal, residual\n",
    "    2. Jittering the residual component\n",
    "    3. Reconstructing: T + S + (R + jitter)\n",
    "    \"\"\"\n",
    "    if decomposition_dict is None:\n",
    "        decomposition_dict = decompose_time_series(series)\n",
    "    \n",
    "    trend = decomposition_dict['trend']\n",
    "    seasonal = decomposition_dict['seasonal']\n",
    "    residual = decomposition_dict['residual']\n",
    "    \n",
    "    augmented_samples = []\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Apply jittering to residuals\n",
    "        jittered_residual = apply_jittering(residual.copy(), std_multiplier=JITTER_STD_MULTIPLIER)\n",
    "\n",
    "        # Reconstruct: T + S + (R + jitter)\n",
    "        augmented_series = trend + seasonal + jittered_residual\n",
    "        augmented_samples.append(augmented_series)\n",
    "    \n",
    "    return augmented_samples\n",
    "\n",
    "\n",
    "def visualize_decomposition(series: pd.Series, \n",
    "                           decomposition_dict: dict,\n",
    "                           title: str = \"Time Series Decomposition\") -> None:\n",
    "    \"\"\"\n",
    "    Visualize the decomposition of a time series.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(4, 1, figsize=(12, 10))\n",
    "    \n",
    "    # Original series\n",
    "    axes[0].plot(series.index, series.values, 'b-', linewidth=1.5)\n",
    "    axes[0].set_ylabel('Original')\n",
    "    axes[0].set_title(title)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Trend\n",
    "    axes[1].plot(series.index, decomposition_dict['trend'], 'g-', linewidth=1.5)\n",
    "    axes[1].set_ylabel('Trend')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Seasonal\n",
    "    axes[2].plot(series.index, decomposition_dict['seasonal'], 'orange', linewidth=1.5)\n",
    "    axes[2].set_ylabel('Seasonal')\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Residual\n",
    "    axes[3].plot(series.index, decomposition_dict['residual'], 'r-', linewidth=1.5)\n",
    "    axes[3].set_ylabel('Residual')\n",
    "    axes[3].set_xlabel('Time Index')\n",
    "    axes[3].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_augmented_samples(original_series: pd.Series,\n",
    "                               augmented_samples: list,\n",
    "                               title: str = \"Original vs Augmented Series\") -> None:\n",
    "    \"\"\"\n",
    "    Visualize original series and augmented versions.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot original\n",
    "    plt.plot(original_series.index, original_series.values, \n",
    "             'b-', linewidth=2.5, label='Original', alpha=0.8)\n",
    "    \n",
    "    # Plot augmented samples\n",
    "    colors = ['red', 'green', 'orange', 'purple', 'brown']\n",
    "    for idx, aug in enumerate(augmented_samples):\n",
    "        plt.plot(original_series.index, aug, \n",
    "                linewidth=1, alpha=0.5, \n",
    "                label=f'Augmented {idx+1}',\n",
    "                color=colors[idx % len(colors)])\n",
    "    \n",
    "    plt.xlabel('Time Index')\n",
    "    plt.ylabel('Normalized Value')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4) MAIN PIPELINE\n",
    "# ============================================================\n",
    "df_raw = df_2022.copy()\n",
    "\n",
    "df = ensure_datetime_column(df_raw, date_col=DATE_COL)\n",
    "df = clean_and_impute(df)\n",
    "\n",
    "overall = prepare_overall_series2(df)\n",
    "overall = overall.sort_values(\"ds\").reset_index(drop=True)\n",
    "overall = overall.rename(columns={TARGET_COL: \"y\"})\n",
    "# --- GLOBAL SCALING (Prophet + Hybrid + Final Outputs) ---\n",
    "scaler_y_global = StandardScaler()\n",
    "overall[\"y_norm\"] = scaler_y_global.fit_transform(overall[[\"y\"]])\n",
    "\n",
    "# Train/test split\n",
    "split_point = len(overall) - TEST_MONTHS\n",
    "train_overall = overall.iloc[:split_point].copy()\n",
    "test_overall  = overall.iloc[split_point:].copy()\n",
    "\n",
    "print(\"Train:\", len(train_overall), \"Test:\", len(test_overall))\n",
    "\n",
    "# ============================================================\n",
    "# PHASE 2: DECOMPOSITION-AWARE AUGMENTATION\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PHASE 2: DECOMPOSITION-AWARE AUGMENTATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Step 1: Decompose the original training series\n",
    "original_train_series = pd.Series(\n",
    "    train_overall['y_norm'].values,\n",
    "    index=range(len(train_overall))\n",
    ")\n",
    "\n",
    "decomposition_dict = decompose_time_series(\n",
    "    original_train_series,\n",
    "    period=DECOMPOSITION_PERIOD\n",
    ")\n",
    "\n",
    "print(f\"\\n[DECOMPOSITION] Seasonal period: {decomposition_dict['period']}\")\n",
    "print(f\"[DECOMPOSITION] Trend shape: {decomposition_dict['trend'].shape}\")\n",
    "print(f\"[DECOMPOSITION] Seasonal shape: {decomposition_dict['seasonal'].shape}\")\n",
    "print(f\"[DECOMPOSITION] Residual shape: {decomposition_dict['residual'].shape}\")\n",
    "\n",
    "# Visualize decomposition (optional - comment out if not needed)\n",
    "visualize_decomposition(original_train_series, decomposition_dict, \n",
    "                       title=\"Training Data Decomposition\")\n",
    "\n",
    "# Step 2: Create augmented samples\n",
    "augmented_samples = create_augmented_samples(\n",
    "    original_train_series,\n",
    "    num_samples=NUM_AUGMENTED_SAMPLES,\n",
    "    decomposition_dict=decomposition_dict,\n",
    "    std_multiplier=JITTER_STD_MULTIPLIER\n",
    ")\n",
    "\n",
    "print(f\"\\n[AUGMENTATION] Created {len(augmented_samples)} augmented samples\")\n",
    "print(f\"[AUGMENTATION] Jitter std multiplier: {JITTER_STD_MULTIPLIER}\")\n",
    "\n",
    "# Visualize augmentation (optional - comment out if not needed)\n",
    "visualize_augmented_samples(original_train_series, augmented_samples,\n",
    "                           title=\"Original vs Augmented Training Samples\")\n",
    "\n",
    "# Step 3: Prepare augmented training dataframe for Prophet\n",
    "train_prophet_augmented = train_overall[[\"ds\", \"y_norm\"]].rename(\n",
    "    columns={\"y_norm\": \"y\"}\n",
    ").copy()\n",
    "\n",
    "# Create additional augmented dataframes\n",
    "for idx, aug_values in enumerate(augmented_samples):\n",
    "    aug_df = train_overall[[\"ds\"]].copy()\n",
    "    aug_df[\"y\"] = aug_values\n",
    "    train_prophet_augmented = pd.concat(\n",
    "        [train_prophet_augmented, aug_df],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "# Sort by ds for Prophet (important for time series)\n",
    "train_prophet_augmented = train_prophet_augmented.sort_values(\"ds\").reset_index(drop=True)\n",
    "\n",
    "print(f\"\\n[PROPHET TRAINING DATA]\")\n",
    "print(f\"Original: {len(train_overall)}\")\n",
    "print(f\"Augmented: {len(train_prophet_augmented)}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5) PROPHET (NORMALIZED)\n",
    "# ============================================================\n",
    "train_prophet = train_overall[[\"ds\", \"y_norm\"]].rename(columns={\"y_norm\": \"y\"})\n",
    "test_prophet  = test_overall[[\"ds\", \"y_norm\"]].rename(columns={\"y_norm\": \"y\"})\n",
    "\n",
    "m_overall = Prophet(\n",
    "    seasonality_mode=\"multiplicative\",\n",
    "    yearly_seasonality=True,\n",
    "    weekly_seasonality=False,\n",
    "    daily_seasonality=False,\n",
    "    changepoint_prior_scale=0.01,\n",
    ")\n",
    "m_overall.fit(train_prophet)\n",
    "\n",
    "\n",
    "future_all = m_overall.make_future_dataframe(periods=len(test_prophet), freq=\"MS\")\n",
    "fcst_all   = m_overall.predict(future_all)\n",
    "\n",
    "df_prophet_test = (\n",
    "    fcst_all[[\"ds\", \"yhat\"]]\n",
    "    .merge(test_prophet[[\"ds\", \"y\"]], on=\"ds\", how=\"inner\")\n",
    "    .sort_values(\"ds\")\n",
    ")\n",
    "\n",
    "# MAPE in NORMALIZED SPACE\n",
    "mape_prophet_norm = mean_absolute_percentage_error(\n",
    "    df_prophet_test[\"y\"], df_prophet_test[\"yhat\"]\n",
    ")\n",
    "\n",
    "test_overall[\"prophet_pred_norm\"] = scaler_y_global.inverse_transform(df_prophet_test[\"yhat\"].values .reshape(-1, 1)).flatten()\n",
    "print(\"Norm Train: \",len(train_prophet))\n",
    "print(\"Aug futures: \",len(fcst_all))\n",
    "\n",
    "print(f\"[Prophet]  MAPE (normalized): {mape_prophet_norm:.4f}\")\n",
    "\n",
    "# ============================================================\n",
    "m_overall = Prophet(\n",
    "    seasonality_mode=\"additive\",\n",
    "    yearly_seasonality=True,\n",
    "    weekly_seasonality=False,\n",
    "    daily_seasonality=False,\n",
    "    changepoint_prior_scale=0.01,\n",
    ")\n",
    "m_overall.fit(train_prophet_augmented)\n",
    "print(\"Augmented Train: \",len(train_prophet_augmented))\n",
    "future_all = m_overall.make_future_dataframe(periods=len(test_prophet), freq=\"MS\")\n",
    "fcst_all   = m_overall.predict(future_all)\n",
    "print(\"Aug futures: \",len(fcst_all))\n",
    "df_prophet_test_aug = (\n",
    "    fcst_all[[\"ds\", \"yhat\"]]\n",
    "    .merge(test_prophet[[\"ds\", \"y\"]], on=\"ds\", how=\"inner\")\n",
    "    .sort_values(\"ds\")\n",
    ")\n",
    "\n",
    "# MAPE in NORMALIZED SPACE\n",
    "mape_prophet_norm = mean_absolute_percentage_error(\n",
    "    df_prophet_test_aug[\"y\"], df_prophet_test_aug[\"yhat\"]\n",
    ")\n",
    "print(f\"[Prophet]  MAPE (aug): {mape_prophet_norm:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a97472e",
   "metadata": {},
   "source": [
    "## STL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2043c031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 0) IMPORTS & CONFIG\n",
    "# ============================================================\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "\n",
    "RANDOM_SEED   = 42\n",
    "DATE_COL      = \"datetime\"\n",
    "TARGET_COL    = \"Scope1_per_unit\"\n",
    "PLANT_COL     = \"Plant Name\"\n",
    "TEST_MONTHS   = 7\n",
    "PHYS_COL      = \"Electricity Grid Energy Per Unit (GJ)\"  # optional physics feature\n",
    "TargetCol_raw= \"Scope1\"\n",
    "ProductionCol= \"Production Actual Quantity (MT/Month)\"\n",
    "\n",
    "# ============================================================\n",
    "# 0) NEW: AUGMENTATION PARAMETERS\n",
    "# ============================================================\n",
    "NUM_AUGMENTED_SAMPLES = 4  \n",
    "JITTER_STD_MULTIPLIER = 0.5  # sigma = 0.05 * std(residuals)\n",
    "DECOMPOSITION_PERIOD = 8\n",
    "\n",
    "# ============================================================\n",
    "# 1) SEEDING\n",
    "# ============================================================\n",
    "def set_seed(seed: int = 42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(RANDOM_SEED)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) DATA PREP FUNCTIONS\n",
    "# ============================================================\n",
    "def ensure_datetime_column(df: pd.DataFrame,\n",
    "                           date_col: str = DATE_COL) -> pd.DataFrame:\n",
    "    \"\"\"Ensure a monthly datetime column 'ds' exists.\"\"\"\n",
    "    if date_col in df.columns:\n",
    "        ds = pd.to_datetime(df[date_col], errors=\"coerce\", infer_datetime_format=True)\n",
    "    else:\n",
    "        ds = pd.Series(pd.NaT, index=df.index)\n",
    "\n",
    "    needs_rebuild = ds.isna()\n",
    "    if needs_rebuild.any():\n",
    "        if not {\"year\", \"month\"}.issubset(df.columns):\n",
    "            raise ValueError(\n",
    "                \"No valid datetime and missing 'year'/'month' to rebuild dates.\"\n",
    "            )\n",
    "        ds_rebuilt = pd.to_datetime(\n",
    "            dict(\n",
    "                year=df.loc[needs_rebuild, \"year\"],\n",
    "                month=df.loc[needs_rebuild, \"month\"],\n",
    "                day=1,\n",
    "            )\n",
    "        )\n",
    "        ds.loc[needs_rebuild] = ds_rebuilt\n",
    "\n",
    "    df[\"ds\"] = pd.to_datetime(ds).dt.to_period(\"M\").dt.to_timestamp()\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_and_impute(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Drop duplicates and impute numeric columns (0→NaN→mean).\"\"\"\n",
    "    if {\"ds\", PLANT_COL, TARGET_COL}.issubset(df.columns):\n",
    "        df = df.drop_duplicates(subset=[\"ds\", PLANT_COL, TARGET_COL], keep=\"last\")\n",
    "\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    def impute_zero_nan_with_mean(group: pd.DataFrame) -> pd.DataFrame:\n",
    "        g = group.copy()\n",
    "        for c in num_cols:\n",
    "            g[c] = g[c].replace(0, np.nan)\n",
    "            if g[c].notna().any():\n",
    "                g[c] = g[c].fillna(g[c].mean())\n",
    "        return g\n",
    "\n",
    "    if \"year\" in df.columns:\n",
    "        df = df.groupby(\"year\", group_keys=False).apply(impute_zero_nan_with_mean)\n",
    "    else:\n",
    "        df = impute_zero_nan_with_mean(df)\n",
    "\n",
    "    for c in num_cols:\n",
    "        if df[c].isna().any():\n",
    "            df[c] = df[c].fillna(df[c].mean())\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def prepare_overall_series2(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregate ALL numeric columns across plants into a single monthly series.\n",
    "    For each month (ds), returns the sum of every numeric column.\n",
    "    \"\"\"\n",
    "\n",
    "    # Select numeric columns only (avoid summing strings)\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    # Ensure 'ds' is not dropped accidentally\n",
    "    if \"ds\" not in df.columns:\n",
    "        raise ValueError(\"The dataframe must contain a 'ds' datetime column.\")\n",
    "\n",
    "    # Group by month (ds) and sum ALL numeric columns\n",
    "    agg = (\n",
    "        df.groupby(\"ds\", as_index=False)[numeric_cols]\n",
    "          .sum()\n",
    "          .sort_values(\"ds\")\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    return agg\n",
    "\n",
    "# ============================================================\n",
    "# NEW SECTION: DECOMPOSITION-AWARE AUGMENTATION FUNCTIONS\n",
    "# ============================================================\n",
    "def decompose_time_series(series: pd.Series, \n",
    "                          period: int = DECOMPOSITION_PERIOD) -> dict:\n",
    "    # Ensure we have enough data for decomposition\n",
    "    if len(series) < 2 * period:\n",
    "        print(f\"Warning: Series length ({len(series)}) < 2*period ({2*period}). \"\n",
    "              f\"Adjusting period to {len(series)//2}.\")\n",
    "        period = max(2, len(series) // 2)\n",
    "    \n",
    "    try:\n",
    "        decomposition = seasonal_decompose(\n",
    "            series, \n",
    "            model='additive', \n",
    "            period=period,\n",
    "            \n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'trend': decomposition.trend.fillna(method=\"bfill\").fillna(method=\"ffill\").values,\n",
    "            'seasonal': decomposition.seasonal.values,\n",
    "            'residual': decomposition.resid.fillna(0).values,\n",
    "            'period': period\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Decomposition failed: {e}. Returning zero components.\")\n",
    "        return {\n",
    "            'trend': series.values,\n",
    "            'seasonal': np.zeros_like(series.values),\n",
    "            'residual': np.zeros_like(series.values),\n",
    "            'period': period\n",
    "        }\n",
    "\n",
    "\n",
    "def apply_jittering(residuals: np.ndarray, \n",
    "                   std_multiplier: float = JITTER_STD_MULTIPLIER) -> np.ndarray:\n",
    "\n",
    "    residual_std = np.std(residuals[~np.isnan(residuals)])\n",
    "    if residual_std == 0:\n",
    "        residual_std = 1.0\n",
    "    \n",
    "    noise_std = std_multiplier * residual_std\n",
    "    jitter = np.random.normal(0, noise_std, size=len(residuals))\n",
    "    \n",
    "    return residuals + jitter\n",
    "\n",
    "\n",
    "def create_augmented_samples(series: pd.Series,\n",
    "                             num_samples: int = NUM_AUGMENTED_SAMPLES,\n",
    "                             decomposition_dict: dict = None, std_multiplier: float = JITTER_STD_MULTIPLIER) -> list:\n",
    "    \"\"\"\n",
    "    Create augmented time series by:\n",
    "    1. Decomposing into trend, seasonal, residual\n",
    "    2. Jittering the residual component\n",
    "    3. Reconstructing: T + S + (R + jitter)\n",
    "    \"\"\"\n",
    "    if decomposition_dict is None:\n",
    "        decomposition_dict = decompose_time_series_stl(series)\n",
    "    \n",
    "    trend = decomposition_dict['trend']\n",
    "    seasonal = decomposition_dict['seasonal']\n",
    "    residual = decomposition_dict['residual']\n",
    "    \n",
    "    augmented_samples = []\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Apply jittering to residuals\n",
    "        jittered_residual = apply_jittering(residual.copy(), std_multiplier=JITTER_STD_MULTIPLIER)\n",
    "\n",
    "        # Reconstruct: T + S + (R + jitter)\n",
    "        augmented_series = trend + seasonal + jittered_residual\n",
    "        augmented_samples.append(augmented_series)\n",
    "    \n",
    "    return augmented_samples\n",
    "\n",
    "\n",
    "def visualize_decomposition(series: pd.Series, \n",
    "                           decomposition_dict: dict,\n",
    "                           title: str = \"Time Series Decomposition\") -> None:\n",
    "    \"\"\"\n",
    "    Visualize the decomposition of a time series.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(4, 1, figsize=(12, 10))\n",
    "    \n",
    "    # Original series\n",
    "    axes[0].plot(series.index, series.values, 'b-', linewidth=1.5)\n",
    "    axes[0].set_ylabel('Original')\n",
    "    axes[0].set_title(title)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Trend\n",
    "    axes[1].plot(series.index, decomposition_dict['trend'], 'g-', linewidth=1.5)\n",
    "    axes[1].set_ylabel('Trend')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Seasonal\n",
    "    axes[2].plot(series.index, decomposition_dict['seasonal'], 'orange', linewidth=1.5)\n",
    "    axes[2].set_ylabel('Seasonal')\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Residual\n",
    "    axes[3].plot(series.index, decomposition_dict['residual'], 'r-', linewidth=1.5)\n",
    "    axes[3].set_ylabel('Residual')\n",
    "    axes[3].set_xlabel('Time Index')\n",
    "    axes[3].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_augmented_samples(original_series: pd.Series,\n",
    "                               augmented_samples: list,\n",
    "                               title: str = \"Original vs Augmented Series\") -> None:\n",
    "    \"\"\"\n",
    "    Visualize original series and augmented versions.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot original\n",
    "    plt.plot(original_series.index, original_series.values, \n",
    "             'b-', linewidth=2.5, label='Original', alpha=0.8)\n",
    "    \n",
    "    # Plot augmented samples\n",
    "    colors = ['red', 'green', 'orange', 'purple', 'brown']\n",
    "    for idx, aug in enumerate(augmented_samples):\n",
    "        plt.plot(original_series.index, aug, \n",
    "                linewidth=1, alpha=0.5, \n",
    "                label=f'Augmented {idx+1}',\n",
    "                color=colors[idx % len(colors)])\n",
    "    \n",
    "    plt.xlabel('Time Index')\n",
    "    plt.ylabel('Normalized Value')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "#STL \n",
    "# ============================================================\n",
    "# STL (LOESS) DECOMPOSITION\n",
    "# ============================================================\n",
    "def decompose_time_series_stl(\n",
    "    series: pd.Series,\n",
    "    period: int = DECOMPOSITION_PERIOD,\n",
    "    robust: bool = True\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Decompose a time series using STL (LOESS-based).\n",
    "    Returns trend, seasonal, residual components.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure enough data\n",
    "    if len(series) < 2 * period:\n",
    "        print(\n",
    "            f\"Warning: Series length ({len(series)}) < 2*period ({2*period}). \"\n",
    "            f\"Adjusting period to {len(series)//2}.\"\n",
    "        )\n",
    "        period = max(2, len(series) // 2)\n",
    "\n",
    "    try:\n",
    "        stl = STL(\n",
    "            series,\n",
    "            period=period,\n",
    "            robust=robust\n",
    "        )\n",
    "        result = stl.fit()\n",
    "\n",
    "        trend = result.trend\n",
    "        seasonal = result.seasonal\n",
    "        resid = result.resid\n",
    "\n",
    "        # Safety: fill NaNs\n",
    "        trend = trend.fillna(method=\"bfill\").fillna(method=\"ffill\")\n",
    "        resid = resid.fillna(0.0)\n",
    "\n",
    "        return {\n",
    "            \"trend\": trend.values,\n",
    "            \"seasonal\": seasonal.values,\n",
    "            \"residual\": resid.values,\n",
    "            \"period\": period,\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[STL] Decomposition failed: {e}. Falling back to identity.\")\n",
    "        return {\n",
    "            \"trend\": series.values,\n",
    "            \"seasonal\": np.zeros_like(series.values),\n",
    "            \"residual\": np.zeros_like(series.values),\n",
    "            \"period\": period,\n",
    "        }\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4) MAIN PIPELINE\n",
    "# ============================================================\n",
    "df_raw = df_2022.copy()\n",
    "\n",
    "df = ensure_datetime_column(df_raw, date_col=DATE_COL)\n",
    "df = clean_and_impute(df)\n",
    "\n",
    "overall = prepare_overall_series2(df)\n",
    "overall = overall.sort_values(\"ds\").reset_index(drop=True)\n",
    "overall = overall.rename(columns={TARGET_COL: \"y\"})\n",
    "# --- GLOBAL SCALING (Prophet + Hybrid + Final Outputs) ---\n",
    "scaler_y_global = StandardScaler()\n",
    "overall[\"y_norm\"] = scaler_y_global.fit_transform(overall[[\"y\"]])\n",
    "\n",
    "# Train/test split\n",
    "split_point = len(overall) - TEST_MONTHS\n",
    "train_overall = overall.iloc[:split_point].copy()\n",
    "test_overall  = overall.iloc[split_point:].copy()\n",
    "\n",
    "print(\"Train:\", len(train_overall), \"Test:\", len(test_overall))\n",
    "\n",
    "# ============================================================\n",
    "# PHASE 2: DECOMPOSITION-AWARE AUGMENTATION\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PHASE 2: DECOMPOSITION-AWARE AUGMENTATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Step 1: Decompose the original training series\n",
    "original_train_series = pd.Series(\n",
    "    train_overall['y_norm'].values,\n",
    "    index=range(len(train_overall))\n",
    ")\n",
    "\n",
    "decomposition_dict = decompose_time_series_stl(\n",
    "    original_train_series,\n",
    "    period=DECOMPOSITION_PERIOD,\n",
    "    robust=True\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"\\n[DECOMPOSITION] Seasonal period: {decomposition_dict['period']}\")\n",
    "print(f\"[DECOMPOSITION] Trend shape: {decomposition_dict['trend'].shape}\")\n",
    "print(f\"[DECOMPOSITION] Seasonal shape: {decomposition_dict['seasonal'].shape}\")\n",
    "print(f\"[DECOMPOSITION] Residual shape: {decomposition_dict['residual'].shape}\")\n",
    "\n",
    "# Visualize decomposition (optional - comment out if not needed)\n",
    "visualize_decomposition(original_train_series, decomposition_dict, \n",
    "                       title=\"Training Data Decomposition\")\n",
    "\n",
    "# Step 2: Create augmented samples\n",
    "augmented_samples = create_augmented_samples(\n",
    "    original_train_series,\n",
    "    num_samples=NUM_AUGMENTED_SAMPLES,\n",
    "    decomposition_dict=decomposition_dict,\n",
    "    std_multiplier=JITTER_STD_MULTIPLIER\n",
    ")\n",
    "\n",
    "print(f\"\\n[AUGMENTATION] Created {len(augmented_samples)} augmented samples\")\n",
    "print(f\"[AUGMENTATION] Jitter std multiplier: {JITTER_STD_MULTIPLIER}\")\n",
    "\n",
    "# Visualize augmentation (optional - comment out if not needed)\n",
    "visualize_augmented_samples(original_train_series, augmented_samples,\n",
    "                           title=\"Original vs Augmented Training Samples\")\n",
    "\n",
    "# Step 3: Prepare augmented training dataframe for Prophet\n",
    "train_prophet_augmented = train_overall[[\"ds\", \"y_norm\"]].rename(\n",
    "    columns={\"y_norm\": \"y\"}\n",
    ").copy()\n",
    "\n",
    "# Create additional augmented dataframes\n",
    "for idx, aug_values in enumerate(augmented_samples):\n",
    "    aug_df = train_overall[[\"ds\"]].copy()\n",
    "    aug_df[\"y\"] = aug_values\n",
    "    train_prophet_augmented = pd.concat(\n",
    "        [train_prophet_augmented, aug_df],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "# Sort by ds for Prophet (important for time series)\n",
    "train_prophet_augmented = train_prophet_augmented.sort_values(\"ds\").reset_index(drop=True)\n",
    "\n",
    "print(f\"\\n[PROPHET TRAINING DATA]\")\n",
    "print(f\"Original: {len(train_overall)}\")\n",
    "print(f\"Augmented: {len(train_prophet_augmented)}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5) PROPHET (NORMALIZED)\n",
    "# ============================================================\n",
    "train_prophet = train_overall[[\"ds\", \"y_norm\"]].rename(columns={\"y_norm\": \"y\"})\n",
    "test_prophet  = test_overall[[\"ds\", \"y_norm\"]].rename(columns={\"y_norm\": \"y\"})\n",
    "\n",
    "m_overall = Prophet(\n",
    "    seasonality_mode=\"multiplicative\",\n",
    "    yearly_seasonality=True,\n",
    "    weekly_seasonality=False,\n",
    "    daily_seasonality=False,\n",
    "    changepoint_prior_scale=0.01,\n",
    ")\n",
    "m_overall.fit(train_prophet)\n",
    "\n",
    "\n",
    "future_all = m_overall.make_future_dataframe(periods=len(test_prophet), freq=\"MS\")\n",
    "fcst_all   = m_overall.predict(future_all)\n",
    "\n",
    "df_prophet_test = (\n",
    "    fcst_all[[\"ds\", \"yhat\"]]\n",
    "    .merge(test_prophet[[\"ds\", \"y\"]], on=\"ds\", how=\"inner\")\n",
    "    .sort_values(\"ds\")\n",
    ")\n",
    "\n",
    "# MAPE in NORMALIZED SPACE\n",
    "mape_prophet_norm = mean_absolute_percentage_error(\n",
    "    df_prophet_test[\"y\"], df_prophet_test[\"yhat\"]\n",
    ")\n",
    "\n",
    "test_overall[\"prophet_pred_norm\"] = scaler_y_global.inverse_transform(df_prophet_test[\"yhat\"].values .reshape(-1, 1)).flatten()\n",
    "print(\"Norm Train: \",len(train_prophet))\n",
    "print(\"Aug futures: \",len(fcst_all))\n",
    "\n",
    "print(f\"[Prophet]  MAPE (normalized): {mape_prophet_norm:.4f}\")\n",
    "\n",
    "# ============================================================\n",
    "m_overall = Prophet(\n",
    "    seasonality_mode=\"additive\",\n",
    "    yearly_seasonality=True,\n",
    "    weekly_seasonality=False,\n",
    "    daily_seasonality=False,\n",
    "    changepoint_prior_scale=0.01,\n",
    ")\n",
    "m_overall.fit(train_prophet_augmented)\n",
    "print(\"Augmented Train: \",len(train_prophet_augmented))\n",
    "future_all = m_overall.make_future_dataframe(periods=len(test_prophet), freq=\"MS\")\n",
    "fcst_all   = m_overall.predict(future_all)\n",
    "print(\"Aug futures: \",len(fcst_all))\n",
    "df_prophet_test_aug = (\n",
    "    fcst_all[[\"ds\", \"yhat\"]]\n",
    "    .merge(test_prophet[[\"ds\", \"y\"]], on=\"ds\", how=\"inner\")\n",
    "    .sort_values(\"ds\")\n",
    ")\n",
    "\n",
    "# MAPE in NORMALIZED SPACE\n",
    "mape_prophet_norm = mean_absolute_percentage_error(\n",
    "    df_prophet_test_aug[\"y\"], df_prophet_test_aug[\"yhat\"]\n",
    ")\n",
    "print(f\"[Prophet]  MAPE (aug): {mape_prophet_norm:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e872cd",
   "metadata": {},
   "source": [
    "# seasonal Moving bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba03e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 0) ADDITIONAL IMPORTS\n",
    "# ============================================================\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "import random\n",
    "\n",
    "# ============================================================\n",
    "# 1) SEASONAL MOVING BLOCK BOOTSTRAP (MBB)\n",
    "# ============================================================\n",
    "def seasonal_moving_block_bootstrap(\n",
    "    series: pd.Series,\n",
    "    block_size: int = 12,\n",
    "    num_samples: int = 4,\n",
    "    random_seed: int = 2\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    Seasonal Moving Block Bootstrap (MBB)\n",
    "\n",
    "    List[np.ndarray]\n",
    "        Bootstrapped time series samples.\n",
    "    \"\"\"\n",
    "    random_seed=2\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "\n",
    "    values = series.values\n",
    "    n = len(values)\n",
    "\n",
    "    if n < block_size:\n",
    "        raise ValueError(\"Time series shorter than block size.\")\n",
    "\n",
    "    # All possible seasonal blocks\n",
    "    blocks = []\n",
    "    for start in range(0, n - block_size + 1):\n",
    "        blocks.append(values[start : start + block_size])\n",
    "\n",
    "    blocks = np.array(blocks)\n",
    "    n_blocks_needed = int(np.ceil(n / block_size))\n",
    "\n",
    "    augmented_series_list = []\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        sampled_blocks = blocks[\n",
    "            np.random.choice(len(blocks), size=n_blocks_needed, replace=True)\n",
    "        ]\n",
    "\n",
    "        bootstrapped = np.concatenate(sampled_blocks)[:n]\n",
    "\n",
    "        augmented_series_list.append(bootstrapped)\n",
    "\n",
    "    return augmented_series_list\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) VISUALIZATION (OPTIONAL)\n",
    "# ============================================================\n",
    "def visualize_mbb(\n",
    "    original_series: pd.Series,\n",
    "    augmented_samples: list,\n",
    "    title: str = \"Seasonal Moving Block Bootstrap\"\n",
    "):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(\n",
    "        original_series.index,\n",
    "        original_series.values,\n",
    "        label=\"Original\",\n",
    "        linewidth=3,\n",
    "        alpha=0.9,\n",
    "        color=\"black\"\n",
    "    )\n",
    "\n",
    "    for i, aug in enumerate(augmented_samples):\n",
    "        plt.plot(\n",
    "            original_series.index,\n",
    "            aug,\n",
    "            linewidth=1.5,\n",
    "            alpha=0.6,\n",
    "            label=f\"MBB Sample {i+1}\"\n",
    "        )\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Time Index\")\n",
    "    plt.ylabel(\"Normalized Value\")\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3) APPLY MBB TO TRAINING DATA\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PHASE 3: SEASONAL MOVING BLOCK BOOTSTRAP (MBB)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "original_train_series = pd.Series(\n",
    "    train_overall[\"y_norm\"].values,\n",
    "    index=train_overall[\"ds\"]\n",
    ")\n",
    "\n",
    "mbb_augmented_samples = seasonal_moving_block_bootstrap(\n",
    "    series=original_train_series,\n",
    "    block_size=DECOMPOSITION_PERIOD,  # 12 months\n",
    "    num_samples=NUM_AUGMENTED_SAMPLES,\n",
    "    random_seed=RANDOM_SEED\n",
    ")\n",
    "\n",
    "print(f\"[MBB] Generated {len(mbb_augmented_samples)} bootstrapped samples\")\n",
    "\n",
    "# Optional visualization\n",
    "visualize_mbb(\n",
    "    original_series=original_train_series,\n",
    "    augmented_samples=mbb_augmented_samples,\n",
    "    title=\"Original vs Seasonal MBB-Augmented Series\"\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# 4) PREPARE PROPHET TRAINING DATA (MBB)\n",
    "# ============================================================\n",
    "train_prophet_mbb = train_overall[[\"ds\", \"y_norm\"]].rename(\n",
    "    columns={\"y_norm\": \"y\"}\n",
    ").copy()\n",
    "\n",
    "for aug in mbb_augmented_samples:\n",
    "    aug_df = train_overall[[\"ds\"]].copy()\n",
    "    aug_df[\"y\"] = aug\n",
    "    train_prophet_mbb = pd.concat(\n",
    "        [train_prophet_mbb, aug_df],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "train_prophet_mbb = (\n",
    "    train_prophet_mbb\n",
    "    .sort_values(\"ds\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(f\"[MBB] Prophet training size: {len(train_prophet_mbb)}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5) PROPHET TRAINING (MBB-AUGMENTED)\n",
    "# ============================================================\n",
    "m_mbb = Prophet(\n",
    "    seasonality_mode=\"multiplicative\",\n",
    "    yearly_seasonality=True,\n",
    "    weekly_seasonality=False,\n",
    "    daily_seasonality=False,\n",
    "    changepoint_prior_scale=0.1,\n",
    ")\n",
    "\n",
    "m_mbb.fit(train_prophet_mbb)\n",
    "\n",
    "future_mbb = m_mbb.make_future_dataframe(\n",
    "    periods=len(test_overall),\n",
    "    freq=\"MS\"\n",
    ")\n",
    "\n",
    "forecast_mbb = m_mbb.predict(future_mbb)\n",
    "\n",
    "df_prophet_test_mbb = (\n",
    "    forecast_mbb[[\"ds\", \"yhat\"]]\n",
    "    .merge(\n",
    "        test_overall[[\"ds\", \"y_norm\"]].rename(columns={\"y_norm\": \"y\"}),\n",
    "        on=\"ds\",\n",
    "        how=\"inner\"\n",
    "    )\n",
    "    .sort_values(\"ds\")\n",
    ")\n",
    "\n",
    "mape_prophet_mbb = mean_absolute_percentage_error(\n",
    "    df_prophet_test_mbb[\"y\"],\n",
    "    df_prophet_test_mbb[\"yhat\"]\n",
    ")\n",
    "\n",
    "print(f\"[Prophet + MBB] MAPE (normalized): {mape_prophet_mbb:.4f}\")\n",
    "\n",
    "# Inverse-transform predictions (original scale)\n",
    "test_overall[\"prophet_pred_mbb\"] = scaler_y_global.inverse_transform(\n",
    "    df_prophet_test_mbb[\"yhat\"].values.reshape(-1, 1)\n",
    ").flatten()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282a23b3",
   "metadata": {},
   "source": [
    "## INTERPOLATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f118932d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 0) IMPORTS\n",
    "# ============================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "from scipy.interpolate import interp1d, CubicSpline, UnivariateSpline\n",
    "from numpy.fft import fft, ifft\n",
    "\n",
    "# ============================================================\n",
    "# 1) INTERPOLATION AUGMENTATION FUNCTIONS\n",
    "# ============================================================\n",
    "def linear_interpolation_augmentation(series, scale_factors=(0.9, 1.1)):\n",
    "    x = np.arange(len(series))\n",
    "    y = series.values\n",
    "    augmented = []\n",
    "\n",
    "    for scale in scale_factors:\n",
    "        new_len = int(len(series) * scale)\n",
    "        x_new = np.linspace(0, len(series) - 1, new_len)\n",
    "        f = interp1d(x, y, kind=\"linear\")\n",
    "        y_new = f(x_new)\n",
    "        y_resampled = np.interp(x, np.linspace(0, new_len - 1, new_len), y_new)\n",
    "        augmented.append(y_resampled)\n",
    "\n",
    "    return augmented\n",
    "\n",
    "\n",
    "def cubic_interpolation_augmentation(series, scale_factors=(0.9, 1.1)):\n",
    "    x = np.arange(len(series))\n",
    "    y = series.values\n",
    "    augmented = []\n",
    "\n",
    "    for scale in scale_factors:\n",
    "        new_len = int(len(series) * scale)\n",
    "        x_new = np.linspace(0, len(series) - 1, new_len)\n",
    "        cs = CubicSpline(x, y)\n",
    "        y_new = cs(x_new)\n",
    "        y_resampled = np.interp(x, np.linspace(0, new_len - 1, new_len), y_new)\n",
    "        augmented.append(y_resampled)\n",
    "\n",
    "    return augmented\n",
    "\n",
    "\n",
    "def spline_interpolation_augmentation(series, smoothing_factors=(0.1, 0.3)):\n",
    "    x = np.arange(len(series))\n",
    "    y = series.values\n",
    "    augmented = []\n",
    "\n",
    "    for s in smoothing_factors:\n",
    "        spline = UnivariateSpline(x, y, s=s * len(series))\n",
    "        augmented.append(spline(x))\n",
    "\n",
    "    return augmented\n",
    "\n",
    "\n",
    "def fourier_interpolation_augmentation(series, keep_ratio=0.85):\n",
    "    y = series.values\n",
    "    n = len(y)\n",
    "\n",
    "    coeffs = fft(y)\n",
    "    cutoff = int(keep_ratio * n / 2)\n",
    "\n",
    "    filtered = np.zeros_like(coeffs)\n",
    "    filtered[:cutoff] = coeffs[:cutoff]\n",
    "    filtered[-cutoff:] = coeffs[-cutoff:]\n",
    "\n",
    "    return [np.real(ifft(filtered))]\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) CREATE INTERPOLATION-AUGMENTED TRAINING SET\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PHASE: PROPHET WITH INTERPOLATION AUGMENTATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "original_train_series = pd.Series(\n",
    "    train_overall[\"y_norm\"].values,\n",
    "    index=train_overall[\"ds\"]\n",
    ")\n",
    "\n",
    "# Linear interpolation\n",
    "augmented_linear = linear_interpolation_augmentation(\n",
    "    original_train_series,\n",
    "    scale_factors=(0.9, 1.1, 0.5, 1.75)\n",
    ")\n",
    "\n",
    "# Cubic interpolation\n",
    "augmented_cubic = cubic_interpolation_augmentation(\n",
    "    original_train_series,\n",
    "    scale_factors=(0.9, 1.1, 0.5, 0.05)\n",
    ")\n",
    "\n",
    "# Spline interpolation\n",
    "augmented_spline = spline_interpolation_augmentation(\n",
    "    original_train_series,\n",
    "    smoothing_factors=(0.1, 0.3, 0.5, 0.7)\n",
    ")\n",
    "\n",
    "# Fourier interpolation\n",
    "augmented_fourier = fourier_interpolation_augmentation(\n",
    "    original_train_series,\n",
    "    keep_ratio=0.75\n",
    ")\n",
    "\n",
    "augmented_samples = (\n",
    "    # augmented_linear +\n",
    "    # augmented_cubic +\n",
    "    # augmented_spline +\n",
    "    augmented_fourier\n",
    ")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# VISUALIZATION OF INTERPOLATION AUGMENTATIONS\n",
    "# ============================================================\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_interpolation_augmentations(\n",
    "    original_series: pd.Series,\n",
    "    linear_samples: list = [],\n",
    "    cubic_samples: list = [],\n",
    "    spline_samples: list = [],\n",
    "    fourier_samples: list = [],\n",
    "    title: str = \"Interpolation-Based Time Series Augmentation\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualize original and interpolated augmented samples.\n",
    "    \"\"\"\n",
    "\n",
    "    plt.figure(figsize=(14, 7))\n",
    "\n",
    "    # Plot original series\n",
    "    plt.plot(\n",
    "        original_series.index,\n",
    "        original_series.values,\n",
    "        label=\"Original\",\n",
    "        linewidth=3,\n",
    "        color=\"black\",\n",
    "        alpha=0.9\n",
    "    )\n",
    "\n",
    "    # Linear interpolation\n",
    "    for i, aug in enumerate(linear_samples):\n",
    "        plt.plot(\n",
    "            original_series.index,\n",
    "            aug,\n",
    "            linestyle=\"--\",\n",
    "            linewidth=1.5,\n",
    "            alpha=0.6,\n",
    "            label=f\"Linear Aug {i+1}\"\n",
    "        )\n",
    "\n",
    "    # Cubic interpolation\n",
    "    for i, aug in enumerate(cubic_samples):\n",
    "        plt.plot(\n",
    "            original_series.index,\n",
    "            aug,\n",
    "            linestyle=\"-.\",\n",
    "            linewidth=1.5,\n",
    "            alpha=0.6,\n",
    "            label=f\"Cubic Aug {i+1}\"\n",
    "        )\n",
    "\n",
    "    # Spline interpolation\n",
    "    for i, aug in enumerate(spline_samples):\n",
    "        plt.plot(\n",
    "            original_series.index,\n",
    "            aug,\n",
    "            linestyle=\":\",\n",
    "            linewidth=2,\n",
    "            alpha=0.6,\n",
    "            label=f\"Spline Aug {i+1}\"\n",
    "        )\n",
    "\n",
    "    # Fourier interpolation\n",
    "    for i, aug in enumerate(fourier_samples):\n",
    "        plt.plot(\n",
    "            original_series.index,\n",
    "            aug,\n",
    "            linestyle=\"-\",\n",
    "            linewidth=2,\n",
    "            alpha=0.7,\n",
    "            label=f\"Fourier Aug {i+1}\"\n",
    "        )\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Normalized Value\")\n",
    "    plt.legend(\n",
    "        loc=\"upper left\",\n",
    "        ncol=2,\n",
    "        fontsize=9\n",
    "    )\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# APPLY VISUALIZATION\n",
    "# ============================================================\n",
    "# ============================================================\n",
    "# CREATE INTERPOLATION AUGMENTATIONS (DEFINE VARIABLES)\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "print(\"Augmented samples created:\")\n",
    "print(f\"Linear  : {len(augmented_linear)}\")\n",
    "print(f\"Cubic   : {len(augmented_cubic)}\")\n",
    "print(f\"Spline  : {len(augmented_spline)}\")\n",
    "print(f\"Fourier : {len(augmented_fourier)}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# VISUALIZE AUGMENTATIONS\n",
    "# ============================================================\n",
    "\n",
    "visualize_interpolation_augmentations(\n",
    "    original_series=original_train_series,\n",
    "    # linear_samples=augmented_linear,\n",
    "    # cubic_samples=augmented_cubic,\n",
    "    # spline_samples=augmented_spline,\n",
    "    fourier_samples=augmented_fourier,\n",
    "    title=\"Original vs Interpolation-Based Augmented Series\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"[Interpolation] Augmented samples: {len(augmented_samples)}\")\n",
    "\n",
    "train_prophet_interp = train_overall[[\"ds\", \"y_norm\"]].rename(\n",
    "    columns={\"y_norm\": \"y\"}\n",
    ").copy()\n",
    "\n",
    "for aug in augmented_samples:\n",
    "    aug_df = train_overall[[\"ds\"]].copy()\n",
    "    aug_df[\"y\"] = aug\n",
    "    train_prophet_interp = pd.concat(\n",
    "        [train_prophet_interp, aug_df],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "train_prophet_interp = (\n",
    "    train_prophet_interp\n",
    "    .sort_values(\"ds\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(f\"[Prophet] Training rows (augmented): {len(train_prophet_interp)}\")\n",
    "\n",
    "# ============================================================\n",
    "# 3) TRAIN PROPHET\n",
    "# ============================================================\n",
    "m_interp = Prophet(\n",
    "    seasonality_mode=\"additive\",\n",
    "    yearly_seasonality=True,\n",
    "    weekly_seasonality=False,\n",
    "    daily_seasonality=False,\n",
    "    changepoint_prior_scale=0.01\n",
    ")\n",
    "\n",
    "m_interp.fit(train_prophet_interp)\n",
    "\n",
    "# ============================================================\n",
    "# 4) FORECAST\n",
    "# ============================================================\n",
    "future = m_interp.make_future_dataframe(\n",
    "    periods=len(test_overall),\n",
    "    freq=\"MS\"\n",
    ")\n",
    "\n",
    "forecast = m_interp.predict(future)\n",
    "\n",
    "df_test_interp = (\n",
    "    forecast[[\"ds\", \"yhat\"]]\n",
    "    .merge(\n",
    "        test_overall[[\"ds\", \"y_norm\"]].rename(columns={\"y_norm\": \"y\"}),\n",
    "        on=\"ds\",\n",
    "        how=\"inner\"\n",
    "    )\n",
    "    .sort_values(\"ds\")\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# 5) EVALUATION\n",
    "# ============================================================\n",
    "\n",
    "mape_interp_unnorm= mean_absolute_percentage_error(\n",
    "    scaler_y_global.inverse_transform(df_test_interp[\"y\"].values.reshape(-1, 1)),\n",
    "    scaler_y_global.inverse_transform(df_test_interp[\"yhat\"].values.reshape(-1, 1))\n",
    ")\n",
    "\n",
    "print(f\"[Prophet + Interpolation] MAPE (normalized): {mape_interp:.4f}\")\n",
    "print(f\"[Prophet + Interpolation] MAPE (unnormalized): {mape_interp_unnorm:.4f}\")\n",
    "\n",
    "# ============================================================\n",
    "# 6) INVERSE SCALE PREDICTIONS\n",
    "# ============================================================\n",
    "test_overall[\"prophet_pred_interp\"] = scaler_y_global.inverse_transform(\n",
    "    df_test_interp[\"yhat\"].values.reshape(-1, 1)\n",
    ").flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9967f0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TIME SERIES PLOT: ACTUAL vs PREDICTED\n",
    "# (Prophet, NN, Hybrid — ORIGINAL UNITS)\n",
    "# ============================================================\n",
    "\n",
    "plt.figure(figsize=(16, 7))\n",
    "\n",
    "# -----------------------\n",
    "# 1) Training data (actual)\n",
    "# -----------------------\n",
    "plt.plot(\n",
    "    train_overall[\"ds\"],\n",
    "    train_overall[\"y_norm\"],\n",
    "    label=\"Training Actual\",\n",
    "    color=\"black\",\n",
    "    linewidth=2\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# 2) Test actuals\n",
    "# -----------------------\n",
    "plt.plot(\n",
    "    test_overall[\"ds\"],\n",
    "    test_overall[\"y_norm\"],\n",
    "    label=\"Test Actual\",\n",
    "    color=\"black\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# 3) Prophet predictions\n",
    "# -----------------------\n",
    "plt.plot(\n",
    "    test_overall[\"ds\"],\n",
    "    df_prophet_test[\"yhat\"],\n",
    "    label=\"Prophet Prediction\",\n",
    "    linestyle=\":\",\n",
    "    linewidth=2.5,\n",
    "    color=\"tab:blue\"\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# 4) NN predictions\n",
    "# -----------------------\n",
    "plt.plot(\n",
    "    test_overall[\"ds\"],\n",
    "    df_prophet_test_mbb[\"yhat\"],\n",
    "    label=\"NN Prediction\",\n",
    "    linestyle=\":\",\n",
    "    linewidth=2.5,\n",
    "    color=\"tab:green\"\n",
    ")\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# 6) Vertical line for train/test split\n",
    "# -----------------------\n",
    "split_date = train_overall[\"ds\"].iloc[-1]\n",
    "\n",
    "plt.axvline(\n",
    "    x=split_date,\n",
    "    color=\"gray\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2,\n",
    "    label=\"Train/Test Split\"\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# Labels, title, legend\n",
    "# -----------------------\n",
    "plt.xlabel(\"Date\", fontsize=12)\n",
    "plt.ylabel(\"Scope 1 Emissions (Original Units)\", fontsize=12)\n",
    "plt.title(\n",
    "    \"Actual vs Predicted Emissions\\n\"\n",
    "    \"(Prophet vs Rule-Regularised NN vs Hybrid)\",\n",
    "    fontsize=14\n",
    ")\n",
    "\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "plt.legend(fontsize=12)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4156ac32",
   "metadata": {},
   "source": [
    "# phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36aaf35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 0) Imports =====\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8\")  # updated style name\n",
    "sns.set_palette(\"viridis\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# ===== 1) CONFIG =====\n",
    "DATE_COL = \"datetime\"\n",
    "TARGET_COL = \"Scope1_per_unit\"\n",
    "PLANT_COL = \"Plant Name\"\n",
    "FORECAST_MONTHS = 7\n",
    "TEST_MONTHS = 7\n",
    "\n",
    "# ===== 2) START FROM df_2020 =====\n",
    "df = df_2022.copy()\n",
    "# ----- 2a) Ensure datetime -----\n",
    "if DATE_COL in df.columns:\n",
    "    ds = pd.to_datetime(df[DATE_COL], errors=\"coerce\", infer_datetime_format=True)\n",
    "else:\n",
    "    ds = pd.Series(pd.NaT, index=df.index)\n",
    "\n",
    "needs_rebuild = ds.isna()\n",
    "if needs_rebuild.any():\n",
    "    if not {\"year\", \"month\"}.issubset(df.columns):\n",
    "        raise ValueError(\"No valid 'Current Date' and missing 'year'/'month' to rebuild dates.\")\n",
    "    ds_rebuilt = pd.to_datetime(dict(year=df.loc[needs_rebuild, \"year\"],\n",
    "                                     month=df.loc[needs_rebuild, \"month\"],\n",
    "                                     day=1))\n",
    "    ds.loc[needs_rebuild] = ds_rebuilt\n",
    "\n",
    "df[\"ds\"] = pd.to_datetime(ds).dt.to_period(\"M\").dt.to_timestamp()\n",
    "\n",
    "# ----- 2b) Cleaning -----\n",
    "df = df.drop_duplicates(subset=[\"ds\", PLANT_COL, TARGET_COL], keep=\"last\")\n",
    "\n",
    "# ----- 2c) Imputation -----\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "def impute_zero_nan_with_mean(group: pd.DataFrame) -> pd.DataFrame:\n",
    "    g = group.copy()\n",
    "    for c in num_cols:\n",
    "        g[c] = g[c].replace(0, np.nan)\n",
    "        if g[c].notna().any():\n",
    "            g[c] = g[c].fillna(g[c].mean())\n",
    "    return g\n",
    "\n",
    "if \"year\" in df.columns:\n",
    "    df = df.groupby(\"year\", group_keys=False).apply(impute_zero_nan_with_mean)\n",
    "else:\n",
    "    df = impute_zero_nan_with_mean(df)\n",
    "\n",
    "for c in num_cols:\n",
    "    if df[c].isna().any():\n",
    "        df[c] = df[c].fillna(df[c].mean())\n",
    "\n",
    "# ===== 3) OVERALL SERIES =====\n",
    "def prepare_overall_series(df_in: pd.DataFrame) -> pd.DataFrame:\n",
    "    s = (df_in.groupby(\"ds\", as_index=False)[TARGET_COL]\n",
    "         .sum()\n",
    "         .sort_values(\"ds\"))\n",
    "    s = s.rename(columns={TARGET_COL: \"y\"})\n",
    "    return s\n",
    "\n",
    "overall = prepare_overall_series(df)\n",
    "overall = overall.sort_values(\"ds\").reset_index(drop=True)\n",
    "\n",
    "# Train/test split\n",
    "split_point = len(overall) - TEST_MONTHS if len(overall) > TEST_MONTHS else len(overall)\n",
    "train_overall = overall.iloc[:split_point].copy()\n",
    "test_overall = overall.iloc[split_point:].copy()\n",
    "\n",
    "# ===== 3A) PROPHET =====\n",
    "m_overall = Prophet(seasonality_mode=\"multiplicative\", yearly_seasonality=True, weekly_seasonality=False, daily_seasonality=False,changepoint_prior_scale=0.1 )\n",
    "m_overall.fit(train_overall)\n",
    "future_all = m_overall.make_future_dataframe(periods=len(test_overall), freq=\"MS\")\n",
    "fcst_all = m_overall.predict(future_all)\n",
    "# take exactly the test horizon (no overlap with train)\n",
    "\n",
    "\n",
    "if not test_overall.empty:\n",
    "    y_pred_prophet = (fcst_all[[\"ds\", \"yhat\"]]\n",
    "              .merge(test_overall[[\"ds\", \"y\"]], on=\"ds\", how=\"inner\"))\n",
    "    mape_prophet = mean_absolute_percentage_error(y_pred_prophet[\"y\"], y_pred_prophet[\"yhat\"])\n",
    "else:\n",
    "    mape_prophet = np.nan\n",
    "print(mape_prophet)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Merge forecast and actuals for plotting\n",
    "overall_pred = fcst_all.merge(overall, on=\"ds\", how=\"left\")\n",
    "overall_pred[\"residual\"] = overall_pred[\"y\"] - overall_pred[\"yhat\"]\n",
    "\n",
    "# Create a figure with 4 subplots\n",
    "fig, axes = plt.subplots(4, 1, figsize=(14, 20))\n",
    "\n",
    "# 1️⃣ Forecast vs Actuals\n",
    "axes[0].plot(overall_pred[\"ds\"], overall_pred[\"y\"], label=\"Actual\", marker=\"o\")\n",
    "axes[0].plot(overall_pred[\"ds\"], overall_pred[\"yhat\"], label=\"Prophet Forecast\", linestyle=\"--\")\n",
    "axes[0].fill_between(overall_pred[\"ds\"], overall_pred[\"yhat_lower\"], overall_pred[\"yhat_upper\"], color=\"skyblue\", alpha=0.3, label=\"Uncertainty\")\n",
    "axes[0].set_title(\"Forecast vs Actuals\")\n",
    "axes[0].set_xlabel(\"Date\")\n",
    "axes[0].set_ylabel(\"Scope1_per_unit\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# 2️⃣ Trend\n",
    "trend = fcst_all[[\"ds\", \"trend\"]]\n",
    "axes[1].plot(trend[\"ds\"], trend[\"trend\"], color=\"green\")\n",
    "axes[1].set_title(\"Trend Component\")\n",
    "axes[1].set_xlabel(\"Date\")\n",
    "axes[1].set_ylabel(\"Trend\")\n",
    "axes[1].grid(True)\n",
    "\n",
    "# 3️⃣ Yearly Seasonality\n",
    "seasonality = fcst_all[[\"ds\", \"yearly\"]]\n",
    "axes[2].plot(seasonality[\"ds\"], seasonality[\"yearly\"], color=\"orange\")\n",
    "axes[2].set_title(\"Yearly Seasonality Component\")\n",
    "axes[2].set_xlabel(\"Date\")\n",
    "axes[2].set_ylabel(\"Seasonality\")\n",
    "axes[2].grid(True)\n",
    "\n",
    "# 4️⃣ Residuals\n",
    "axes[3].plot(overall_pred[\"ds\"], overall_pred[\"residual\"], color=\"red\", marker=\"o\")\n",
    "axes[3].axhline(0, color=\"black\", linestyle=\"--\")\n",
    "axes[3].set_title(\"Residuals (Actual - Forecast)\")\n",
    "axes[3].set_xlabel(\"Date\")\n",
    "axes[3].set_ylabel(\"Residual\")\n",
    "axes[3].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1070398",
   "metadata": {},
   "source": [
    "# Breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1edc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 0) Imports =====\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8\")  # updated style name\n",
    "sns.set_palette(\"viridis\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# ===== 1) CONFIG =====\n",
    "DATE_COL = \"datetime\"\n",
    "TARGET_COL = \"Scope1_per_unit\"\n",
    "PLANT_COL = \"Plant Name\"\n",
    "FORECAST_MONTHS = 7\n",
    "TEST_MONTHS = 7\n",
    "\n",
    "# ===== 2) START FROM df_2020 =====\n",
    "df = df_2022.copy()\n",
    "df[\"India_Flag\"] = df[\"Plant Location\"].apply(lambda x: \"India\" if x==\"India\" else \"Outside India\")\n",
    "df=df[df['India_Flag']=='Outside India']\n",
    "# ----- 2a) Ensure datetime -----\n",
    "if DATE_COL in df.columns:\n",
    "    ds = pd.to_datetime(df[DATE_COL], errors=\"coerce\", infer_datetime_format=True)\n",
    "else:\n",
    "    ds = pd.Series(pd.NaT, index=df.index)\n",
    "\n",
    "needs_rebuild = ds.isna()\n",
    "if needs_rebuild.any():\n",
    "    if not {\"year\", \"month\"}.issubset(df.columns):\n",
    "        raise ValueError(\"No valid 'Current Date' and missing 'year'/'month' to rebuild dates.\")\n",
    "    ds_rebuilt = pd.to_datetime(dict(year=df.loc[needs_rebuild, \"year\"],\n",
    "                                     month=df.loc[needs_rebuild, \"month\"],\n",
    "                                     day=1))\n",
    "    ds.loc[needs_rebuild] = ds_rebuilt\n",
    "\n",
    "df[\"ds\"] = pd.to_datetime(ds).dt.to_period(\"M\").dt.to_timestamp()\n",
    "\n",
    "# ----- 2b) Cleaning -----\n",
    "df = df.drop_duplicates(subset=[\"ds\", PLANT_COL, TARGET_COL], keep=\"last\")\n",
    "\n",
    "# ----- 2c) Imputation -----\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "def impute_zero_nan_with_mean(group: pd.DataFrame) -> pd.DataFrame:\n",
    "    g = group.copy()\n",
    "    for c in num_cols:\n",
    "        g[c] = g[c].replace(0, np.nan)\n",
    "        if g[c].notna().any():\n",
    "            g[c] = g[c].fillna(g[c].mean())\n",
    "    return g\n",
    "\n",
    "if \"year\" in df.columns:\n",
    "    df = df.groupby(\"year\", group_keys=False).apply(impute_zero_nan_with_mean)\n",
    "else:\n",
    "    df = impute_zero_nan_with_mean(df)\n",
    "\n",
    "for c in num_cols:\n",
    "    if df[c].isna().any():\n",
    "        df[c] = df[c].fillna(df[c].mean())\n",
    "\n",
    "# ===== 3) OVERALL SERIES =====\n",
    "def prepare_overall_series(df_in: pd.DataFrame) -> pd.DataFrame:\n",
    "    s = (df_in.groupby(\"ds\", as_index=False)[TARGET_COL]\n",
    "         .sum()\n",
    "         .sort_values(\"ds\"))\n",
    "    s = s.rename(columns={TARGET_COL: \"y\"})\n",
    "    return s\n",
    "\n",
    "overall = prepare_overall_series(df)\n",
    "overall = overall.sort_values(\"ds\").reset_index(drop=True)\n",
    "\n",
    "# Train/test split\n",
    "split_point = len(overall) - TEST_MONTHS if len(overall) > TEST_MONTHS else len(overall)\n",
    "train_overall = overall.iloc[:split_point].copy()\n",
    "test_overall = overall.iloc[split_point:].copy()\n",
    "\n",
    "# ===== 3A) PROPHET =====\n",
    "m_overall = Prophet(seasonality_mode=\"multiplicative\", yearly_seasonality=True, weekly_seasonality=False, daily_seasonality=False,changepoint_prior_scale=0.1 )\n",
    "m_overall.fit(train_overall)\n",
    "future_all = m_overall.make_future_dataframe(periods=len(test_overall), freq=\"MS\")\n",
    "fcst_all = m_overall.predict(future_all)\n",
    "# take exactly the test horizon (no overlap with train)\n",
    "\n",
    "\n",
    "if not test_overall.empty:\n",
    "    y_pred_prophet = (fcst_all[[\"ds\", \"yhat\"]]\n",
    "              .merge(test_overall[[\"ds\", \"y\"]], on=\"ds\", how=\"inner\"))\n",
    "    mape_prophet = mean_absolute_percentage_error(y_pred_prophet[\"y\"], y_pred_prophet[\"yhat\"])\n",
    "else:\n",
    "    mape_prophet = np.nan\n",
    "print(mape_prophet)\n",
    "flag=True\n",
    "itr=0\n",
    "while flag:\n",
    "# ===== 3B) PINN FORECAST =====\n",
    "    class PINN(nn.Module):\n",
    "        def __init__(self, in_dim=1, hidden_dim=32, out_dim=1):\n",
    "            super().__init__()\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(in_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, out_dim)\n",
    "            )\n",
    "        def forward(self, x):\n",
    "            return self.net(x)\n",
    "\n",
    "    # Time index as feature\n",
    "    train_time = np.arange(len(train_overall)).reshape(-1, 1)\n",
    "    test_time = np.arange(len(train_overall), len(overall)).reshape(-1, 1)\n",
    "    y_train = train_overall[\"y\"].values.reshape(-1, 1)\n",
    "    # Physics features (optional)\n",
    "    if set(['Electricity Grid Energy Per Unit (GJ)']).issubset(df.columns):\n",
    "        agg = df.groupby(\"ds\")[['Electricity Grid Energy Per Unit (GJ)']].sum().reset_index()\n",
    "        agg = agg.sort_values(\"ds\").reset_index(drop=True)\n",
    "        X_phys = (agg[['Electricity Grid Energy Per Unit (GJ)']]/1000).values\n",
    "    else:\n",
    "        X_phys = np.ones((len(overall), 3))\n",
    "\n",
    "    X_phys_train = X_phys[:len(train_overall)]\n",
    "    X_phys_test = X_phys[len(train_overall):]\n",
    "\n",
    "\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    # ==== Standardize features & target ====\n",
    "    scaler_X = StandardScaler()\n",
    "    scaler_Y = StandardScaler()\n",
    "    scaler_phys = StandardScaler()\n",
    "\n",
    "    # Fit scalers on training data only\n",
    "    train_time_scaled = scaler_X.fit_transform(train_time)\n",
    "    test_time_scaled = scaler_X.transform(test_time)\n",
    "\n",
    "    y_train_scaled = scaler_Y.fit_transform(y_train)\n",
    "\n",
    "    X_phys_train_scaled = scaler_phys.fit_transform(X_phys_train)\n",
    "    X_phys_test_scaled = scaler_phys.transform(X_phys_test)\n",
    "\n",
    "    # Torch tensors (scaled data)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    X_t = torch.tensor(train_time_scaled, dtype=torch.float32).to(device)\n",
    "    Y_t = torch.tensor(y_train_scaled, dtype=torch.float32).to(device)\n",
    "    X_phys_t = torch.tensor(X_phys_train_scaled, dtype=torch.float32).to(device)\n",
    "\n",
    "\n",
    "    model = PINN(in_dim=1, hidden_dim=32, out_dim=1).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    class MAPELoss(nn.Module):\n",
    "        def __init__(self, eps=1e-6):\n",
    "            super().__init__()\n",
    "            self.eps = eps\n",
    "        def forward(self, y_pred, y_true):\n",
    "            return torch.mean(torch.abs(y_pred - y_true) / (torch.abs(y_true) + self.eps))\n",
    "\n",
    "    def physics_residual_loss_mape(y_pred, features, eps=1e-6):\n",
    "        elec = features[:, 0]\n",
    "        physics_estimate = elec/8\n",
    "        violation = torch.relu(physics_estimate - y_pred.squeeze())\n",
    "        return torch.mean(violation / (torch.abs(physics_estimate) + eps))\n",
    "\n",
    "    mape_loss_fn = MAPELoss()\n",
    "\n",
    "    # Train PINN\n",
    "    for epoch in range(10000):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_t)\n",
    "        data_loss = torch.mean((y_pred - Y_t) ** 2)\n",
    "        phys_loss = physics_residual_loss_mape(y_pred, X_phys_t)\n",
    "        loss =  data_loss \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # if epoch%1000==0:\n",
    "        #     print(f\"Epoch {epoch}, MAPE Loss: {loss.item():.6f}\")\n",
    "        #     print(f\"data: {data_loss:.3f}, phy:{phys_loss:.3f}, total: {(data_loss+phys_loss):.3f}\")\n",
    "        #     print()\n",
    "\n",
    "\n",
    "    # Predictions\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_train_pred_scaled = model(torch.tensor(train_time_scaled, dtype=torch.float32).to(device)).cpu().numpy()\n",
    "        y_test_pred_scaled = model(torch.tensor(test_time_scaled, dtype=torch.float32).to(device)).cpu().numpy()\n",
    "\n",
    "    # Inverse transform to original units\n",
    "    y_train_pred = scaler_Y.inverse_transform(y_train_pred_scaled)\n",
    "    y_test_pred = scaler_Y.inverse_transform(y_test_pred_scaled)\n",
    "\n",
    "    train_overall[\"pinn_pred\"] = y_train_pred.flatten()\n",
    "    test_overall[\"pinn_pred\"] = y_test_pred.flatten()\n",
    "\n",
    "\n",
    "    if not test_overall.empty:\n",
    "        mape_pinn = mean_absolute_percentage_error(test_overall[\"y\"], test_overall[\"pinn_pred\"])\n",
    "    else:\n",
    "        mape_pinn = np.nan\n",
    "\n",
    "    # ===== 3C) PINN + PROPHET RESIDUAL STACK =====\n",
    "    train_overall[\"residual\"] = train_overall[\"y\"] - train_overall[\"pinn_pred\"]\n",
    "    test_overall[\"residual\"]  = test_overall[\"y\"] - test_overall[\"pinn_pred\"]\n",
    "\n",
    "    train_res = train_overall[[\"ds\",\"residual\"]].rename(columns={\"residual\":\"y\"})\n",
    "    test_res  = test_overall[[\"ds\",\"residual\"]].rename(columns={\"residual\":\"y\"})\n",
    "\n",
    "\n",
    "\n",
    "    m_res = Prophet(seasonality_mode=\"multiplicative\", yearly_seasonality=True, weekly_seasonality=False, daily_seasonality=False,changepoint_prior_scale=0.1 )\n",
    "    m_res.fit(train_res)\n",
    "\n",
    "    future_res = m_res.make_future_dataframe(periods=len(test_res), freq=\"MS\")\n",
    "    forecast_res = m_res.predict(future_res)\n",
    "\n",
    "    res_pred = forecast_res[\"yhat\"].iloc[len(train_res):].values\n",
    "    test_overall[\"final_pred\"] = test_overall[\"pinn_pred\"].values + res_pred\n",
    "\n",
    "    if not test_overall.empty:\n",
    "        mape_pinn_prophet = mean_absolute_percentage_error(test_overall[\"y\"], test_overall[\"final_pred\"])\n",
    "    else:\n",
    "        mape_pinn_prophet = np.nan\n",
    "\n",
    "    itr+=1\n",
    "    if mape_pinn_prophet<mape_prophet:\n",
    "        print(mape_pinn_prophet,mape_prophet)\n",
    "        flag=False\n",
    "    else:\n",
    "        print(f'itr: {itr}, mape_pinn_prophet: {mape_pinn_prophet}, mape_prophet: {mape_prophet}')\n",
    "\n",
    "\n",
    "print(\"Iterations:\",itr)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Train residuals\n",
    "plt.plot(train_res[\"ds\"], train_res[\"y\"], \n",
    "         label=\"Train Residuals\", marker=\"o\", linestyle=\"-\", color=\"blue\")\n",
    "\n",
    "# Test residuals\n",
    "plt.plot(test_res[\"ds\"], test_res[\"y\"], \n",
    "         label=\"Test Residuals\", marker=\"s\", linestyle=\"--\", color=\"red\")\n",
    "\n",
    "# Reference line (zero residuals)\n",
    "plt.axhline(0, color=\"black\", linestyle=\"--\", linewidth=1)\n",
    "\n",
    "plt.title(\"Residuals: Train vs Test (PINN Predictions)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Residual (y - y_pred)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# ===== 4) RESULTS =====\n",
    "print(\"\\n--- RESULTS ---\")\n",
    "print(f\"Prophet-only MAPE: {mape_prophet:.2f}\")\n",
    "print(f\"PINN-only MAPE: {mape_pinn:.2f}\")\n",
    "print(f\"PINN + Prophet MAPE: {mape_pinn_prophet:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e13de78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Collect predictions\n",
    "predictions = {\n",
    "    \"Prophet-only\": y_pred_prophet[\"yhat\"].values if not test_overall.empty else [],\n",
    "    \"PINN-only\": test_overall[\"pinn_pred\"].values if not test_overall.empty else [],\n",
    "    \"PINN + Prophet\": test_overall[\"final_pred\"].values if not test_overall.empty else []\n",
    "}\n",
    "\n",
    "# Compute MAPEs\n",
    "results = {\n",
    "    \"Prophet-only\": mape_prophet,\n",
    "    \"PINN-only\": mape_pinn,\n",
    "    \"PINN + Prophet\": mape_pinn_prophet\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Training data (actuals)\n",
    "plt.plot(train_overall[\"ds\"], train_overall[\"y\"], \n",
    "         label=\"Train (Actual)\", color=\"black\", linewidth=2)\n",
    "\n",
    "# Test data (actuals)\n",
    "plt.plot(test_overall[\"ds\"], test_overall[\"y\"], \n",
    "         label=\"Test (Actual)\", color=\"blue\", linewidth=2)\n",
    "# Forecasts\n",
    "for name, y_pred in predictions.items():\n",
    "    if len(y_pred) > 0:  # only plot if available\n",
    "        plt.plot(test_overall[\"ds\"], y_pred, '--', label=f\"{name} (MAPE {results[name]:.2f}%)\", linewidth=2)\n",
    "\n",
    "# Vertical line for train/test split\n",
    "if not test_overall.empty:\n",
    "    split_date = test_overall[\"ds\"].iloc[0]\n",
    "    plt.axvline(split_date, color=\"gray\", linestyle=\"--\", label=\"Train/Test Split\")\n",
    "\n",
    "plt.title(\"Carbon Emissions Forecast (Outside India): Prophet vs PINN vs Hybrid\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Monthly Emissions (Scope 1)\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aa8291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 0) Imports =====\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8\")  # updated style name\n",
    "sns.set_palette(\"viridis\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# ===== 1) CONFIG =====\n",
    "DATE_COL = \"datetime\"\n",
    "TARGET_COL = \"Scope1_per_unit\"\n",
    "PLANT_COL = \"Plant Name\"\n",
    "FORECAST_MONTHS = 7\n",
    "TEST_MONTHS = 7\n",
    "\n",
    "# ===== 2) START FROM df_2020 =====\n",
    "df = df_2022.copy()\n",
    "df[\"India_Flag\"] = df[\"Plant Location\"].apply(lambda x: \"India\" if x==\"India\" else \"Outside India\")\n",
    "df=df[df['India_Flag']=='India']\n",
    "# ----- 2a) Ensure datetime -----\n",
    "if DATE_COL in df.columns:\n",
    "    ds = pd.to_datetime(df[DATE_COL], errors=\"coerce\", infer_datetime_format=True)\n",
    "else:\n",
    "    ds = pd.Series(pd.NaT, index=df.index)\n",
    "\n",
    "needs_rebuild = ds.isna()\n",
    "if needs_rebuild.any():\n",
    "    if not {\"year\", \"month\"}.issubset(df.columns):\n",
    "        raise ValueError(\"No valid 'Current Date' and missing 'year'/'month' to rebuild dates.\")\n",
    "    ds_rebuilt = pd.to_datetime(dict(year=df.loc[needs_rebuild, \"year\"],\n",
    "                                     month=df.loc[needs_rebuild, \"month\"],\n",
    "                                     day=1))\n",
    "    ds.loc[needs_rebuild] = ds_rebuilt\n",
    "\n",
    "df[\"ds\"] = pd.to_datetime(ds).dt.to_period(\"M\").dt.to_timestamp()\n",
    "\n",
    "# ----- 2b) Cleaning -----\n",
    "df = df.drop_duplicates(subset=[\"ds\", PLANT_COL, TARGET_COL], keep=\"last\")\n",
    "\n",
    "# ----- 2c) Imputation -----\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "def impute_zero_nan_with_mean(group: pd.DataFrame) -> pd.DataFrame:\n",
    "    g = group.copy()\n",
    "    for c in num_cols:\n",
    "        g[c] = g[c].replace(0, np.nan)\n",
    "        if g[c].notna().any():\n",
    "            g[c] = g[c].fillna(g[c].mean())\n",
    "    return g\n",
    "\n",
    "if \"year\" in df.columns:\n",
    "    df = df.groupby(\"year\", group_keys=False).apply(impute_zero_nan_with_mean)\n",
    "else:\n",
    "    df = impute_zero_nan_with_mean(df)\n",
    "\n",
    "for c in num_cols:\n",
    "    if df[c].isna().any():\n",
    "        df[c] = df[c].fillna(df[c].mean())\n",
    "\n",
    "# ===== 3) OVERALL SERIES =====\n",
    "def prepare_overall_series(df_in: pd.DataFrame) -> pd.DataFrame:\n",
    "    s = (df_in.groupby(\"ds\", as_index=False)[TARGET_COL]\n",
    "         .sum()\n",
    "         .sort_values(\"ds\"))\n",
    "    s = s.rename(columns={TARGET_COL: \"y\"})\n",
    "    return s\n",
    "\n",
    "overall = prepare_overall_series(df)\n",
    "overall = overall.sort_values(\"ds\").reset_index(drop=True)\n",
    "\n",
    "# Train/test split\n",
    "split_point = len(overall) - TEST_MONTHS if len(overall) > TEST_MONTHS else len(overall)\n",
    "train_overall = overall.iloc[:split_point].copy()\n",
    "test_overall = overall.iloc[split_point:].copy()\n",
    "\n",
    "# ===== 3A) PROPHET =====\n",
    "m_overall = Prophet(seasonality_mode=\"multiplicative\", yearly_seasonality=True, weekly_seasonality=False, daily_seasonality=False,changepoint_prior_scale=0.1 )\n",
    "m_overall.fit(train_overall)\n",
    "future_all = m_overall.make_future_dataframe(periods=len(test_overall), freq=\"MS\")\n",
    "fcst_all = m_overall.predict(future_all)\n",
    "# take exactly the test horizon (no overlap with train)\n",
    "\n",
    "\n",
    "if not test_overall.empty:\n",
    "    y_pred_prophet = (fcst_all[[\"ds\", \"yhat\"]]\n",
    "              .merge(test_overall[[\"ds\", \"y\"]], on=\"ds\", how=\"inner\"))\n",
    "    mape_prophet = mean_absolute_percentage_error(y_pred_prophet[\"y\"], y_pred_prophet[\"yhat\"])\n",
    "else:\n",
    "    mape_prophet = np.nan\n",
    "print(mape_prophet)\n",
    "flag=True\n",
    "itr=0\n",
    "while flag:\n",
    "# ===== 3B) PINN FORECAST =====\n",
    "    class PINN(nn.Module):\n",
    "        def __init__(self, in_dim=1, hidden_dim=32, out_dim=1):\n",
    "            super().__init__()\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(in_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, out_dim)\n",
    "            )\n",
    "        def forward(self, x):\n",
    "            return self.net(x)\n",
    "\n",
    "    # Time index as feature\n",
    "    train_time = np.arange(len(train_overall)).reshape(-1, 1)\n",
    "    test_time = np.arange(len(train_overall), len(overall)).reshape(-1, 1)\n",
    "    y_train = train_overall[\"y\"].values.reshape(-1, 1)\n",
    "    # Physics features (optional)\n",
    "    if set(['Electricity Grid Energy Per Unit (GJ)']).issubset(df.columns):\n",
    "        agg = df.groupby(\"ds\")[['Electricity Grid Energy Per Unit (GJ)']].sum().reset_index()\n",
    "        agg = agg.sort_values(\"ds\").reset_index(drop=True)\n",
    "        X_phys = (agg[['Electricity Grid Energy Per Unit (GJ)']]/1000).values\n",
    "    else:\n",
    "        X_phys = np.ones((len(overall), 3))\n",
    "\n",
    "    X_phys_train = X_phys[:len(train_overall)]\n",
    "    X_phys_test = X_phys[len(train_overall):]\n",
    "\n",
    "\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    # ==== Standardize features & target ====\n",
    "    scaler_X = StandardScaler()\n",
    "    scaler_Y = StandardScaler()\n",
    "    scaler_phys = StandardScaler()\n",
    "\n",
    "    # Fit scalers on training data only\n",
    "    train_time_scaled = scaler_X.fit_transform(train_time)\n",
    "    test_time_scaled = scaler_X.transform(test_time)\n",
    "\n",
    "    y_train_scaled = scaler_Y.fit_transform(y_train)\n",
    "\n",
    "    X_phys_train_scaled = scaler_phys.fit_transform(X_phys_train)\n",
    "    X_phys_test_scaled = scaler_phys.transform(X_phys_test)\n",
    "\n",
    "    # Torch tensors (scaled data)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    X_t = torch.tensor(train_time_scaled, dtype=torch.float32).to(device)\n",
    "    Y_t = torch.tensor(y_train_scaled, dtype=torch.float32).to(device)\n",
    "    X_phys_t = torch.tensor(X_phys_train_scaled, dtype=torch.float32).to(device)\n",
    "\n",
    "\n",
    "    model = PINN(in_dim=1, hidden_dim=32, out_dim=1).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    class MAPELoss(nn.Module):\n",
    "        def __init__(self, eps=1e-6):\n",
    "            super().__init__()\n",
    "            self.eps = eps\n",
    "        def forward(self, y_pred, y_true):\n",
    "            return torch.mean(torch.abs(y_pred - y_true) / (torch.abs(y_true) + self.eps))\n",
    "\n",
    "    def physics_residual_loss_mape(y_pred, features, eps=1e-6):\n",
    "        elec = features[:, 0]\n",
    "        physics_estimate = elec/8\n",
    "        violation = torch.relu(physics_estimate - y_pred.squeeze())\n",
    "        return torch.mean(violation / (torch.abs(physics_estimate) + eps))\n",
    "\n",
    "    mape_loss_fn = MAPELoss()\n",
    "\n",
    "    # Train PINN\n",
    "    for epoch in range(10000):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_t)\n",
    "        data_loss = torch.mean((y_pred - Y_t) ** 2)\n",
    "        phys_loss = physics_residual_loss_mape(y_pred, X_phys_t)\n",
    "        loss =  data_loss \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # if epoch%1000==0:\n",
    "        #     print(f\"Epoch {epoch}, MAPE Loss: {loss.item():.6f}\")\n",
    "        #     print(f\"data: {data_loss:.3f}, phy:{phys_loss:.3f}, total: {(data_loss+phys_loss):.3f}\")\n",
    "        #     print()\n",
    "\n",
    "\n",
    "    # Predictions\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_train_pred_scaled = model(torch.tensor(train_time_scaled, dtype=torch.float32).to(device)).cpu().numpy()\n",
    "        y_test_pred_scaled = model(torch.tensor(test_time_scaled, dtype=torch.float32).to(device)).cpu().numpy()\n",
    "\n",
    "    # Inverse transform to original units\n",
    "    y_train_pred = scaler_Y.inverse_transform(y_train_pred_scaled)\n",
    "    y_test_pred = scaler_Y.inverse_transform(y_test_pred_scaled)\n",
    "\n",
    "    train_overall[\"pinn_pred\"] = y_train_pred.flatten()\n",
    "    test_overall[\"pinn_pred\"] = y_test_pred.flatten()\n",
    "\n",
    "\n",
    "    if not test_overall.empty:\n",
    "        mape_pinn = mean_absolute_percentage_error(test_overall[\"y\"], test_overall[\"pinn_pred\"])\n",
    "    else:\n",
    "        mape_pinn = np.nan\n",
    "\n",
    "    # ===== 3C) PINN + PROPHET RESIDUAL STACK =====\n",
    "    train_overall[\"residual\"] = train_overall[\"y\"] - train_overall[\"pinn_pred\"]\n",
    "    test_overall[\"residual\"]  = test_overall[\"y\"] - test_overall[\"pinn_pred\"]\n",
    "\n",
    "    train_res = train_overall[[\"ds\",\"residual\"]].rename(columns={\"residual\":\"y\"})\n",
    "    test_res  = test_overall[[\"ds\",\"residual\"]].rename(columns={\"residual\":\"y\"})\n",
    "\n",
    "\n",
    "\n",
    "    m_res = Prophet(seasonality_mode=\"multiplicative\", yearly_seasonality=True, weekly_seasonality=False, daily_seasonality=False,changepoint_prior_scale=0.1 )\n",
    "    m_res.fit(train_res)\n",
    "\n",
    "    future_res = m_res.make_future_dataframe(periods=len(test_res), freq=\"MS\")\n",
    "    forecast_res = m_res.predict(future_res)\n",
    "\n",
    "    res_pred = forecast_res[\"yhat\"].iloc[len(train_res):].values\n",
    "    test_overall[\"final_pred\"] = test_overall[\"pinn_pred\"].values + res_pred\n",
    "\n",
    "    if not test_overall.empty:\n",
    "        mape_pinn_prophet = mean_absolute_percentage_error(test_overall[\"y\"], test_overall[\"final_pred\"])\n",
    "    else:\n",
    "        mape_pinn_prophet = np.nan\n",
    "\n",
    "    itr+=1\n",
    "    if mape_pinn_prophet<mape_prophet:\n",
    "        print(mape_pinn_prophet,mape_prophet)\n",
    "        flag=False\n",
    "    else:\n",
    "        print(f'itr: {itr}, mape_pinn_prophet: {mape_pinn_prophet}, mape_prophet: {mape_prophet}')\n",
    "\n",
    "\n",
    "print(\"Iterations:\",itr)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Train residuals\n",
    "plt.plot(train_res[\"ds\"], train_res[\"y\"], \n",
    "         label=\"Train Residuals\", marker=\"o\", linestyle=\"-\", color=\"blue\")\n",
    "\n",
    "# Test residuals\n",
    "plt.plot(test_res[\"ds\"], test_res[\"y\"], \n",
    "         label=\"Test Residuals\", marker=\"s\", linestyle=\"--\", color=\"red\")\n",
    "\n",
    "# Reference line (zero residuals)\n",
    "plt.axhline(0, color=\"black\", linestyle=\"--\", linewidth=1)\n",
    "\n",
    "plt.title(\"Residuals: Train vs Test (PINN Predictions)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Residual (y - y_pred)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# ===== 4) RESULTS =====\n",
    "print(\"\\n--- RESULTS ---\")\n",
    "print(f\"Prophet-only MAPE: {mape_prophet:.2f}\")\n",
    "print(f\"PINN-only MAPE: {mape_pinn:.2f}\")\n",
    "print(f\"PINN + Prophet MAPE: {mape_pinn_prophet:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4a8064",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Collect predictions\n",
    "predictions = {\n",
    "    \"Prophet-only\": y_pred_prophet[\"yhat\"].values if not test_overall.empty else [],\n",
    "    \"PINN-only\": test_overall[\"pinn_pred\"].values if not test_overall.empty else [],\n",
    "    \"PINN + Prophet\": test_overall[\"final_pred\"].values if not test_overall.empty else []\n",
    "}\n",
    "\n",
    "# Compute MAPEs\n",
    "results = {\n",
    "    \"Prophet-only\": mape_prophet,\n",
    "    \"PINN-only\": mape_pinn,\n",
    "    \"PINN + Prophet\": mape_pinn_prophet\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Training data (actuals)\n",
    "plt.plot(train_overall[\"ds\"], train_overall[\"y\"], \n",
    "         label=\"Train (Actual)\", color=\"black\", linewidth=2)\n",
    "\n",
    "# Test data (actuals)\n",
    "plt.plot(test_overall[\"ds\"], test_overall[\"y\"], \n",
    "         label=\"Test (Actual)\", color=\"blue\", linewidth=2)\n",
    "# Forecasts\n",
    "for name, y_pred in predictions.items():\n",
    "    if len(y_pred) > 0:  # only plot if available\n",
    "        plt.plot(test_overall[\"ds\"], y_pred, '--', label=f\"{name} (MAPE {results[name]:.2f}%)\", linewidth=2)\n",
    "\n",
    "# Vertical line for train/test split\n",
    "if not test_overall.empty:\n",
    "    split_date = test_overall[\"ds\"].iloc[0]\n",
    "    plt.axvline(split_date, color=\"gray\", linestyle=\"--\", label=\"Train/Test Split\")\n",
    "\n",
    "plt.title(\"Carbon Emissions Forecast (Outside India): Prophet vs PINN vs Hybrid\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Monthly Emissions (Scope 1)\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e62e639",
   "metadata": {},
   "source": [
    "# Other Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57879252",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df_2022.copy()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f315abcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fca0187",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# ---------------------------\n",
    "# Feature Engineering\n",
    "# ---------------------------\n",
    "def create_features(df_final):\n",
    "    df_monthly = df_final.groupby('datetime')[['Scope1_per_unit','Electricity Grid Energy Per Unit (GJ)','Scope1','Production Actual Quantity (MT/Month)']].sum().reset_index()\n",
    "    df_monthly = df_monthly.rename(columns={'datetime':'ds','Scope1_per_unit':'y'})\n",
    "    df = df_monthly.copy()\n",
    "    df['year'] = df['ds'].dt.year\n",
    "    df['month'] = df['ds'].dt.month\n",
    "    df['quarter'] = df['ds'].dt.quarter\n",
    "    df['dayofyear'] = df['ds'].dt.dayofyear\n",
    "    df['sin_month'] = np.sin(2 * np.pi * df['month']/12)\n",
    "    df['cos_month'] = np.cos(2 * np.pi * df['month']/12)\n",
    "    return df\n",
    "\n",
    "df_ml = create_features(df)\n",
    "\n",
    "# Train/Test split (same as earlier)\n",
    "train_ml = df_ml[(df_ml['ds'] >= '2021-01-01') & (df_ml['ds'] < '2025-01-01')]\n",
    "test_ml  = df_ml[(df_ml['ds'] >= '2025-01-01') & (df_ml['ds'] <= '2026-05-01')]\n",
    "\n",
    "X_train = train_ml.drop(columns=['ds','y'])\n",
    "y_train = train_ml['y']\n",
    "X_test  = test_ml.drop(columns=['ds','y'])\n",
    "y_test  = test_ml['y']\n",
    "\n",
    "# ---------------------------\n",
    "# Machine Learning Models\n",
    "# ---------------------------\n",
    "\n",
    "models = {\n",
    "    \"RandomForest\": RandomForestRegressor(n_estimators=200, random_state=42),\n",
    "    \"GradientBoosting\": GradientBoostingRegressor(n_estimators=200, random_state=42),\n",
    "    \"XGBoost\": XGBRegressor(n_estimators=200, random_state=42),\n",
    "    \"SVR\": SVR(kernel='rbf', C=200, gamma=0.1)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------------------\n",
    "# Train ML models and store predictions\n",
    "# ---------------------------\n",
    "predictions = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    predictions[name] = y_pred\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "    results[name] = mape\n",
    "\n",
    "# ---------------------------\n",
    "# Plot Train, Test, and Predictions\n",
    "# ---------------------------\n",
    "plt.figure(figsize=(18,8))\n",
    "\n",
    "# Training data\n",
    "plt.plot(train_ml['ds'], y_train, label=\"Train\", color=\"black\", linewidth=2)\n",
    "\n",
    "# Test actual\n",
    "plt.plot(test_ml['ds'], y_test, label=\"Test (Actual)\", color=\"blue\", linewidth=2)\n",
    "\n",
    "# Forecasts from ML models\n",
    "for name, y_pred in predictions.items():\n",
    "    plt.plot(test_ml['ds'], y_pred, '--', label=f\"{name} (MAPE {results[name]:.2f}%)\", linewidth=2)\n",
    "\n",
    "# Vertical line for train/test split\n",
    "plt.axvline(pd.Timestamp(\"2025-01-01\"), color=\"gray\", linestyle=\"--\", label=\"Train/Test Split\")\n",
    "\n",
    "plt.title(\"Scope1 Emissions Forecast (Machine Learning): RF, Gradient Boosting, XGBoost, SVR\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Total Scope1 per Unit of production MT\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Compare All Models\n",
    "# ---------------------------\n",
    "print(\"\\nModel Comparison:\")\n",
    "for model, mape in results.items():\n",
    "    print(f\"{model}: {mape:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd11c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "# ===== 1) CONFIG =====\n",
    "DATE_COL = \"datetime\"\n",
    "TARGET_COL = \"Scope1_per_unit\"\n",
    "PLANT_COL = \"Plant Name\"\n",
    "FORECAST_MONTHS = 7\n",
    "TEST_MONTHS = 7\n",
    "\n",
    "# ===== 2) START FROM df_2020 =====\n",
    "df = df_2022.copy()\n",
    "# ----- 2a) Ensure datetime -----\n",
    "if DATE_COL in df.columns:\n",
    "    ds = pd.to_datetime(df[DATE_COL], errors=\"coerce\", infer_datetime_format=True)\n",
    "else:\n",
    "    ds = pd.Series(pd.NaT, index=df.index)\n",
    "\n",
    "needs_rebuild = ds.isna()\n",
    "if needs_rebuild.any():\n",
    "    if not {\"year\", \"month\"}.issubset(df.columns):\n",
    "        raise ValueError(\"No valid 'Current Date' and missing 'year'/'month' to rebuild dates.\")\n",
    "    ds_rebuilt = pd.to_datetime(dict(year=df.loc[needs_rebuild, \"year\"],\n",
    "                                     month=df.loc[needs_rebuild, \"month\"],\n",
    "                                     day=1))\n",
    "    ds.loc[needs_rebuild] = ds_rebuilt\n",
    "\n",
    "df[\"ds\"] = pd.to_datetime(ds).dt.to_period(\"M\").dt.to_timestamp()\n",
    "\n",
    "# ----- 2b) Cleaning -----\n",
    "df = df.drop_duplicates(subset=[\"ds\", PLANT_COL, TARGET_COL], keep=\"last\")\n",
    "\n",
    "# ----- 2c) Imputation -----\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "def impute_zero_nan_with_mean(group: pd.DataFrame) -> pd.DataFrame:\n",
    "    g = group.copy()\n",
    "    for c in num_cols:\n",
    "        g[c] = g[c].replace(0, np.nan)\n",
    "        if g[c].notna().any():\n",
    "            g[c] = g[c].fillna(g[c].mean())\n",
    "    return g\n",
    "\n",
    "if \"year\" in df.columns:\n",
    "    df = df.groupby(\"year\", group_keys=False).apply(impute_zero_nan_with_mean)\n",
    "else:\n",
    "    df = impute_zero_nan_with_mean(df)\n",
    "\n",
    "for c in num_cols:\n",
    "    if df[c].isna().any():\n",
    "        df[c] = df[c].fillna(df[c].mean())\n",
    "\n",
    "# ===== 3) OVERALL SERIES =====\n",
    "def prepare_overall_series(df_in: pd.DataFrame) -> pd.DataFrame:\n",
    "    s = (df_in.groupby(\"ds\", as_index=False)[TARGET_COL]\n",
    "         .sum()\n",
    "         .sort_values(\"ds\"))\n",
    "    s = s.rename(columns={TARGET_COL: \"y\"})\n",
    "    return s\n",
    "\n",
    "overall = prepare_overall_series(df)\n",
    "overall = overall.sort_values(\"ds\").reset_index(drop=True)\n",
    "\n",
    "# Train/test split\n",
    "split_point = len(overall) - TEST_MONTHS if len(overall) > TEST_MONTHS else len(overall)\n",
    "train_overall = overall.iloc[:split_point].copy()\n",
    "test_overall = overall.iloc[split_point:].copy()\n",
    "\n",
    "# ===== 3A) PROPHET =====\n",
    "m_overall = Prophet(seasonality_mode=\"multiplicative\", yearly_seasonality=True, weekly_seasonality=False, daily_seasonality=False,changepoint_prior_scale=0.1 )\n",
    "m_overall.fit(train_overall)\n",
    "future_all = m_overall.make_future_dataframe(periods=len(test_overall), freq=\"MS\")\n",
    "fcst_all = m_overall.predict(future_all)\n",
    "# take exactly the test horizon (no overlap with train)\n",
    "\n",
    "\n",
    "if not test_overall.empty:\n",
    "    y_pred_prophet = (fcst_all[[\"ds\", \"yhat\"]]\n",
    "              .merge(test_overall[[\"ds\", \"y\"]], on=\"ds\", how=\"inner\"))\n",
    "    mape_prophet = mean_absolute_percentage_error(y_pred_prophet[\"y\"], y_pred_prophet[\"yhat\"])\n",
    "else:\n",
    "    mape_prophet = np.nan\n",
    "print(mape_prophet)\n",
    "\n",
    "# -------------------------\n",
    "# (2) Holt-Winters (monthly seasonality)\n",
    "# -------------------------\n",
    "hw_model = ExponentialSmoothing(train_overall['y'],\n",
    "                                trend='add',\n",
    "                                seasonal='add',\n",
    "                                seasonal_periods=12).fit()\n",
    "hw_forecast = hw_model.forecast(len(test_overall['y']))\n",
    "\n",
    "# -------------------------\n",
    "# (3) ARIMA\n",
    "# -------------------------\n",
    "arima_model = ARIMA(train_overall['y'], order=(12,1,12))  \n",
    "arima_fit = arima_model.fit()\n",
    "arima_forecast = arima_fit.forecast(len(test_overall['y']))\n",
    "\n",
    "# Seasonal ARIMA\n",
    "import pmdarima as pm\n",
    "\n",
    "sarima_model = pm.auto_arima(train_overall['y'],\n",
    "                             seasonal=True,\n",
    "                             m=12,  # 12 months in a seasonal cycle\n",
    "                             stepwise=True,\n",
    "                             suppress_warnings=True)\n",
    "sarima_forecast = sarima_model.predict(n_periods=len(test_overall['y']))\n",
    "\n",
    "sarima_mape = mean_absolute_percentage_error(test_overall['y'], sarima_forecast) * 100\n",
    "# -------------------------\n",
    "# Evaluate MAPE\n",
    "# -------------------------\n",
    "hw_mape = mean_absolute_percentage_error(test_overall['y'], hw_forecast) * 100\n",
    "arima_mape = mean_absolute_percentage_error(test_overall['y'], arima_forecast) * 100\n",
    "print(f\"Prophet MAPE: {mape_prophet:.2f}%\")\n",
    "print(f\"Holt-Winters MAPE: {hw_mape:.2f}%\")\n",
    "print(f\"ARIMA MAPE: {arima_mape:.2f}%\")\n",
    "print(f\"SARIMA MAPE: {sarima_mape:.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Plot Comparison\n",
    "# -------------------------\n",
    "\n",
    "\n",
    "plt.figure(figsize=(14,5))\n",
    "plt.plot(train_overall['ds'], train_overall['y'], label=\"Train\", color = 'black', linewidth=2)\n",
    "plt.plot(test_overall['ds'], test_overall['y'], label=\"Test (Actual)\", color='blue',linewidth=2)\n",
    "\n",
    "# Prophet forecast (assuming you have eval_df with 'ds','yhat')\n",
    "plt.plot(eval_df['ds'], eval_df['yhat'], '--', label=f\"Prophet (MAPE={mape_prophet:.2f}%)\")\n",
    "\n",
    "# Holt-Winters forecast\n",
    "plt.plot(test_overall['ds'], hw_forecast, '--', label=f\"Holt-Winters (MAPE={hw_mape:.2f}%)\")\n",
    "\n",
    "# ARIMA forecast\n",
    "plt.plot(test_overall['ds'], sarima_forecast, '--', label=f\"SARIMA (MAPE={sarima_mape:.2f}%)\")\n",
    "plt.plot(test_overall['ds'], arima_forecast, '--', label=f\"ARIMA (MAPE={arima_mape:.2f}%)\")\n",
    "\n",
    "\n",
    "plt.axvline(pd.Timestamp('2024-01-01'), color='gray', linestyle='--', label=\"Train/Test Split\")\n",
    "plt.title(\"Carbon Emissions Forecast (Classical Models): Prophet, Holt-Winters, SARIMA, ARIMA \")\n",
    "plt.xlabel(\"Date\"); plt.ylabel(\"Total Emissions\")\n",
    "plt.legend(); plt.grid(alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7447f84b",
   "metadata": {},
   "source": [
    "# sindy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51844b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATE_COL = \"datetime\"\n",
    "TARGET_COL = \"Scope1_per_unit\"\n",
    "PLANT_COL = \"Plant Name\"\n",
    "FORECAST_MONTHS = 7\n",
    "TEST_MONTHS = 7\n",
    "\n",
    "# ===== 2) START FROM df_2020 =====\n",
    "df = df_2022.copy()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5837bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "col=['Scope_2','totalWaterConsumption','Electricity Grid Energy Per Unit (GJ) ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1dcadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 0) Imports =====\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import seaborn as sns\n",
    "from prophet import Prophet\n",
    "import pysindy as ps\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8\")  # updated style name\n",
    "sns.set_palette(\"viridis\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# ===== 1) CONFIG =====\n",
    "DATE_COL = \"datetime\"\n",
    "TARGET_COL = \"Scope1_per_unit\"\n",
    "PLANT_COL = \"Plant Name\"\n",
    "FORECAST_MONTHS = 7\n",
    "TEST_MONTHS = 7\n",
    "\n",
    "col1='Scope_2'\n",
    "col2='totalWaterConsumption'\n",
    "col3='Electricity Grid Energy Per Unit (GJ)'\n",
    "\n",
    "# ===== 2) START FROM df_2020 =====\n",
    "df = df_2022.copy()\n",
    "# ----- 2a) Ensure datetime -----\n",
    "if DATE_COL in df.columns:\n",
    "    ds = pd.to_datetime(df[DATE_COL], errors=\"coerce\", infer_datetime_format=True)\n",
    "else:\n",
    "    ds = pd.Series(pd.NaT, index=df.index)\n",
    "\n",
    "needs_rebuild = ds.isna()\n",
    "if needs_rebuild.any():\n",
    "    if not {\"year\", \"month\"}.issubset(df.columns):\n",
    "        raise ValueError(\"No valid 'Current Date' and missing 'year'/'month' to rebuild dates.\")\n",
    "    ds_rebuilt = pd.to_datetime(dict(year=df.loc[needs_rebuild, \"year\"],\n",
    "                                     month=df.loc[needs_rebuild, \"month\"],\n",
    "                                     day=1))\n",
    "    ds.loc[needs_rebuild] = ds_rebuilt\n",
    "\n",
    "df[\"ds\"] = pd.to_datetime(ds).dt.to_period(\"M\").dt.to_timestamp()\n",
    "\n",
    "# ----- 2b) Cleaning -----\n",
    "df = df.drop_duplicates(subset=[\"ds\", PLANT_COL, TARGET_COL], keep=\"last\")\n",
    "\n",
    "# ----- 2c) Imputation -----\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "def impute_zero_nan_with_mean(group: pd.DataFrame) -> pd.DataFrame:\n",
    "    g = group.copy()\n",
    "    for c in num_cols:\n",
    "        g[c] = g[c].replace(0, np.nan)\n",
    "        if g[c].notna().any():\n",
    "            g[c] = g[c].fillna(g[c].mean())\n",
    "    return g\n",
    "\n",
    "if \"year\" in df.columns:\n",
    "    df = df.groupby(\"year\", group_keys=False).apply(impute_zero_nan_with_mean)\n",
    "else:\n",
    "    df = impute_zero_nan_with_mean(df)\n",
    "\n",
    "for c in num_cols:\n",
    "    if df[c].isna().any():\n",
    "        df[c] = df[c].fillna(df[c].mean())\n",
    "\n",
    "# ===== 3) OVERALL SERIES =====\n",
    "def prepare_overall_series(df_in: pd.DataFrame) -> pd.DataFrame:\n",
    "    s = (df_in.groupby(\"ds\", as_index=False)[[TARGET_COL,col1,col2,col3]]\n",
    "         .sum()\n",
    "         .sort_values(\"ds\"))\n",
    "    s = s.rename(columns={TARGET_COL: \"y\"})\n",
    "    return s\n",
    "\n",
    "overall = prepare_overall_series(df)\n",
    "overall = overall.sort_values(\"ds\").reset_index(drop=True)\n",
    "\n",
    "# Train/test split\n",
    "split_point = len(overall) - TEST_MONTHS if len(overall) > TEST_MONTHS else len(overall)\n",
    "train_overall = overall[['ds','y']].iloc[:split_point].copy()\n",
    "test_overall = overall[['ds','y']].iloc[split_point:].copy()\n",
    "\n",
    "sindyTrain=overall.iloc[:split_point].copy()\n",
    "sindyTest=overall.iloc[split_point:].copy()\n",
    "\n",
    "\n",
    "\n",
    "# ===== 3A) PROPHET =====\n",
    "m_overall = Prophet(seasonality_mode=\"multiplicative\", yearly_seasonality=True, weekly_seasonality=False, daily_seasonality=False,changepoint_prior_scale=0.1 )\n",
    "m_overall.fit(train_overall)\n",
    "future_all = m_overall.make_future_dataframe(periods=len(test_overall), freq=\"MS\")\n",
    "fcst_all = m_overall.predict(future_all)\n",
    "# take exactly the test horizon (no overlap with train)\n",
    "\n",
    "\n",
    "if not test_overall.empty:\n",
    "    y_pred_prophet = (fcst_all[[\"ds\", \"yhat\"]]\n",
    "              .merge(test_overall[[\"ds\", \"y\"]], on=\"ds\", how=\"inner\"))\n",
    "    mape_prophet = mean_absolute_percentage_error(y_pred_prophet[\"y\"], y_pred_prophet[\"yhat\"])\n",
    "else:\n",
    "    mape_prophet = np.nan\n",
    "print(mape_prophet)\n",
    "\n",
    "\n",
    "\n",
    "#SINDy\n",
    "X_train=sindyTrain.drop(['y','ds'], axis=1)\n",
    "y_train=sindyTrain['y']\n",
    "\n",
    "scaler_X = StandardScaler()\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "scaler_y = StandardScaler()\n",
    "y_train_scaled = scaler_y.fit_transform(np.array(y_train).reshape(-1, 1)).flatten()\n",
    "# -------------------------\n",
    "# 3) Build SINDy model using precomputed derivatives\n",
    "# -------------------------\n",
    "# Combine features + target to be the state for SINDy: \n",
    "import numpy as np\n",
    "\n",
    "X_sindy = np.hstack((X_train_scaled, y_train_scaled.reshape(-1, 1))).astype(float)\n",
    "N = X_sindy.shape[0]\n",
    "# Build time vector (uniform monthly steps)\n",
    "dt = 1.0\n",
    "t = np.arange(N) * dt  # 1D numeric time vector\n",
    "x_dot = np.gradient(X_sindy, t, axis=0)   # shape (N, 4)\n",
    "\n",
    "# Fit SINDy using precomputed derivatives (robust)\n",
    "feature_library = ps.PolynomialLibrary(degree=2)\n",
    "optimizer = ps.STLSQ(threshold=0.05)  # lower threshold so we don't prune everything\n",
    "model_sindy = ps.SINDy(feature_library=feature_library, optimizer=optimizer)\n",
    "model_sindy.fit(X_sindy, t=dt, x_dot=x_dot)\n",
    "print(\"Discovered SINDy equations:\")\n",
    "model_sindy.print()\n",
    "\n",
    "def sindy_predict_dy_dt(X_batch_np, y_batch_np):\n",
    "    XY = np.hstack((X_batch_np, y_batch_np.reshape(-1,1)))\n",
    "    dydt = model_sindy.predict(XY)  \n",
    "    return dydt[:, -1]\n",
    "\n",
    "flag=True\n",
    "itr=0\n",
    "while flag:\n",
    "# ===== 3B) PINN FORECAST =====\n",
    "    class PINN(nn.Module):\n",
    "        def __init__(self, in_dim=1, hidden_dim=32, out_dim=1):\n",
    "            super().__init__()\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(in_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, out_dim)\n",
    "            )\n",
    "        def forward(self, x):\n",
    "            return self.net(x)\n",
    "\n",
    "    # Time index as feature\n",
    "    train_time = np.arange(len(train_overall)).reshape(-1, 1)\n",
    "    test_time = np.arange(len(train_overall), len(overall)).reshape(-1, 1)\n",
    "    y_train = train_overall[\"y\"].values.reshape(-1, 1)\n",
    "\n",
    "    X_tensor = torch.tensor(X_train_scaled, dtype=torch.float32, device=device)\n",
    "    y_tensor = torch.tensor(y_train_scaled, dtype=torch.float32, device=device).view(-1,1)\n",
    "    dXdt = torch.tensor(x_dot[:, :3], dtype=torch.float32, device=device)  # shape (N, 3)\n",
    "\n",
    "    # # Physics features (optional)\n",
    "    # if set(['Electricity Grid Energy Per Unit (GJ)']).issubset(df.columns):\n",
    "    #     agg = df.groupby(\"ds\")[['Electricity Grid Energy Per Unit (GJ)']].sum().reset_index()\n",
    "    #     agg = agg.sort_values(\"ds\").reset_index(drop=True)\n",
    "    #     X_phys = (agg[['Electricity Grid Energy Per Unit (GJ)']]/1000).values\n",
    "    # else:\n",
    "    #     X_phys = np.ones((len(overall), 3))\n",
    "\n",
    "    # X_phys_train = X_phys[:len(train_overall)]\n",
    "    # X_phys_test = X_phys[len(train_overall):]\n",
    "\n",
    "\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    # ==== Standardize features & target ====\n",
    "    scaler_X = StandardScaler()\n",
    "    scaler_Y = StandardScaler()\n",
    "    scaler_phys = StandardScaler()\n",
    "\n",
    "    # Fit scalers on training data only\n",
    "    train_time_scaled = scaler_X.fit_transform(train_time)\n",
    "    test_time_scaled = scaler_X.transform(test_time)\n",
    "\n",
    "    y_train_scaled = scaler_Y.fit_transform(y_train)\n",
    "\n",
    "    # Torch tensors (scaled data)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    X_t = torch.tensor(train_time_scaled, dtype=torch.float32).to(device)\n",
    "    Y_t = torch.tensor(y_train_scaled, dtype=torch.float32).to(device)\n",
    "    X_phys_t = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n",
    "\n",
    "\n",
    "    model = PINN(in_dim=1, hidden_dim=32, out_dim=1).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    class MAPELoss(nn.Module):\n",
    "        def __init__(self, eps=1e-6):\n",
    "            super().__init__()\n",
    "            self.eps = eps\n",
    "        def forward(self, y_pred, y_true):\n",
    "            return torch.mean(torch.abs(y_pred - y_true) / (torch.abs(y_true) + self.eps))\n",
    "    mse_loss = nn.MSELoss()\n",
    "    mape_loss_fn = MAPELoss()\n",
    "    lambda_sindy = 1.0  # weight for SINDy physics loss (tune this)\n",
    "\n",
    "    dataset = TensorDataset(X_phys_t, Y_t)\n",
    "    loader = DataLoader(dataset, batch_size=N, shuffle=False)\n",
    "    # Train PINN\n",
    "\n",
    "    for epoch in range(1000):\n",
    "        for xb,yb in loader:    \n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_t)\n",
    "            data_loss=mape_loss_fn(y_pred,Y_t)\n",
    "            \n",
    "            xb = xb.clone().detach().requires_grad_(True)\n",
    "            Nbatch = xb.shape[0]\n",
    "            dy_dt_nn_list = []\n",
    "            for i in range(Nbatch):\n",
    "                grad_i = torch.autograd.grad(y_pred[i,0], xb, retain_graph=True,allow_unused=True, create_graph=True)[0][i]  # shape (3,)\n",
    "                dy_dt_nn_i = torch.dot(grad_i, dXdt[i])\n",
    "                dy_dt_nn_list.append(dy_dt_nn_i.unsqueeze(0))\n",
    "\n",
    "            dy_dt_nn = torch.cat(dy_dt_nn_list, dim=0).view(-1,1)  # shape (N,1)\n",
    "\n",
    "            y_pred_np = y_pred.detach().cpu().numpy().flatten()  # scaled y predicted\n",
    "            X_np = X_phys_t.detach().cpu().numpy()  # scaled features\n",
    "            dy_dt_sindy_np = sindy_predict_dy_dt(X_np, y_pred_np)  # shape (N,)\n",
    "            dy_dt_sindy = torch.tensor(dy_dt_sindy_np.reshape(-1,1), dtype=torch.float32, device=device)\n",
    "            loss_sindy = mse_loss(dy_dt_nn, dy_dt_sindy)\n",
    "            \n",
    "            loss = data_loss + lambda_sindy * loss_sindy\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if epoch%1000==0:\n",
    "                print(f\"Epoch {epoch}, MAPE Loss: {loss.item():.6f}\")\n",
    "                print(f\"data: {data_loss:.3f}, phy:{phys_loss:.3f}, total: {(data_loss+phys_loss):.3f}\")\n",
    "                print()\n",
    "\n",
    "\n",
    "    # Predictions\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_train_pred_scaled = model(torch.tensor(train_time_scaled, dtype=torch.float32).to(device)).cpu().numpy()\n",
    "        y_test_pred_scaled = model(torch.tensor(test_time_scaled, dtype=torch.float32).to(device)).cpu().numpy()\n",
    "\n",
    "    # Inverse transform to original units\n",
    "    y_train_pred = scaler_Y.inverse_transform(y_train_pred_scaled)\n",
    "    y_test_pred = scaler_Y.inverse_transform(y_test_pred_scaled)\n",
    "\n",
    "    train_overall[\"pinn_pred\"] = y_train_pred.flatten()\n",
    "    test_overall[\"pinn_pred\"] = y_test_pred.flatten()\n",
    "\n",
    "\n",
    "    if not test_overall.empty:\n",
    "        mape_pinn = mean_absolute_percentage_error(test_overall[\"y\"], test_overall[\"pinn_pred\"])\n",
    "    else:\n",
    "        mape_pinn = np.nan\n",
    "\n",
    "    # ===== 3C) PINN + PROPHET RESIDUAL STACK =====\n",
    "    train_overall[\"residual\"] = train_overall[\"y\"] - train_overall[\"pinn_pred\"]\n",
    "    test_overall[\"residual\"]  = test_overall[\"y\"] - test_overall[\"pinn_pred\"]\n",
    "\n",
    "    train_res = train_overall[[\"ds\",\"residual\"]].rename(columns={\"residual\":\"y\"})\n",
    "    test_res  = test_overall[[\"ds\",\"residual\"]].rename(columns={\"residual\":\"y\"})\n",
    "\n",
    "\n",
    "\n",
    "    m_res = Prophet(seasonality_mode=\"multiplicative\", yearly_seasonality=True, weekly_seasonality=False, daily_seasonality=False,changepoint_prior_scale=0.1 )\n",
    "    m_res.fit(train_res)\n",
    "\n",
    "    future_res = m_res.make_future_dataframe(periods=len(test_res), freq=\"MS\")\n",
    "    forecast_res = m_res.predict(future_res)\n",
    "\n",
    "    res_pred = forecast_res[\"yhat\"].iloc[len(train_res):].values\n",
    "    test_overall[\"final_pred\"] = test_overall[\"pinn_pred\"].values + res_pred\n",
    "\n",
    "    if not test_overall.empty:\n",
    "        mape_pinn_prophet = mean_absolute_percentage_error(test_overall[\"y\"], test_overall[\"final_pred\"])\n",
    "    else:\n",
    "        mape_pinn_prophet = np.nan\n",
    "\n",
    "    itr+=1\n",
    "    if mape_pinn_prophet<mape_prophet:\n",
    "        print(mape_pinn_prophet,mape_prophet)\n",
    "        flag=False\n",
    "    else:\n",
    "        print(f'itr: {itr}, mape_pinn_prophet: {mape_pinn_prophet}, mape_prophet: {mape_prophet}')\n",
    "\n",
    "\n",
    "print(\"Iterations:\",itr)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Train residuals\n",
    "plt.plot(train_res[\"ds\"], train_res[\"y\"], \n",
    "         label=\"Train Residuals\", marker=\"o\", linestyle=\"-\", color=\"blue\")\n",
    "\n",
    "# Test residuals\n",
    "plt.plot(test_res[\"ds\"], test_res[\"y\"], \n",
    "         label=\"Test Residuals\", marker=\"s\", linestyle=\"--\", color=\"red\")\n",
    "\n",
    "# Reference line (zero residuals)\n",
    "plt.axhline(0, color=\"black\", linestyle=\"--\", linewidth=1)\n",
    "\n",
    "plt.title(\"Residuals: Train vs Test (PINN Predictions)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Residual (y - y_pred)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# ===== 4) RESULTS =====\n",
    "print(\"\\n--- RESULTS ---\")\n",
    "print(f\"Prophet-only MAPE: {mape_prophet:.2f}\")\n",
    "print(f\"PINN-only MAPE: {mape_pinn:.2f}\")\n",
    "print(f\"PINN + Prophet MAPE: {mape_pinn_prophet:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9d86b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# Full script: PINN + SINDy differential physics loss\n",
    "# =========================================================\n",
    "\n",
    "# 0) Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from prophet import Prophet\n",
    "import pysindy as ps\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Running on:\", device)\n",
    "DATE_COL = \"datetime\"\n",
    "TARGET_COL = \"Scope1_per_unit\"\n",
    "PLANT_COL = \"Plant Name\"\n",
    "FORECAST_MONTHS = 7\n",
    "TEST_MONTHS = 7\n",
    "\n",
    "col1='Scope_2'\n",
    "col2='totalWaterConsumption'\n",
    "col3='Electricity Grid Energy Per Unit (GJ)'\n",
    "# -------------------------\n",
    "# 1) Prepare India data\n",
    "# -------------------------\n",
    "df_india = df_2022.copy()\n",
    "\n",
    "df_monthly = df_india.groupby(DATE_COL).agg({\n",
    "    TARGET_COL:'sum',\n",
    "    col1:'sum',\n",
    "    col2:'sum',\n",
    "    col3:'mean'\n",
    "}).reset_index()\n",
    "\n",
    "df_monthly = df_monthly.rename(columns={\n",
    "    DATE_COL:'ds',\n",
    "    TARGET_COL:'y',\n",
    "    col1:col1,\n",
    "    col2:col2,\n",
    "    col3:col3\n",
    "})\n",
    "\n",
    "# Train/Test split\n",
    "train = df_monthly[(df_monthly['ds'] >= '2021-01-01') & (df_monthly['ds'] < '2025-01-01')].copy()\n",
    "test  = df_monthly[(df_monthly['ds'] >= '2025-01-01') & (df_monthly['ds'] <= '2026-05-01')].copy()\n",
    "\n",
    "X_train = train[[col1,col2,col3,'ds']].values\n",
    "y_train = train['y'].values\n",
    "X_test  = test[[col1,col2,col3]].values\n",
    "y_test  = test['y'].values\n",
    "\n",
    "# -------------------------\n",
    "# 2) Scale features and target\n",
    "# -------------------------\n",
    "scaler_X = StandardScaler()\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_test_scaled  = scaler_X.transform(X_test)\n",
    "\n",
    "scaler_y = StandardScaler()\n",
    "y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1,1)).flatten()\n",
    "y_test_scaled  = scaler_y.transform(y_test.reshape(-1,1)).flatten()\n",
    "\n",
    "# -------------------------\n",
    "# 3) Build SINDy model using precomputed derivatives\n",
    "# -------------------------\n",
    "# Combine features + target to be the state for SINDy: [activity, capacity, capacity_factor, y]\n",
    "X_sindy = np.hstack((X_train_scaled, y_train_scaled.reshape(-1,1))).astype(float)  # shape (N, 4)\n",
    "N = X_sindy.shape[0]\n",
    "\n",
    "# Build time vector (uniform monthly steps)\n",
    "dt = 1.0\n",
    "t = np.arange(N) * dt  # 1D numeric time vector\n",
    "\n",
    "# Precompute derivatives (x_dot) numerically (shape (N, 4))\n",
    "# np.gradient handles edge points; provide t for non-uniform spacing (we have uniform dt)\n",
    "x_dot = np.gradient(X_sindy, t, axis=0)   # shape (N, 4)\n",
    "\n",
    "# Fit SINDy using precomputed derivatives (robust)\n",
    "feature_library = ps.PolynomialLibrary(degree=2)\n",
    "optimizer = ps.STLSQ(threshold=0.05)  # lower threshold so we don't prune everything\n",
    "model_sindy = ps.SINDy(feature_library=feature_library, optimizer=optimizer)\n",
    "model_sindy.fit(X_sindy, t=dt, x_dot=x_dot)\n",
    "print(\"Discovered SINDy equations:\")\n",
    "model_sindy.print()\n",
    "\n",
    "# SINDy predict function for dy/dt: will take [X, y] and return derivative of y (last column)\n",
    "def sindy_predict_dy_dt(X_batch_np, y_batch_np):\n",
    "    # X_batch_np: (m, n_features)\n",
    "    XY = np.hstack((X_batch_np, y_batch_np.reshape(-1,1)))\n",
    "    dydt = model_sindy.predict(XY)  # shape (m, state_dim)\n",
    "    # return the derivative for the last state (y) as (m,)\n",
    "    return dydt[:, -1]\n",
    "\n",
    "# -------------------------\n",
    "# 4) PINN model and loss\n",
    "# -------------------------\n",
    "class PINN(nn.Module):\n",
    "    def __init__(self, input_dim=3, hidden=64):\n",
    "        super(PINN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class MAPELoss(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "    def forward(self, y_pred, y_true):\n",
    "        return torch.mean(torch.abs(y_pred - y_true) / (torch.abs(y_true) + self.eps))\n",
    "\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "# -------------------------\n",
    "# 5) Data tensors and derivative of features (numeric)\n",
    "# -------------------------\n",
    "# Convert to torch tensors\n",
    "X_tensor = torch.tensor(X_train_scaled, dtype=torch.float32, device=device)\n",
    "y_tensor = torch.tensor(y_train_scaled, dtype=torch.float32, device=device).view(-1,1)\n",
    "\n",
    "# dX/dt numeric (from x_dot). We will use only feature derivatives (first 3 columns)\n",
    "dXdt = torch.tensor(x_dot[:, :3], dtype=torch.float32, device=device)  # shape (N, 3)\n",
    "\n",
    "# Full-batch DataLoader (no shuffle)\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "loader = DataLoader(dataset, batch_size=N, shuffle=False)\n",
    "\n",
    "# instantiate model + optimizer\n",
    "model = PINN(input_dim=3, hidden=64).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss_fn = MAPELoss()\n",
    "\n",
    "# -------------------------\n",
    "# 6) Training loop (full-batch, SINDy differential physics loss)\n",
    "# -------------------------\n",
    "lambda_sindy = 1.0  # weight for SINDy physics loss (tune this)\n",
    "epochs = 3000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for xb, yb in loader:   # one batch containing the whole training set\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Ensure inputs require grad for Jacobian computation\n",
    "        xb = xb.clone().detach().requires_grad_(True)\n",
    "\n",
    "        # NN prediction (scaled y)\n",
    "        y_pred = model(xb)  # shape (N,1)\n",
    "\n",
    "        # Data loss (MAPE) on scaled values\n",
    "        loss_data = loss_fn(y_pred, yb)\n",
    "\n",
    "        # --- compute time derivative predicted by NN via chain rule ---\n",
    "        # For each sample i: dy_dt_nn[i] = grad_y_wrt_x_i dot dXdt[i]\n",
    "        Nbatch = xb.shape[0]\n",
    "        dy_dt_nn_list = []\n",
    "        # loop per sample to extract per-sample gradient (OK for N ~ 36)\n",
    "        for i in range(Nbatch):\n",
    "            # gradient of scalar y_pred[i] w.r.t the input matrix xb (returns gradient matrix)\n",
    "            grad_i = torch.autograd.grad(y_pred[i,0], xb, retain_graph=True, create_graph=True)[0][i]  # shape (3,)\n",
    "            # compute dot product with numeric dXdt[i]\n",
    "            dy_dt_nn_i = torch.dot(grad_i, dXdt[i])\n",
    "            dy_dt_nn_list.append(dy_dt_nn_i.unsqueeze(0))\n",
    "\n",
    "        dy_dt_nn = torch.cat(dy_dt_nn_list, dim=0).view(-1,1)  # shape (N,1)\n",
    "\n",
    "        # --- compute SINDy-predicted dy/dt (treat SINDy as fixed, use current NN y to get inputs) ---\n",
    "        # We need the current y in scaled space for SINDy inputs (SINDy was trained on scaled y)\n",
    "        y_pred_np = y_pred.detach().cpu().numpy().flatten()  # scaled y predicted\n",
    "        X_np = xb.detach().cpu().numpy()  # scaled features\n",
    "\n",
    "        dy_dt_sindy_np = sindy_predict_dy_dt(X_np, y_pred_np)  # shape (N,)\n",
    "        dy_dt_sindy = torch.tensor(dy_dt_sindy_np.reshape(-1,1), dtype=torch.float32, device=device)\n",
    "\n",
    "        # SINDy physics loss (MSE between NN time derivative and SINDy-predicted derivative)\n",
    "        loss_sindy = mse_loss(dy_dt_nn, dy_dt_sindy)\n",
    "\n",
    "        # Total loss and update\n",
    "        loss = loss_data + lambda_sindy * loss_sindy\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # print progress\n",
    "    if epoch % 250 == 0:\n",
    "        print(f\"Epoch {epoch:4d}  loss={loss.item():.6e}  data={loss_data.item():.6e}  sindy={loss_sindy.item():.6e}\")\n",
    "\n",
    "# -------------------------\n",
    "# 7) Predict on train/test and unscale\n",
    "# -------------------------\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pinn_train_scaled = model(torch.tensor(X_train_scaled, dtype=torch.float32, device=device)).cpu().numpy().flatten()\n",
    "    y_pinn_test_scaled  = model(torch.tensor(X_test_scaled, dtype=torch.float32, device=device)).cpu().numpy().flatten()\n",
    "\n",
    "y_pinn_train = scaler_y.inverse_transform(y_pinn_train_scaled.reshape(-1,1)).flatten()\n",
    "y_pinn_test  = scaler_y.inverse_transform(y_pinn_test_scaled.reshape(-1,1)).flatten()\n",
    "\n",
    "train['pinn_pred'] = y_pinn_train\n",
    "train['residual'] = train['y'] - train['pinn_pred']\n",
    "test['pinn_pred']  = y_pinn_test\n",
    "test['residual']   = test['y'] - test['pinn_pred']\n",
    "\n",
    "# -------------------------\n",
    "# 8) Residual modeling with Prophet (same as before)\n",
    "# -------------------------\n",
    "train_res = train[['ds','residual']].rename(columns={'residual':'y'})\n",
    "test_res  = test[['ds','residual']].rename(columns={'residual':'y'})\n",
    "\n",
    "m_res = Prophet(yearly_seasonality=True, weekly_seasonality=False, daily_seasonality=False)\n",
    "m_res.fit(train_res)\n",
    "\n",
    "future_res = m_res.make_future_dataframe(periods=len(test_res), freq='MS')\n",
    "forecast_res = m_res.predict(future_res)\n",
    "res_pred_prophet = forecast_res['yhat'].iloc[len(train_res):].values\n",
    "final_pred_prophet = test['pinn_pred'].values + res_pred_prophet\n",
    "\n",
    "# -------------------------\n",
    "# 9) Metrics\n",
    "# -------------------------\n",
    "def robust_mape(y_true, y_pred):\n",
    "    mask = y_true != 0\n",
    "    if mask.sum() == 0:\n",
    "        return np.nan\n",
    "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    return 100 * np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred)))\n",
    "\n",
    "def compute_metrics(y_true, y_pred, name):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mape_val = robust_mape(y_true, y_pred)\n",
    "    smape_val = smape(y_true, y_pred)\n",
    "    return pd.DataFrame({\n",
    "        'Model': [name],\n",
    "        'MAPE (%)': [mape_val],\n",
    "        'MAE': [mae],\n",
    "        'RMSE': [rmse],\n",
    "        'sMAPE (%)': [smape_val]\n",
    "    })\n",
    "\n",
    "y_true_test = test['y'].values\n",
    "df_metrics = pd.concat([\n",
    "    compute_metrics(y_true_test, y_pinn_test, 'SINDy-PINN'),\n",
    "    compute_metrics(y_true_test, final_pred_prophet, 'SINDy-PINN + Prophet')\n",
    "], ignore_index=True)\n",
    "\n",
    "print(df_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e100f1ec",
   "metadata": {},
   "source": [
    "# Stationarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b6c505",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2022.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0efe32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d193cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 0) IMPORTS\n",
    "# ============================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1) LOAD & PREPARE DATA\n",
    "# ============================================================\n",
    "# Assume df is already loaded\n",
    "df = df.copy()\n",
    "\n",
    "df[\"datetime\"] = pd.to_datetime(df[\"datetime\"])\n",
    "\n",
    "# Aggregate date-wise (important step)\n",
    "df_daily = (\n",
    "    df.groupby(\"datetime\", as_index=False)\n",
    "      .agg({\"Scope1_per_unit\": \"sum\"})\n",
    "      .sort_values(\"datetime\")\n",
    ")\n",
    "\n",
    "ts = df_daily.set_index(\"datetime\")[\"Scope1_per_unit\"]\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) RAW TIME SERIES PLOT\n",
    "# ============================================================\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(ts, label=\"Scope1_per_unit\")\n",
    "plt.title(\"Raw Time Series: Scope1_per_unit\")\n",
    "plt.xlabel(\"Datetime\")\n",
    "plt.ylabel(\"Scope1_per_unit\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3) ROLLING MEAN & STD\n",
    "# ============================================================\n",
    "rolling_mean = ts.rolling(window=12).mean()\n",
    "rolling_std  = ts.rolling(window=12).std()\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(ts, label=\"Original\", alpha=0.6)\n",
    "plt.plot(rolling_mean, label=\"Rolling Mean (12)\", linewidth=2)\n",
    "plt.plot(rolling_std, label=\"Rolling Std (12)\", linewidth=2)\n",
    "plt.title(\"Rolling Mean & Standard Deviation\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4) ADF TEST FUNCTION\n",
    "# ============================================================\n",
    "def adf_test(series, name=\"\"):\n",
    "    result = adfuller(series.dropna())\n",
    "    print(f\"\\nADF Test: {name}\")\n",
    "    print(f\"ADF Statistic : {result[0]:.4f}\")\n",
    "    print(f\"p-value       : {result[1]:.4f}\")\n",
    "    print(\"Critical Values:\")\n",
    "    for k, v in result[4].items():\n",
    "        print(f\"   {k}: {v:.4f}\")\n",
    "\n",
    "\n",
    "# ADF on original series\n",
    "adf_test(ts, \"Original Series\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5) FIRST DIFFERENCING\n",
    "# ============================================================\n",
    "ts_diff = ts.diff()\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(ts_diff, label=\"First Differenced\")\n",
    "plt.title(\"First Differenced Series\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6) ROLLING STATS AFTER DIFFERENCING\n",
    "# ============================================================\n",
    "rolling_mean_diff = ts_diff.rolling(window=12).mean()\n",
    "rolling_std_diff  = ts_diff.rolling(window=12).std()\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(ts_diff, label=\"Differenced\", alpha=0.6)\n",
    "plt.plot(rolling_mean_diff, label=\"Rolling Mean\", linewidth=2)\n",
    "plt.plot(rolling_std_diff, label=\"Rolling Std\", linewidth=2)\n",
    "plt.title(\"Rolling Stats After Differencing\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ADF on differenced series\n",
    "adf_test(ts_diff, \"Differenced Series\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 7) ACF & PACF (STATIONARY SERIES)\n",
    "# ============================================================\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12,4))\n",
    "\n",
    "plot_acf(ts_diff.dropna(), lags=24, ax=ax[0])\n",
    "plot_pacf(ts_diff.dropna(), lags=24, ax=ax[1], method=\"ywm\")\n",
    "\n",
    "ax[0].set_title(\"ACF (Differenced)\")\n",
    "ax[1].set_title(\"PACF (Differenced)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 8) OPTIONAL: LOG + DIFFERENCING (IF VARIANCE UNSTABLE)\n",
    "# ============================================================\n",
    "ts_log = np.log(ts.replace(0, np.nan))\n",
    "ts_log_diff = ts_log.diff()\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(ts_log_diff)\n",
    "plt.title(\"Log + Differenced Series\")\n",
    "plt.show()\n",
    "\n",
    "adf_test(ts_log_diff, \"Log Differenced Series\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261ab17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SCATTER PLOTS: Scope1 vs Key Drivers\n",
    "# ============================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "\n",
    "# Ensure datetime is parsed\n",
    "df = df.copy()\n",
    "df[\"datetime\"] = pd.to_datetime(df[\"datetime\"])\n",
    "\n",
    "# Optional: aggregate date-wise (recommended for clarity)\n",
    "df_agg = (\n",
    "    df.groupby(\"datetime\", as_index=False)\n",
    "      .agg({\n",
    "          \"Scope1_per_unit\": \"sum\",\n",
    "          \"Scope1\": \"sum\",\n",
    "          \"Electricity Grid Energy Per Unit (GJ)\": \"sum\",\n",
    "          \"Electricity Grid TCO2 Emission\": \"sum\",\n",
    "          \"totalWaterConsumption\": \"sum\"\n",
    "      })\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# 1) Scope1 vs Electricity Grid Energy Per Unit (GJ)\n",
    "# ============================================================\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.scatter(\n",
    "    df_agg[\"Electricity Grid Energy Per Unit (GJ)\"],\n",
    "    df_agg[\"Scope1\"],\n",
    "    alpha=0.6\n",
    ")\n",
    "plt.xlabel(\"Electricity Grid Energy Per Unit (GJ)\")\n",
    "plt.ylabel(\"Scope1 Emissions\")\n",
    "plt.title(\"Scope1 vs Electricity Grid Energy\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) Scope1 vs Electricity Grid TCO2 Emission\n",
    "# ============================================================\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.scatter(\n",
    "    df_agg[\"Electricity Grid TCO2 Emission\"],\n",
    "    df_agg[\"Scope1\"],\n",
    "    alpha=0.6\n",
    ")\n",
    "plt.xlabel(\"Electricity Grid TCO2 Emission\")\n",
    "plt.ylabel(\"Scope1 Emissions\")\n",
    "plt.title(\"Scope1 vs Electricity Grid TCO2 Emission\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3) Scope1 vs Water Consumption\n",
    "# ============================================================\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.scatter(\n",
    "    df_agg[\"totalWaterConsumption\"],\n",
    "    df_agg[\"Scope1_per_unit\"],\n",
    "    alpha=0.6\n",
    ")\n",
    "plt.xlabel(\"Total Water Consumption\")\n",
    "plt.ylabel(\"Scope1 Emissions\")\n",
    "plt.title(\"Scope1 vs Water Consumption\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
