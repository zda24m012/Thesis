{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89c60475",
   "metadata": {},
   "source": [
    "# Data Load, Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffb4cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData():    \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    filepath='combined_emissions_sources2.csv'\n",
    "        # Step 1: Load the CSV file into a pandas DataFrame\n",
    "    try:\n",
    "        print(f\"Loading data from '{filepath}'...\")\n",
    "        df = pd.read_csv(filepath)\n",
    "        print(\"Data loaded successfully.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{filepath}' was not found.\")\n",
    "        print(\"Please make sure you have already run the first script to generate this file,\")\n",
    "        print(\"and that it is in the same directory as this script.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the file: {e}\")\n",
    "    # Step 2: Display the size of the DataFrame (rows, columns)\n",
    "    rows, cols = df.shape\n",
    "    print(\"\\n--- DataFrame Size ---\")\n",
    "    print(f\"The DataFrame has {rows} rows and {cols} columns.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa41921e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=loadData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e1b91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['start_time'])\n",
    "# --- 2. Identify and Drop Columns with Nulls > 100,000 ---\n",
    "threshold = 100000\n",
    "null_counts = df.isnull().sum()\n",
    "columns_to_drop = null_counts[null_counts > threshold].index.tolist()\n",
    "\n",
    "if not columns_to_drop:\n",
    "    print(\"No columns have more than 100,000 null values. No columns were dropped.\")\n",
    "else:\n",
    "\n",
    "    print(f\"--- Columns to be Dropped (>{threshold} nulls) ---\")\n",
    "    print(columns_to_drop)\n",
    "    print(\"\\n--- Columns Dropped ---\")\n",
    "    print(f\"Old shape of DataFrame: {df.shape}\")\n",
    "    df.drop(columns=columns_to_drop, axis=1, inplace=True)\n",
    "    print(f\"New shape of the DataFrame: {df.shape}\")\n",
    "\n",
    "columns_to_drop=['modified_date','source_id','iso3_country']\n",
    "df.drop(columns=columns_to_drop, axis=1, inplace=True)\n",
    "print(df.columns)\n",
    "\n",
    "\n",
    "\n",
    "# --- 2. Find and Display Null Value Counts ---\n",
    "null_counts = df.isnull().sum()\n",
    "df_imputed = df.copy()\n",
    "sector_null_count = df_imputed['sector'].isnull().sum()\n",
    "if sector_null_count > 0:\n",
    "    df_imputed.dropna(subset=['sector'], inplace=True)\n",
    "\n",
    "print(\"--- Imputing Missing Values ---\")\n",
    "# Loop through each column to apply the correct imputation strategy\n",
    "for col in df_imputed.columns:\n",
    "    if df_imputed[col].isnull().any():\n",
    "        # STRATEGY 1: For non-numeric (object/categorical) columns\n",
    "        if df_imputed[col].dtype == 'object':\n",
    "            mode_value = df_imputed[col].mode()[0]\n",
    "            df_imputed[col].fillna(mode_value, inplace=True)\n",
    "            print(f\"Imputed non-numeric column '{col}' with mode: '{mode_value}'\")\n",
    "            \n",
    "        # STRATEGY 2: For numeric (float/int) columns\n",
    "        else:\n",
    "            mean_value = df_imputed[col].mean()\n",
    "            df_imputed[col].fillna(mean_value, inplace=True)\n",
    "            print(f\"Imputed numeric column '{col}' with mean: {mean_value:.2f}\")\n",
    "\n",
    "print(\"\\nImputation complete.\")\n",
    "print(\"\\n--- Null values count after imputation ---\")\n",
    "display(df_imputed.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dc4f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Display unique values from the 'source_name' column\n",
    "print(\"\\n--- Unique Values in 'sectors' ---\")\n",
    "if 'sector' in df_imputed.columns:\n",
    "    unique_sectors = df_imputed['sector'].unique()\n",
    "    print(f\"Found {len(unique_sectors)} unique source names. Displaying a sample:\")\n",
    "    # Display the first 15 unique names, or all if less than 15\n",
    "    display_limit = min(15, len(unique_sectors))\n",
    "    for i, name in enumerate(unique_sectors[:display_limit]):\n",
    "        print(f\"- {name}\")\n",
    "    if len(unique_sectors) > display_limit:\n",
    "        print(f\"... and {len(unique_sectors) - display_limit} more.\")\n",
    "else:\n",
    "    print(\"The column 'sector' was not found in the DataFrame.\")\n",
    "\n",
    "all_null_rows_count = df_imputed.isnull().all(axis=1).sum()\n",
    "print(f\"Contains {all_null_rows_count} rows where all values are null.\")\n",
    "print(f\"The shape of current cleaned df: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08e0fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def impute_categorical_with_percentiles(df, columns):\n",
    "    label_to_quantile = {\n",
    "        'very high': 0.95,\n",
    "        'high': 0.90,\n",
    "        'medium': 0.65,\n",
    "        'low': 0.45,\n",
    "        'very low': 0.35,\n",
    "    }\n",
    "    \n",
    "    for col in columns:\n",
    "        if col not in df.columns:\n",
    "            print(f\"Warning: column '{col}' not found. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        s = df[col].astype(str)\n",
    "        numeric_series = pd.to_numeric(df[col], errors='coerce')\n",
    "        numeric_only = numeric_series.dropna()\n",
    "\n",
    "        if numeric_only.empty:\n",
    "            print(f\"Warning: column '{col}' has no numeric data to compute percentiles. Skipping.\")\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            continue\n",
    "\n",
    "        qvals = numeric_only.quantile([\n",
    "            label_to_quantile['very high'],\n",
    "            label_to_quantile['high'],\n",
    "            label_to_quantile['medium'],\n",
    "            label_to_quantile['low'],\n",
    "            label_to_quantile['very low']\n",
    "        ])\n",
    "        # Map quantile index back to labels\n",
    "        pmap = {\n",
    "            'very high': qvals.loc[label_to_quantile['very high']],\n",
    "            'high': qvals.loc[label_to_quantile['high']],\n",
    "            'medium': qvals.loc[label_to_quantile['medium']],\n",
    "            'low': qvals.loc[label_to_quantile['low']],\n",
    "            'very low': qvals.loc[label_to_quantile['very low']],\n",
    "        }\n",
    "\n",
    "        s_clean = s.str.strip().str.lower().replace(pmap)\n",
    "        df[col] = pd.to_numeric(s_clean, errors='coerce')\n",
    "\n",
    "        # Report\n",
    "        print(f\"Processed '{col}': converted to float. Percentiles used: \"\n",
    "              f\"very high={pmap['very high']:.6g}, high={pmap['high']:.6g}, \"\n",
    "              f\"medium={pmap['medium']:.6g}, low={pmap['low']:.6g}, very low={pmap['very low']:.6g}\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80c438a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_fix = ['capacity','capacity_factor','activity','emissions_factor','emissions_quantity']\n",
    "\n",
    "impute_categorical_with_percentiles(df_imputed, cols_to_fix)\n",
    "for c in cols_to_fix:\n",
    "    if c in df_imputed.columns:\n",
    "        print(c, df_imputed[c].dtype, \"-> sample:\", df_imputed[c].dropna().head().tolist())\n",
    "\n",
    "print(\"\\n--- Time Clean Ups ---\\n\")\n",
    "import pandas as pd\n",
    "df_imputed['start_time'] = pd.to_datetime(df_imputed['start_time'], infer_datetime_format=True)\n",
    "df_imputed['end_time']   = pd.to_datetime(df_imputed['end_time'], infer_datetime_format=True)\n",
    "\n",
    "# Optional: verify conversion\n",
    "print(df_imputed[['start_time', 'end_time']].dtypes)\n",
    "print(df_imputed[['start_time', 'end_time']].head())\n",
    "\n",
    "# Add time features\n",
    "df_imputed['year'] = df_imputed['start_time'].dt.year\n",
    "df_imputed['month'] = df_imputed['start_time'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f1305a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "null_counts = df_imputed.isnull().sum()\n",
    "print(null_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30da81e",
   "metadata": {},
   "source": [
    "# Data Cleaning Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1a0fb1",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 1. Data Consolidation & Cleaning\n",
    "- **Consolidation**\n",
    "  - Collected emissions data from multiple sector folders.  \n",
    "  - Merged into a single unified dataset.  \n",
    "- **Cleaning**\n",
    "  - Parsed datetime fields: `start_time`, `end_time`.  \n",
    "  - Imputed missing values:  \n",
    "    - **Numeric values** → replaced with column means.  \n",
    "    - **Categorical strings** (e.g., *“very high”, “low”*) → substituted with percentile-based numeric values.  \n",
    "  - Converted mixed-type numeric columns → coerced to `float`.  \n",
    "  - Dropped columns with excessive nulls.  \n",
    "  - Removed rows where `sector` was null.  \n",
    "\n",
    "- **Cleaned data size**\n",
    "  - `(4527140, 9)`\n",
    "  - 2021-2025 (May)\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Feature Engineering\n",
    "- **Time Features**\n",
    "  - Extracted calendar components: year, month, quarter, month start/end.  \n",
    "  - Added **cyclical encodings** for month (sine/cosine representation).  \n",
    "- **Autoregressive Lags**\n",
    "  - Created lag features for 1, 2, 3, 6, and 12 months.  \n",
    "- **Aggregation**\n",
    "  - Standardized the dataset into a consistent **monthly time series**.  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effc36fd",
   "metadata": {},
   "source": [
    "# EDA Timeline, Sector Data Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ba0c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "plt.style.use(\"seaborn-v0_8\")  # updated style name\n",
    "sns.set_palette(\"viridis\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def sectorWise(df_sector):\n",
    "\n",
    "    # =========================================================\n",
    "    # 1) Build monthly series from df_imputed (NO 'ds','y' assumed)\n",
    "    # =========================================================\n",
    "    # Aggregate total emissions by month\n",
    "    df_monthly = (\n",
    "        df_sector.groupby(['year', 'month'])['emissions_quantity']\n",
    "        .sum()\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Create month-start datetime column 'ds'\n",
    "    df_monthly['ds'] = pd.to_datetime(\n",
    "        df_monthly['year'].astype(str) + '-' + df_monthly['month'].astype(str) + '-01'\n",
    "    ).dt.to_period('M').dt.to_timestamp(how='start')  # enforce month-start timestamps\n",
    "\n",
    "\n",
    "    # Prophet expects columns: ds (date), y (target)\n",
    "    df_monthly = df_monthly.rename(columns={'emissions_quantity': 'y'})\n",
    "    df_monthly = df_monthly[['ds', 'y']].sort_values('ds').reset_index(drop=True)\n",
    "\n",
    "    # =========================================================\n",
    "    # 2) Train/Test split (Train: 2021-01 → 2023-12, Test: 2024-01 → 2025-05)\n",
    "    # =========================================================\n",
    "    train = df_monthly[(df_monthly['ds'] >= '2021-01-01') & (df_monthly['ds'] < '2024-01-01')]\n",
    "    test  = df_monthly[(df_monthly['ds'] >= '2024-01-01') & (df_monthly['ds'] <= '2025-05-01')]\n",
    "\n",
    "    print(f\"Train size: {len(train)} | range: {train['ds'].min().date()} → {train['ds'].max().date()}\")\n",
    "    print(f\"Test  size: {len(test)} | range: {test['ds'].min().date()} → {test['ds'].max().date()}\")\n",
    "\n",
    "    # =========================================================\n",
    "    # 3) Fit Prophet on training data\n",
    "    # =========================================================\n",
    "    m = Prophet(yearly_seasonality=True, weekly_seasonality=False, daily_seasonality=False)\n",
    "    m.fit(train)\n",
    "\n",
    "    # =========================================================\n",
    "    # 4) Create futures\n",
    "    #    - For evaluation/plots, we want a full forecast that includes history.\n",
    "    #    - For metrics, we align exactly to test months to avoid any NaN/mismatch.\n",
    "    # =========================================================\n",
    "    # Full forecast through the end of test period for nice Prophet plots\n",
    "    # (periods = number of months from end of train to 2025-05 inclusive)\n",
    "    last_needed = pd.Timestamp('2025-05-01')\n",
    "    months_needed = (last_needed.to_period('M') - train['ds'].max().to_period('M')).n + 1\n",
    "    future_full = m.make_future_dataframe(periods=months_needed, freq='MS')\n",
    "    forecast_full = m.predict(future_full)\n",
    "\n",
    "    # For strict evaluation: forecast exactly on test dates\n",
    "    future_test = pd.DataFrame({'ds': test['ds']})\n",
    "    forecast_test_only = m.predict(future_test)[['ds', 'yhat']]\n",
    "\n",
    "    # Alignment check (should be zero missing)\n",
    "    missing_in_forecast = sorted(set(test['ds']) - set(forecast_test_only['ds']))\n",
    "    if missing_in_forecast:\n",
    "        print(\"Warning: these test dates are missing predictions:\", missing_in_forecast)\n",
    "\n",
    "    # =========================================================\n",
    "    # 5) Evaluation (MAE, RMSE, MAPE, sMAPE) on 2024-01 → 2025-05\n",
    "    # =========================================================\n",
    "    eval_df = test.merge(forecast_test_only, on='ds', how='left').copy()\n",
    "\n",
    "    # Safety check for any NaNs (should not happen; if it does, we surface it)\n",
    "    if eval_df['yhat'].isna().any() or eval_df['y'].isna().any():\n",
    "        n_nan_pred = eval_df['yhat'].isna().sum()\n",
    "        n_nan_true = eval_df['y'].isna().sum()\n",
    "        raise ValueError(f\"Found NaNs after alignment -> yhat NaNs: {n_nan_pred}, y NaNs: {n_nan_true}. \"\n",
    "                        \"Check the monthly continuity or date alignment.\")\n",
    "\n",
    "    y_true = eval_df['y'].to_numpy()\n",
    "    y_pred = eval_df['yhat'].to_numpy()\n",
    "\n",
    "    mae  = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "    # Robust MAPE: ignore zero-true months in the percentage calc\n",
    "    nonzero_mask = y_true != 0\n",
    "    if nonzero_mask.sum() == 0:\n",
    "        mape = np.nan\n",
    "    else:\n",
    "        mape = np.mean(np.abs((y_true[nonzero_mask] - y_pred[nonzero_mask]) / y_true[nonzero_mask])) * 100\n",
    "\n",
    "    # sMAPE handles zeros better\n",
    "    smape = 100 * np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred)))\n",
    "\n",
    "    print(\"\\nEvaluation (Test: 2024-01 → 2025-05)\")\n",
    "    print(f\"MAE   : {mae:.3f}\")\n",
    "    print(f\"RMSE  : {rmse:.3f}\")\n",
    "    print(f\"MAPE  : {mape:.3f}% (computed on non-zero actuals only)\")\n",
    "    print(f\"sMAPE : {smape:.3f}%\")\n",
    "\n",
    "    # =========================================================\n",
    "    # 6) Plots — keep EVERYTHING\n",
    "    # =========================================================\n",
    "\n",
    "    # A) Forecast vs Actual (Train/Test + Forecast-on-test)\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.plot(train['ds'], train['y'], label=\"Train\", linewidth=2)\n",
    "    plt.plot(test['ds'], test['y'], label=\"Test (Actual)\", linewidth=2)\n",
    "    plt.plot(eval_df['ds'], eval_df['yhat'], '--', label=\"Forecast on Test\", linewidth=2)\n",
    "    plt.axvline(pd.Timestamp('2024-01-01'), color='gray', linestyle='--', label=\"Train/Test Split\")\n",
    "    plt.title(f\"Prophet Forecast vs Actual (Train: 2021–2023, Test: 2024–2025-05),{df_sector['sector'].unique()}\")\n",
    "    plt.xlabel(\"Date\"); plt.ylabel(\"Total Emissions\"); plt.legend(); plt.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "    return mape\n",
    "    # # B) Prophet Forecast (full range) — nice overview figure\n",
    "    # fig_forecast = m.plot(forecast_full)\n",
    "    # plt.title(\"Prophet Forecast (Full Range Through 2025-05)\")\n",
    "    # plt.axvline(pd.Timestamp('2024-01-01'), color='gray', linestyle='--', label=\"Forecast Start\")\n",
    "    # plt.legend(); plt.grid(alpha=0.3)\n",
    "    # plt.show()\n",
    "    # # Plot forecast with uncertainty intervals + actual test points\n",
    "    # plt.figure(figsize=(12, 6))\n",
    "    # plt.plot(train[\"ds\"], train[\"y\"], label=\"Train\", color=\"blue\")\n",
    "    # plt.plot(test[\"ds\"], test[\"y\"], label=\"Test (actual)\", color=\"black\", linestyle=\"dashed\")\n",
    "    # plt.plot(forecast_full[\"ds\"], forecast_full[\"yhat\"], label=\"Forecast\", color=\"red\")\n",
    "    # plt.fill_between(forecast_full[\"ds\"], forecast_full[\"yhat_lower\"], forecast_full[\"yhat_upper\"], color=\"pink\", alpha=0.3)\n",
    "\n",
    "    # # Add test points as scatter dots\n",
    "    # plt.scatter(test[\"ds\"], test[\"y\"], color=\"black\", marker=\"o\", s=40, label=\"Test points\")\n",
    "\n",
    "    # plt.title(\"Forecast vs Actuals (with Test Points)\")\n",
    "    # plt.xlabel(\"Date\")\n",
    "    # plt.ylabel(\"y\")\n",
    "    # plt.legend()\n",
    "    # plt.show()\n",
    "\n",
    "    # # C) Prophet Components (Trend + Seasonality) — make easier to read\n",
    "    # fig_comp = m.plot_components(forecast_full)\n",
    "    # fig_comp.set_size_inches(12, 8)\n",
    "    # for ax in fig_comp.axes:\n",
    "    #     ax.grid(alpha=0.3)\n",
    "    #     ax.set_ylabel(\"Effect\")\n",
    "    # plt.suptitle(\"Trend & Yearly Seasonality Effects\", fontsize=16)\n",
    "    # plt.show()\n",
    "\n",
    "    # # D) Residuals over time (Test period)\n",
    "    # residuals = y_true - y_pred\n",
    "    # plt.figure(figsize=(10,5))\n",
    "    # plt.plot(eval_df['ds'], residuals, marker='o')\n",
    "    # plt.axhline(0, color='red', linestyle='--')\n",
    "    # plt.title(\"Residuals Over Time (Test: 2024–2025-05)\")\n",
    "    # plt.xlabel(\"Date\"); plt.ylabel(\"Residual (Actual - Predicted)\"); plt.grid(alpha=0.3)\n",
    "    # plt.show()\n",
    "\n",
    "    # # E) Correlation Heatmap for numeric columns in df_imputed (as requested earlier)\n",
    "    # num_cols = df_imputed.select_dtypes(include=[np.number]).columns\n",
    "    # if len(num_cols) > 1:\n",
    "    #     plt.figure(figsize=(10,7))\n",
    "    #     corr = df_imputed[num_cols].corr()\n",
    "    #     sns.heatmap(corr, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "    #     plt.title(\"Correlation Heatmap (Numerical Features)\")\n",
    "    #     plt.show()\n",
    "\n",
    "    # # Actual vs Predicted on historical data\n",
    "    # plt.figure(figsize=(10,6))\n",
    "    # plt.plot(eval_df['ds'], eval_df['y'], label=\"Actual\", marker='o')\n",
    "    # plt.plot(eval_df['ds'], eval_df['yhat'], label=\"Predicted\", marker='x')\n",
    "    # plt.title(\"Prophet: Actual vs Predicted \")\n",
    "    # plt.xlabel(\"Date\")\n",
    "    # plt.ylabel(\"Total Emissions\")\n",
    "    # plt.legend()\n",
    "    # plt.grid(alpha=0.3)\n",
    "    # plt.show()\n",
    "\n",
    "def timeline(df):\n",
    "    monthly = df.groupby(['year','month'])['emissions_quantity'].sum().reset_index()\n",
    "    monthly['date'] = pd.to_datetime(monthly[['year','month']].assign(day=1))\n",
    "\n",
    "    plt.figure(figsize=(14,6))\n",
    "    sns.lineplot(data=monthly, x=\"date\", y=\"emissions_quantity\")\n",
    "    plt.title(\"Emissions Over Time\")\n",
    "    plt.ylabel(\"Total Monthly Emissions\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e099b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sector-wise over time (Top 5)\n",
    "top_sectors1 = ['agriculture','forestry-and-land-use','power','fossil-fuel-operations']\n",
    "top_sectors2 = ['buildings','manufacturing','transportation']\n",
    "top_sectors3=['mineral-extraction','waste']\n",
    "\n",
    "def timelineCombo(top_sectors):\n",
    "    sector_trend = (df_imputed[df_imputed['sector'].isin(top_sectors)]\n",
    "                    .groupby(['year','month','sector'])['emissions_quantity']\n",
    "                    .sum()\n",
    "                    .reset_index())\n",
    "    sector_trend['date'] = pd.to_datetime(sector_trend[['year','month']].assign(day=1))\n",
    "\n",
    "    plt.figure(figsize=(14,8))\n",
    "    sns.lineplot(data=sector_trend, x=\"date\", y=\"emissions_quantity\", hue=\"sector\")\n",
    "    plt.title(f\"Sector-wise Emissions Over Time: {top_sectors}\")\n",
    "    plt.show()\n",
    "\n",
    "timelineCombo(top_sectors1)\n",
    "timelineCombo(top_sectors2)\n",
    "timelineCombo(top_sectors3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ee13fa",
   "metadata": {},
   "source": [
    "# Prophet Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b55aaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sectors=['waste','manufacturing','fossil-fuel-operations','transportation','power','agriculture','buildings']\n",
    "df_final=df_imputed[df_imputed['sector'].isin(sectors)]\n",
    "prophet_mape=sectorWise(df_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0f73a1",
   "metadata": {},
   "source": [
    "# Classical Methods: Prophet, Holt-Winters, ARIMA, SARIMA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464f391e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec3d85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "# =========================================================\n",
    "# 1) Train/Test split (Train: 2021-01 → 2023-12, Test: 2024-01 → 2025-05)\n",
    "# =========================================================\n",
    "df_monthly = df_final.groupby('start_time')['emissions_quantity'].sum().reset_index()\n",
    "df_monthly = df_monthly.rename(columns={'start_time':'ds','emissions_quantity':'y'})\n",
    "train = df_monthly[(df_monthly['ds'] >= '2021-01-01') & (df_monthly['ds'] < '2024-01-01')]\n",
    "test  = df_monthly[(df_monthly['ds'] >= '2024-01-01') & (df_monthly['ds'] <= '2025-05-01')]\n",
    "\n",
    "y_train = train.set_index('ds')['y']\n",
    "y_test  = test.set_index('ds')['y']\n",
    "\n",
    "print(f\"Train size: {len(train)} | range: {train['ds'].min().date()} → {train['ds'].max().date()}\")\n",
    "print(f\"Test  size: {len(test)} | range: {test['ds'].min().date()} → {test['ds'].max().date()}\")\n",
    "# =========================================================\n",
    "# 1.2) Fit Prophet on training data\n",
    "# =========================================================\n",
    "m = Prophet(yearly_seasonality=True, weekly_seasonality=False, daily_seasonality=False)\n",
    "m.fit(train)\n",
    "# =========================================================\n",
    "# 1.3) Create futures\n",
    "# =========================================================\n",
    "# (periods = number of months from end of train to 2025-05 inclusive)\n",
    "last_needed = pd.Timestamp('2025-05-01')\n",
    "months_needed = (last_needed.to_period('M') - train['ds'].max().to_period('M')).n + 1\n",
    "future_full = m.make_future_dataframe(periods=months_needed, freq='MS')\n",
    "forecast_full = m.predict(future_full)\n",
    "future_test = pd.DataFrame({'ds': test['ds']})\n",
    "forecast_test_only = m.predict(future_test)[['ds', 'yhat']]\n",
    "missing_in_forecast = sorted(set(test['ds']) - set(forecast_test_only['ds']))\n",
    "if missing_in_forecast:\n",
    "    print(\"Warning: these test dates are missing predictions:\", missing_in_forecast)\n",
    "\n",
    "eval_df = test.merge(forecast_test_only, on='ds', how='left').copy()\n",
    "if eval_df['yhat'].isna().any() or eval_df['y'].isna().any():\n",
    "    n_nan_pred = eval_df['yhat'].isna().sum()\n",
    "    n_nan_true = eval_df['y'].isna().sum()\n",
    "    raise ValueError(f\"Found NaNs after alignment -> yhat NaNs: {n_nan_pred}, y NaNs: {n_nan_true}. \"\n",
    "                    \"Check the monthly continuity or date alignment.\")\n",
    "y_true = eval_df['y'].to_numpy()\n",
    "y_pred = eval_df['yhat'].to_numpy()\n",
    "mae  = mean_absolute_error(y_true, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "# Robust MAPE: ignore zero-true months in the percentage calc\n",
    "nonzero_mask = y_true != 0\n",
    "if nonzero_mask.sum() == 0:\n",
    "    mape = np.nan\n",
    "else:\n",
    "    mape = np.mean(np.abs((y_true[nonzero_mask] - y_pred[nonzero_mask]) / y_true[nonzero_mask])) * 100\n",
    "# sMAPE handles zeros better\n",
    "smape = 100 * np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred)))\n",
    "print(\"\\nEvaluation (Test: 2024-01 → 2025-05)\")\n",
    "print(f\"MAPE  : {mape:.3f}% (computed on non-zero actuals only)\")\n",
    "\n",
    "# -------------------------\n",
    "# (2) Holt-Winters (monthly seasonality)\n",
    "# -------------------------\n",
    "hw_model = ExponentialSmoothing(y_train,\n",
    "                                trend='add',\n",
    "                                seasonal='add',\n",
    "                                seasonal_periods=12).fit()\n",
    "hw_forecast = hw_model.forecast(len(y_test))\n",
    "\n",
    "# -------------------------\n",
    "# (3) ARIMA\n",
    "# -------------------------\n",
    "arima_model = ARIMA(y_train, order=(12,1,12))  \n",
    "arima_fit = arima_model.fit()\n",
    "arima_forecast = arima_fit.forecast(len(y_test))\n",
    "\n",
    "# Seasonal ARIMA\n",
    "import pmdarima as pm\n",
    "\n",
    "sarima_model = pm.auto_arima(y_train,\n",
    "                             seasonal=True,\n",
    "                             m=12,  # 12 months in a seasonal cycle\n",
    "                             stepwise=True,\n",
    "                             suppress_warnings=True)\n",
    "sarima_forecast = sarima_model.predict(n_periods=len(y_test))\n",
    "\n",
    "sarima_mape = mean_absolute_percentage_error(y_test, sarima_forecast) * 100\n",
    "# -------------------------\n",
    "# Evaluate MAPE\n",
    "# -------------------------\n",
    "hw_mape = mean_absolute_percentage_error(y_test, hw_forecast) * 100\n",
    "arima_mape = mean_absolute_percentage_error(y_test, arima_forecast) * 100\n",
    "print(f\"Prophet MAPE: {prophet_mape:.2f}%\")\n",
    "print(f\"Holt-Winters MAPE: {hw_mape:.2f}%\")\n",
    "print(f\"ARIMA MAPE: {arima_mape:.2f}%\")\n",
    "print(f\"SARIMA MAPE: {sarima_mape:.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Plot Comparison\n",
    "# -------------------------\n",
    "\n",
    "\n",
    "plt.figure(figsize=(14,5))\n",
    "plt.plot(train['ds'], train['y'], label=\"Train\", color = 'black', linewidth=2)\n",
    "plt.plot(test['ds'], test['y'], label=\"Test (Actual)\", color='blue',linewidth=2)\n",
    "\n",
    "# Prophet forecast (assuming you have eval_df with 'ds','yhat')\n",
    "plt.plot(eval_df['ds'], eval_df['yhat'], '--', label=f\"Prophet (MAPE={prophet_mape:.2f}%)\")\n",
    "\n",
    "# Holt-Winters forecast\n",
    "plt.plot(test['ds'], hw_forecast, '--', label=f\"Holt-Winters (MAPE={hw_mape:.2f}%)\")\n",
    "\n",
    "# ARIMA forecast\n",
    "plt.plot(test['ds'], sarima_forecast, '--', label=f\"SARIMA (MAPE={sarima_mape:.2f}%)\")\n",
    "plt.plot(test['ds'], arima_forecast, '--', label=f\"ARIMA (MAPE={arima_mape:.2f}%)\")\n",
    "\n",
    "\n",
    "plt.axvline(pd.Timestamp('2024-01-01'), color='gray', linestyle='--', label=\"Train/Test Split\")\n",
    "plt.title(\"Carbon Emissions Forecast (Classical Models): Prophet, Holt-Winters, SARIMA, ARIMA \")\n",
    "plt.xlabel(\"Date\"); plt.ylabel(\"Total Emissions\")\n",
    "plt.legend(); plt.grid(alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5149bc2",
   "metadata": {},
   "source": [
    "# ML Methods RandomForest, GradientBoosting, XGBoost, SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8e713c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# ---------------------------\n",
    "# Feature Engineering\n",
    "# ---------------------------\n",
    "def create_features(df_final):\n",
    "    df_monthly = df_final.groupby('start_time')['emissions_quantity'].sum().reset_index()\n",
    "    df_monthly = df_monthly.rename(columns={'start_time':'ds','emissions_quantity':'y'})\n",
    "    df = df_monthly.copy()\n",
    "    df['year'] = df['ds'].dt.year\n",
    "    df['month'] = df['ds'].dt.month\n",
    "    df['quarter'] = df['ds'].dt.quarter\n",
    "    df['dayofyear'] = df['ds'].dt.dayofyear\n",
    "    df['sin_month'] = np.sin(2 * np.pi * df['month']/12)\n",
    "    df['cos_month'] = np.cos(2 * np.pi * df['month']/12)\n",
    "    return df\n",
    "\n",
    "df_ml = create_features(df_final)\n",
    "\n",
    "# Train/Test split (same as earlier)\n",
    "train_ml = df_ml[(df_ml['ds'] >= '2021-01-01') & (df_ml['ds'] < '2024-01-01')]\n",
    "test_ml  = df_ml[(df_ml['ds'] >= '2024-01-01') & (df_ml['ds'] <= '2025-05-01')]\n",
    "\n",
    "X_train = train_ml.drop(columns=['ds','y'])\n",
    "y_train = train_ml['y']\n",
    "X_test  = test_ml.drop(columns=['ds','y'])\n",
    "y_test  = test_ml['y']\n",
    "\n",
    "# ---------------------------\n",
    "# Machine Learning Models\n",
    "# ---------------------------\n",
    "\n",
    "models = {\n",
    "    \"RandomForest\": RandomForestRegressor(n_estimators=200, random_state=42),\n",
    "    \"GradientBoosting\": GradientBoostingRegressor(n_estimators=200, random_state=42),\n",
    "    \"XGBoost\": XGBRegressor(n_estimators=200, random_state=42),\n",
    "    \"SVR\": SVR(kernel='rbf', C=200, gamma=0.1)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------------------\n",
    "# Train ML models and store predictions\n",
    "# ---------------------------\n",
    "predictions = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    predictions[name] = y_pred\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred) * 100\n",
    "    results[name] = mape\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Plot Train, Test, and Predictions\n",
    "# ---------------------------\n",
    "plt.figure(figsize=(14,5))\n",
    "\n",
    "# Training data\n",
    "plt.plot(train_ml['ds'], y_train, label=\"Train\", color=\"black\", linewidth=2)\n",
    "\n",
    "# Test actual\n",
    "plt.plot(test_ml['ds'], y_test, label=\"Test (Actual)\", color=\"blue\", linewidth=2)\n",
    "\n",
    "# Forecasts from ML models\n",
    "for name, y_pred in predictions.items():\n",
    "    plt.plot(test_ml['ds'], y_pred, '--', label=f\"{name} (MAPE {results[name]:.2f}%)\", linewidth=2)\n",
    "\n",
    "# Vertical line for train/test split\n",
    "plt.axvline(pd.Timestamp(\"2024-01-01\"), color=\"gray\", linestyle=\"--\", label=\"Train/Test Split\")\n",
    "\n",
    "plt.title(\"Carbon Emissions Forecast (Machine Learning): RF, Gradient Boosting, XGBoost, SVR\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Total Emissions\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Compare All Models\n",
    "# ---------------------------\n",
    "print(\"\\nModel Comparison:\")\n",
    "for model, mape in results.items():\n",
    "    print(f\"{model}: {mape:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42c136c",
   "metadata": {},
   "source": [
    "# Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10af081",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa89930",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv('finalDataProphet.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15001a33",
   "metadata": {},
   "source": [
    "# PINN Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676d64b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 0) Imports\n",
    "# =========================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import xgboost as xgb\n",
    "from prophet import Prophet\n",
    "\n",
    "# =========================================================\n",
    "# 1) Prepare India data\n",
    "# =========================================================\n",
    "df_india=df_final.copy()\n",
    "\n",
    "# Aggregate monthly totals\n",
    "df_monthly = df_india.groupby('start_time').agg({\n",
    "    'emissions_quantity':'sum',\n",
    "    'activity':'sum',\n",
    "    'capacity':'sum',\n",
    "    'capacity_factor':'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# rename for clarity\n",
    "df_monthly = df_monthly.rename(columns={\n",
    "    'start_time':'ds',\n",
    "    'emissions_quantity':'y',\n",
    "    'activity':'activity',\n",
    "    'capacity':'capacity',\n",
    "    'capacity_factor':'capacity_factor'\n",
    "})\n",
    "\n",
    "# =========================================================\n",
    "# 2) Train/Test split by date\n",
    "# =========================================================\n",
    "train = df_monthly[(df_monthly['ds'] >= '2021-01-01') & (df_monthly['ds'] < '2024-01-01')].copy()\n",
    "test  = df_monthly[(df_monthly['ds'] >= '2024-01-01') & (df_monthly['ds'] <= '2025-05-01')].copy()\n",
    "\n",
    "X_train = train[['activity','capacity','capacity_factor']].values\n",
    "y_train = train['y'].values\n",
    "X_test  = test[['activity','capacity','capacity_factor']].values\n",
    "y_test  = test['y'].values\n",
    "\n",
    "# =========================================================\n",
    "# 3) Scale features and target\n",
    "# =========================================================\n",
    "scaler_X = StandardScaler()\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_test_scaled  = scaler_X.transform(X_test)\n",
    "\n",
    "scaler_y = StandardScaler()\n",
    "y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1,1)).flatten()\n",
    "y_test_scaled  = scaler_y.transform(y_test.reshape(-1,1)).flatten()\n",
    "\n",
    "# =========================================================\n",
    "# 4) Define PINN with MAPE-compatible loss\n",
    "# =========================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# =========================================================\n",
    "# PINN Definition\n",
    "# =========================================================\n",
    "class PINN(nn.Module):\n",
    "    def __init__(self, input_dim=3, hidden=64):\n",
    "        super(PINN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Differentiable MAPE loss\n",
    "class MAPELoss(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "    def forward(self, y_pred, y_true):\n",
    "        return torch.mean(torch.abs(y_pred - y_true) / (torch.abs(y_true) + self.eps))\n",
    "\n",
    "# Physics-informed residual loss\n",
    "def physics_residual_loss(y_pred, features, eps=1e-6):\n",
    "    # features assumed to have: [activity, emission_factor, capacity_factor, ...]\n",
    "    activity = features[:, 0]\n",
    "    ef = features[:, 1]\n",
    "    cf = features[:, 2]\n",
    "    physics_estimate = activity * ef * cf\n",
    "    return torch.mean((y_pred.squeeze() - physics_estimate) ** 2)\n",
    "\n",
    "# =========================================================\n",
    "# 5) Prepare PyTorch DataLoader\n",
    "# =========================================================\n",
    "X_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y_train_scaled, dtype=torch.float32).view(-1,1)\n",
    "\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Instantiate model, optimizer, loss\n",
    "model = PINN()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss_fn = MAPELoss()\n",
    "\n",
    "# =========================================================\n",
    "# 6) Train PINN\n",
    "# =========================================================\n",
    "lambda_phys = 0  # weight for physics-informed loss\n",
    "\n",
    "for epoch in range(5000):\n",
    "    for xb, yb in loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(xb)\n",
    "        # Data loss\n",
    "        loss_data = loss_fn(y_pred, yb)\n",
    "        # Physics loss (using first 3 cols as activity, ef, cf)\n",
    "        loss_phys = physics_residual_loss(y_pred, xb[:, :3])\n",
    "        # Total loss\n",
    "        loss = loss_data + lambda_phys * loss_phys\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if epoch % 500 == 0:\n",
    "        print(f\"Epoch {epoch}, MAPE Loss: {loss.item():.6f}\")\n",
    "\n",
    "# =========================================================\n",
    "# 7) PINN predictions (scaled -> original)\n",
    "# =========================================================\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pinn_train_scaled = model(torch.tensor(X_train_scaled, dtype=torch.float32)).numpy().flatten()\n",
    "    y_pinn_test_scaled  = model(torch.tensor(X_test_scaled, dtype=torch.float32)).numpy().flatten()\n",
    "\n",
    "y_pinn_train = scaler_y.inverse_transform(y_pinn_train_scaled.reshape(-1,1)).flatten()\n",
    "y_pinn_test  = scaler_y.inverse_transform(y_pinn_test_scaled.reshape(-1,1)).flatten()\n",
    "\n",
    "train['pinn_pred'] = y_pinn_train\n",
    "train['residual'] = train['y'] - train['pinn_pred']\n",
    "test['pinn_pred']  = y_pinn_test\n",
    "test['residual']   = test['y'] - test['pinn_pred']\n",
    "\n",
    "# =========================================================\n",
    "# 8A) Residual modeling with XGBoost\n",
    "# =========================================================\n",
    "# Simple lag features\n",
    "train['res_lag1'] = train['residual'].shift(1).fillna(0)\n",
    "train['res_lag2'] = train['residual'].shift(2).fillna(0)\n",
    "test['res_lag1']  = list(train['residual'].iloc[-2:]) + list(test['residual'].iloc[:-2])\n",
    "test['res_lag2']  = list(train['residual'].iloc[-1:]) + list(test['residual'].iloc[:-1])\n",
    "\n",
    "X_res_train = train[['res_lag1','res_lag2','activity','capacity','capacity_factor']]\n",
    "y_res_train = train['residual']\n",
    "X_res_test  = test[['res_lag1','res_lag2','activity','capacity','capacity_factor']]\n",
    "\n",
    "dtrain = xgb.DMatrix(X_res_train, y_res_train)\n",
    "dtest  = xgb.DMatrix(X_res_test)\n",
    "params = {'objective':'reg:squarederror','verbosity':0}\n",
    "bst = xgb.train(params, dtrain, num_boost_round=300)\n",
    "\n",
    "res_pred_xgb = bst.predict(dtest)\n",
    "final_pred_xgb = test['pinn_pred'].values + res_pred_xgb\n",
    "\n",
    "# =========================================================\n",
    "# 8B) Residual modeling with Prophet\n",
    "# =========================================================\n",
    "train_res = train[['ds','residual']].rename(columns={'residual':'y'})\n",
    "test_res  = test[['ds','residual']].rename(columns={'residual':'y'})\n",
    "\n",
    "m_res = Prophet(yearly_seasonality=True, weekly_seasonality=False, daily_seasonality=False)\n",
    "m_res.fit(train_res)\n",
    "\n",
    "future_res = m_res.make_future_dataframe(periods=len(test_res), freq='MS')\n",
    "forecast_res = m_res.predict(future_res)\n",
    "res_pred_prophet = forecast_res['yhat'].iloc[len(train_res):].values\n",
    "final_pred_prophet = test['pinn_pred'].values + res_pred_prophet\n",
    "\n",
    "# =========================================================\n",
    "# 9) Evaluation metrics\n",
    "# =========================================================\n",
    "def robust_mape(y_true, y_pred):\n",
    "    mask = y_true != 0\n",
    "    if mask.sum() == 0:\n",
    "        return np.nan\n",
    "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    return 100 * np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred)))\n",
    "\n",
    "def compute_metrics(y_true, y_pred, name):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mape_val = robust_mape(y_true, y_pred)\n",
    "    smape_val = smape(y_true, y_pred)\n",
    "    return pd.DataFrame({\n",
    "        'Model': [name],\n",
    "        'MAPE (%)': [mape_val],\n",
    "        'MAE': [mae],\n",
    "        'RMSE': [rmse],\n",
    "        'sMAPE (%)': [smape_val]\n",
    "    })\n",
    "\n",
    "y_true_test = test['y'].values\n",
    "df_metrics = pd.concat([\n",
    "    compute_metrics(y_true_test, y_pinn_test, 'PINN-only'),\n",
    "    compute_metrics(y_true_test, final_pred_xgb, 'PINN + XGBoost'),\n",
    "    compute_metrics(y_true_test, final_pred_prophet, 'PINN + Prophet')\n",
    "], ignore_index=True)\n",
    "\n",
    "print(df_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38b39e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Collect predictions and their names\n",
    "predictions = {\n",
    "    \"PINN-only\": y_pinn_test,\n",
    "    \"PINN + XGBoost\": final_pred_xgb,\n",
    "    \"PINN + Prophet\": final_pred_prophet\n",
    "}\n",
    "\n",
    "# Compute MAPE for annotation\n",
    "results = {name: robust_mape(test['y'].values, y_pred) for name, y_pred in predictions.items()}\n",
    "\n",
    "plt.figure(figsize=(14,5))\n",
    "\n",
    "# Training data\n",
    "plt.plot(train['ds'], train['y'], label=\"Train (Actual)\", color=\"black\", linewidth=2)\n",
    "\n",
    "# Test actual\n",
    "plt.plot(test['ds'], test['y'], label=\"Test (Actual)\", color=\"blue\", linewidth=2)\n",
    "\n",
    "# Forecasts from models\n",
    "for name, y_pred in predictions.items():\n",
    "    plt.plot(test['ds'], y_pred, '--', label=f\"{name} (MAPE {results[name]:.2f}%)\", linewidth=2)\n",
    "\n",
    "# Vertical line for train/test split\n",
    "plt.axvline(pd.Timestamp(\"2024-01-01\"), color=\"gray\", linestyle=\"--\", label=\"Train/Test Split\")\n",
    "\n",
    "plt.title(\"Carbon Emissions Forecast: PINN + Residual Modeling\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Monthly Emissions\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e918992f",
   "metadata": {},
   "source": [
    "# TOP 5 Models: PINN + Prophet, Prophet, XGBoost, PINN + XGBoost, PINN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463e6cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 0) Imports\n",
    "# =========================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import xgboost as xgb\n",
    "from prophet import Prophet\n",
    "\n",
    "# =========================================================\n",
    "# 1) Prepare India data\n",
    "# =========================================================\n",
    "df_india=df_final.copy()\n",
    "\n",
    "# Aggregate monthly totals\n",
    "df_monthly = df_india.groupby('start_time').agg({\n",
    "    'emissions_quantity':'sum',\n",
    "    'activity':'sum',\n",
    "    'capacity':'sum',\n",
    "    'capacity_factor':'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# rename for clarity\n",
    "df_monthly = df_monthly.rename(columns={\n",
    "    'start_time':'ds',\n",
    "    'emissions_quantity':'y',\n",
    "    'activity':'activity',\n",
    "    'capacity':'capacity',\n",
    "    'capacity_factor':'capacity_factor'\n",
    "})\n",
    "\n",
    "# =========================================================\n",
    "# 2) Train/Test split by date\n",
    "# =========================================================\n",
    "train = df_monthly[(df_monthly['ds'] >= '2021-01-01') & (df_monthly['ds'] < '2024-01-01')].copy()\n",
    "test  = df_monthly[(df_monthly['ds'] >= '2024-01-01') & (df_monthly['ds'] <= '2025-05-01')].copy()\n",
    "\n",
    "X_train = train[['activity','capacity','capacity_factor']].values\n",
    "y_train = train['y'].values\n",
    "X_test  = test[['activity','capacity','capacity_factor']].values\n",
    "y_test  = test['y'].values\n",
    "\n",
    "# =========================================================\n",
    "# 3) Scale features and target\n",
    "# =========================================================\n",
    "scaler_X = StandardScaler()\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_test_scaled  = scaler_X.transform(X_test)\n",
    "\n",
    "scaler_y = StandardScaler()\n",
    "y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1,1)).flatten()\n",
    "y_test_scaled  = scaler_y.transform(y_test.reshape(-1,1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039f6c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import xgboost as xgb\n",
    "from prophet import Prophet\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 1) PROPHET\n",
    "# =========================================================\n",
    "m = Prophet(yearly_seasonality=True, weekly_seasonality=False, daily_seasonality=False)\n",
    "m.fit(train)\n",
    "# Create futures\n",
    "last_needed = pd.Timestamp('2025-05-01')\n",
    "months_needed = (last_needed.to_period('M') - train['ds'].max().to_period('M')).n + 1\n",
    "future_full = m.make_future_dataframe(periods=months_needed, freq='MS')\n",
    "forecast_full = m.predict(future_full)\n",
    "future_test = pd.DataFrame({'ds': test['ds']})\n",
    "forecast_test_only = m.predict(future_test)[['ds', 'yhat']]\n",
    "missing_in_forecast = sorted(set(test['ds']) - set(forecast_test_only['ds']))\n",
    "if missing_in_forecast:\n",
    "    print(\"Warning: these test dates are missing predictions:\", missing_in_forecast)\n",
    "# Evaluation (MAPE) on 2024-01 → 2025-05\n",
    "eval_df = test.merge(forecast_test_only, on='ds', how='left').copy()\n",
    "if eval_df['yhat'].isna().any() or eval_df['y'].isna().any():\n",
    "    n_nan_pred = eval_df['yhat'].isna().sum()\n",
    "    n_nan_true = eval_df['y'].isna().sum()\n",
    "    raise ValueError(f\"Found NaNs after alignment -> yhat NaNs: {n_nan_pred}, y NaNs: {n_nan_true}. \"\n",
    "                    \"Check the monthly continuity or date alignment.\")\n",
    "y_true = eval_df['y'].to_numpy()\n",
    "y_pred = eval_df['yhat'].to_numpy()\n",
    "nonzero_mask = y_true != 0\n",
    "if nonzero_mask.sum() == 0:\n",
    "    mape = np.nan\n",
    "else:\n",
    "    mape = np.mean(np.abs((y_true[nonzero_mask] - y_pred[nonzero_mask]) / y_true[nonzero_mask])) * 100\n",
    "prophet_mape=mape\n",
    "print(f\"Prophet MAPE: {prophet_mape:.2f}%\")\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 2) XGBoost\n",
    "# ---------------------------\n",
    "def create_features(df_final):\n",
    "    df_monthly = df_final.groupby('start_time')['emissions_quantity'].sum().reset_index()\n",
    "    df_monthly = df_monthly.rename(columns={'start_time':'ds','emissions_quantity':'y'})\n",
    "    df = df_monthly.copy()\n",
    "    df['year'] = df['ds'].dt.year\n",
    "    df['month'] = df['ds'].dt.month\n",
    "    df['quarter'] = df['ds'].dt.quarter\n",
    "    df['dayofyear'] = df['ds'].dt.dayofyear\n",
    "    df['sin_month'] = np.sin(2 * np.pi * df['month']/12)\n",
    "    df['cos_month'] = np.cos(2 * np.pi * df['month']/12)\n",
    "    return df\n",
    "\n",
    "df_ml = create_features(df_final)\n",
    "\n",
    "# Train/Test split (same as earlier)\n",
    "train_ml = df_ml[(df_ml['ds'] >= '2021-01-01') & (df_ml['ds'] < '2024-01-01')]\n",
    "test_ml  = df_ml[(df_ml['ds'] >= '2024-01-01') & (df_ml['ds'] <= '2025-05-01')]\n",
    "\n",
    "X_train_ml = train_ml.drop(columns=['ds','y'])\n",
    "y_train_ml = train_ml['y']\n",
    "X_test_ml  = test_ml.drop(columns=['ds','y'])\n",
    "y_test_ml  = test_ml['y']\n",
    "\n",
    "models = {\n",
    "    \"XGBoost\": XGBRegressor(n_estimators=200, random_state=42),\n",
    "}\n",
    "\n",
    "resultsML = {}\n",
    "predictionsML = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_ml, y_train_ml)\n",
    "    y_pred = model.predict(X_test_ml)\n",
    "    predictionsML[name] = y_pred\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred) * 100\n",
    "    resultsML[name] = mape\n",
    "    print(f\"{name} MAPE: {resultsML[name]:.2f}%\")\n",
    "# ---------------------------\n",
    "# 3) PINN\n",
    "# ---------------------------\n",
    "# Define PINN with MAPE-compatible loss\n",
    "class PINN(nn.Module):\n",
    "    def __init__(self, input_dim=5, hidden=64):\n",
    "        super(PINN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Differentiable MAPE loss\n",
    "class MAPELoss(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "    def forward(self, y_pred, y_true):\n",
    "        return torch.mean(torch.abs(y_pred - y_true) / (torch.abs(y_true) + self.eps))\n",
    "\n",
    "# Physics-informed residual loss\n",
    "def physics_residual_loss(y_pred, features, eps=1e-6):\n",
    "    # features assumed to have: [activity, emission_factor, capacity_factor, ...]\n",
    "    activity = features[:, 0]\n",
    "    ef = features[:, 1]\n",
    "    cf = features[:, 2]\n",
    "    physics_estimate = activity * ef * cf\n",
    "    return torch.mean((y_pred.squeeze() - physics_estimate) ** 2)\n",
    "\n",
    "# =========================================================\n",
    "# 5) Prepare PyTorch DataLoader\n",
    "# =========================================================\n",
    "X_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y_train_scaled, dtype=torch.float32).view(-1,1)\n",
    "\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Instantiate model, optimizer, loss\n",
    "model = PINN()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss_fn = MAPELoss()\n",
    "\n",
    "# =========================================================\n",
    "# 6) Train PINN\n",
    "# =========================================================\n",
    "lambda_phys = 0.1  # weight for physics-informed loss\n",
    "\n",
    "for epoch in range(5000):\n",
    "    for xb, yb in loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(xb)\n",
    "        # Data loss\n",
    "        loss_data = MAPELoss(y_pred, yb)\n",
    "        # Physics loss (using first 3 cols as activity, ef, cf)\n",
    "        loss_phys = physics_residual_loss(y_pred, xb[:, :3])\n",
    "        # Total loss\n",
    "        loss = loss_data + lambda_phys * loss_phys\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # if epoch % 500 == 0:\n",
    "    #     print(f\"Epoch {epoch}, MAPE Loss: {loss.item():.6f}\")\n",
    "\n",
    "# PINN predictions (scaled -> original)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pinn_train_scaled = model(torch.tensor(X_train_scaled, dtype=torch.float32)).numpy().flatten()\n",
    "    y_pinn_test_scaled  = model(torch.tensor(X_test_scaled, dtype=torch.float32)).numpy().flatten()\n",
    "\n",
    "y_pinn_train = scaler_y.inverse_transform(y_pinn_train_scaled.reshape(-1,1)).flatten()\n",
    "y_pinn_test  = scaler_y.inverse_transform(y_pinn_test_scaled.reshape(-1,1)).flatten()\n",
    "\n",
    "train['pinn_pred'] = y_pinn_train\n",
    "train['residual'] = train['y'] - train['pinn_pred']\n",
    "test['pinn_pred']  = y_pinn_test\n",
    "test['residual']   = test['y'] - test['pinn_pred']\n",
    "\n",
    "# =========================================================\n",
    "# 3A) Residual modeling with XGBoost\n",
    "# =========================================================\n",
    "# Simple lag features\n",
    "train['res_lag1'] = train['residual'].shift(1).fillna(0)\n",
    "train['res_lag2'] = train['residual'].shift(2).fillna(0)\n",
    "test['res_lag1']  = list(train['residual'].iloc[-2:]) + list(test['residual'].iloc[:-2])\n",
    "test['res_lag2']  = list(train['residual'].iloc[-1:]) + list(test['residual'].iloc[:-1])\n",
    "\n",
    "X_res_train = train[['res_lag1','res_lag2','activity','capacity','capacity_factor']]\n",
    "y_res_train = train['residual']\n",
    "X_res_test  = test[['res_lag1','res_lag2','activity','capacity','capacity_factor']]\n",
    "\n",
    "dtrain = xgb.DMatrix(X_res_train, y_res_train)\n",
    "dtest  = xgb.DMatrix(X_res_test)\n",
    "params = {'objective':'reg:squarederror','verbosity':0}\n",
    "bst = xgb.train(params, dtrain, num_boost_round=300)\n",
    "\n",
    "res_pred_xgb = bst.predict(dtest)\n",
    "final_pred_xgb = test['pinn_pred'].values + res_pred_xgb\n",
    "\n",
    "# =========================================================\n",
    "# 3B) Residual modeling with Prophet\n",
    "# =========================================================\n",
    "train_res = train[['ds','residual']].rename(columns={'residual':'y'})\n",
    "test_res  = test[['ds','residual']].rename(columns={'residual':'y'})\n",
    "\n",
    "m_res = Prophet(yearly_seasonality=True, weekly_seasonality=False, daily_seasonality=False)\n",
    "m_res.fit(train_res)\n",
    "\n",
    "future_res = m_res.make_future_dataframe(periods=len(test_res), freq='MS')\n",
    "forecast_res = m_res.predict(future_res)\n",
    "res_pred_prophet = forecast_res['yhat'].iloc[len(train_res):].values\n",
    "final_pred_prophet = test['pinn_pred'].values + res_pred_prophet\n",
    "\n",
    "# Evaluation metrics\n",
    "def robust_mape(y_true, y_pred):\n",
    "    mask = y_true != 0\n",
    "    if mask.sum() == 0:\n",
    "        return np.nan\n",
    "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "\n",
    "def compute_metrics(y_true, y_pred, name):\n",
    "    mape_val = robust_mape(y_true, y_pred)\n",
    "    return pd.DataFrame({\n",
    "        'Model': [name],\n",
    "        'MAPE (%)': [mape_val],\n",
    "    })\n",
    "\n",
    "y_true_test = test['y'].values\n",
    "df_metrics = pd.concat([\n",
    "    compute_metrics(y_true_test, y_pinn_test, 'PINN-only'),\n",
    "    compute_metrics(y_true_test, final_pred_xgb, 'PINN + XGBoost'),\n",
    "    compute_metrics(y_true_test, final_pred_prophet, 'PINN + Prophet'),\n",
    "    pd.DataFrame({'Model':['Prophet'], 'MAPE (%)':[prophet_mape],}),\n",
    "    pd.DataFrame({'Model':['XGBoost'], 'MAPE (%)':[resultsML['XGBoost']],}),\n",
    "\n",
    "], ignore_index=True)\n",
    "print(\"\\n\",'-'*8,'RESULTS','-'*8)\n",
    "print(df_metrics)\n",
    "\n",
    "# Collect predictions and their names\n",
    "predictionsPinn = {\n",
    "    \"PINN + Prophet\": final_pred_prophet,\n",
    "    \"PINN + XGBoost\": final_pred_xgb\n",
    "}\n",
    "\n",
    "resultsPinn = {name: robust_mape(test['y'].values, y_pred) for name, y_pred in predictionsPinn.items()}\n",
    "\n",
    "# =========================================================\n",
    "# 4) PLOT\n",
    "# =========================================================\n",
    "plt.figure(figsize=(14,5))\n",
    "# Training data\n",
    "plt.plot(train['ds'], train['y'], label=\"Train (Actual)\", color=\"black\", linewidth=2)\n",
    "# Test actual\n",
    "plt.plot(test['ds'], test['y'], label=\"Test (Actual)\", color=\"blue\", linewidth=2)\n",
    "\n",
    "# Forecasts from models\n",
    "plt.plot(eval_df['ds'], eval_df['yhat'], '--', label=f\"Prophet (MAPE={prophet_mape:.2f}%)\")\n",
    "for name, y_pred in predictionsML.items():\n",
    "    plt.plot(test_ml['ds'], y_pred, '--', label=f\"{name} (MAPE {resultsML[name]:.2f}%)\", linewidth=2)\n",
    "for name, y_pred in predictionsPinn.items():\n",
    "    plt.plot(test['ds'], y_pred, '--', label=f\"{name} (MAPE {resultsPinn[name]:.2f}%)\", linewidth=2)\n",
    "\n",
    "    \n",
    "# Vertical line for train/test split\n",
    "plt.axvline(pd.Timestamp(\"2024-01-01\"), color=\"gray\", linestyle=\"--\", label=\"Train/Test Split\")\n",
    "plt.title(\"Carbon Emissions Forecast - India: Top Methods\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Total Monthly Carbon Emissions\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e89051",
   "metadata": {},
   "source": [
    "# SINDy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11badb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d92620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# Full script: PINN + SINDy differential physics loss\n",
    "# =========================================================\n",
    "\n",
    "# 0) Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from prophet import Prophet\n",
    "import pysindy as ps\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Running on:\", device)\n",
    "\n",
    "# -------------------------\n",
    "# 1) Prepare India data\n",
    "# -------------------------\n",
    "df_india = df_final.copy()\n",
    "\n",
    "df_monthly = df_india.groupby('start_time').agg({\n",
    "    'emissions_quantity':'sum',\n",
    "    'activity':'sum',\n",
    "    'capacity':'sum',\n",
    "    'capacity_factor':'mean'\n",
    "}).reset_index()\n",
    "\n",
    "df_monthly = df_monthly.rename(columns={\n",
    "    'start_time':'ds',\n",
    "    'emissions_quantity':'y',\n",
    "    'activity':'activity',\n",
    "    'capacity':'capacity',\n",
    "    'capacity_factor':'capacity_factor'\n",
    "})\n",
    "\n",
    "# Train/Test split\n",
    "train = df_monthly[(df_monthly['ds'] >= '2021-01-01') & (df_monthly['ds'] < '2024-01-01')].copy()\n",
    "test  = df_monthly[(df_monthly['ds'] >= '2024-01-01') & (df_monthly['ds'] <= '2025-05-01')].copy()\n",
    "\n",
    "X_train = train[['activity','capacity','capacity_factor']].values\n",
    "y_train = train['y'].values\n",
    "X_test  = test[['activity','capacity','capacity_factor']].values\n",
    "y_test  = test['y'].values\n",
    "\n",
    "# -------------------------\n",
    "# 2) Scale features and target\n",
    "# -------------------------\n",
    "scaler_X = StandardScaler()\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_test_scaled  = scaler_X.transform(X_test)\n",
    "\n",
    "scaler_y = StandardScaler()\n",
    "y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1,1)).flatten()\n",
    "y_test_scaled  = scaler_y.transform(y_test.reshape(-1,1)).flatten()\n",
    "\n",
    "# -------------------------\n",
    "# 3) Build SINDy model using precomputed derivatives\n",
    "# -------------------------\n",
    "# Combine features + target to be the state for SINDy: [activity, capacity, capacity_factor, y]\n",
    "X_sindy = np.hstack((X_train_scaled, y_train_scaled.reshape(-1,1))).astype(float)  # shape (N, 4)\n",
    "N = X_sindy.shape[0]\n",
    "\n",
    "# Build time vector (uniform monthly steps)\n",
    "dt = 1.0\n",
    "t = np.arange(N) * dt  # 1D numeric time vector\n",
    "\n",
    "# Precompute derivatives (x_dot) numerically (shape (N, 4))\n",
    "# np.gradient handles edge points; provide t for non-uniform spacing (we have uniform dt)\n",
    "x_dot = np.gradient(X_sindy, t, axis=0)   # shape (N, 4)\n",
    "\n",
    "# Fit SINDy using precomputed derivatives (robust)\n",
    "feature_library = ps.PolynomialLibrary(degree=2)\n",
    "optimizer = ps.STLSQ(threshold=0.05)  # lower threshold so we don't prune everything\n",
    "model_sindy = ps.SINDy(feature_library=feature_library, optimizer=optimizer)\n",
    "model_sindy.fit(X_sindy, t=dt, x_dot=x_dot)\n",
    "print(\"Discovered SINDy equations:\")\n",
    "model_sindy.print()\n",
    "\n",
    "# SINDy predict function for dy/dt: will take [X, y] and return derivative of y (last column)\n",
    "def sindy_predict_dy_dt(X_batch_np, y_batch_np):\n",
    "    # X_batch_np: (m, n_features)\n",
    "    XY = np.hstack((X_batch_np, y_batch_np.reshape(-1,1)))\n",
    "    dydt = model_sindy.predict(XY)  # shape (m, state_dim)\n",
    "    # return the derivative for the last state (y) as (m,)\n",
    "    return dydt[:, -1]\n",
    "\n",
    "# -------------------------\n",
    "# 4) PINN model and loss\n",
    "# -------------------------\n",
    "class PINN(nn.Module):\n",
    "    def __init__(self, input_dim=3, hidden=64):\n",
    "        super(PINN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class MAPELoss(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "    def forward(self, y_pred, y_true):\n",
    "        return torch.mean(torch.abs(y_pred - y_true) / (torch.abs(y_true) + self.eps))\n",
    "\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "# -------------------------\n",
    "# 5) Data tensors and derivative of features (numeric)\n",
    "# -------------------------\n",
    "# Convert to torch tensors\n",
    "X_tensor = torch.tensor(X_train_scaled, dtype=torch.float32, device=device)\n",
    "y_tensor = torch.tensor(y_train_scaled, dtype=torch.float32, device=device).view(-1,1)\n",
    "\n",
    "# dX/dt numeric (from x_dot). We will use only feature derivatives (first 3 columns)\n",
    "dXdt = torch.tensor(x_dot[:, :3], dtype=torch.float32, device=device)  # shape (N, 3)\n",
    "\n",
    "# Full-batch DataLoader (no shuffle)\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "loader = DataLoader(dataset, batch_size=N, shuffle=False)\n",
    "\n",
    "# instantiate model + optimizer\n",
    "model = PINN(input_dim=3, hidden=64).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss_fn = MAPELoss()\n",
    "\n",
    "# -------------------------\n",
    "# 6) Training loop (full-batch, SINDy differential physics loss)\n",
    "# -------------------------\n",
    "lambda_sindy = 1.0  # weight for SINDy physics loss (tune this)\n",
    "epochs = 3000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for xb, yb in loader:   # one batch containing the whole training set\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Ensure inputs require grad for Jacobian computation\n",
    "        xb = xb.clone().detach().requires_grad_(True)\n",
    "\n",
    "        # NN prediction (scaled y)\n",
    "        y_pred = model(xb)  # shape (N,1)\n",
    "\n",
    "        # Data loss (MAPE) on scaled values\n",
    "        loss_data = loss_fn(y_pred, yb)\n",
    "\n",
    "        # --- compute time derivative predicted by NN via chain rule ---\n",
    "        # For each sample i: dy_dt_nn[i] = grad_y_wrt_x_i dot dXdt[i]\n",
    "        Nbatch = xb.shape[0]\n",
    "        dy_dt_nn_list = []\n",
    "        # loop per sample to extract per-sample gradient (OK for N ~ 36)\n",
    "        for i in range(Nbatch):\n",
    "            # gradient of scalar y_pred[i] w.r.t the input matrix xb (returns gradient matrix)\n",
    "            grad_i = torch.autograd.grad(y_pred[i,0], xb, retain_graph=True, create_graph=True)[0][i]  # shape (3,)\n",
    "            # compute dot product with numeric dXdt[i]\n",
    "            dy_dt_nn_i = torch.dot(grad_i, dXdt[i])\n",
    "            dy_dt_nn_list.append(dy_dt_nn_i.unsqueeze(0))\n",
    "\n",
    "        dy_dt_nn = torch.cat(dy_dt_nn_list, dim=0).view(-1,1)  # shape (N,1)\n",
    "\n",
    "        # --- compute SINDy-predicted dy/dt (treat SINDy as fixed, use current NN y to get inputs) ---\n",
    "        # We need the current y in scaled space for SINDy inputs (SINDy was trained on scaled y)\n",
    "        y_pred_np = y_pred.detach().cpu().numpy().flatten()  # scaled y predicted\n",
    "        X_np = xb.detach().cpu().numpy()  # scaled features\n",
    "\n",
    "        dy_dt_sindy_np = sindy_predict_dy_dt(X_np, y_pred_np)  # shape (N,)\n",
    "        dy_dt_sindy = torch.tensor(dy_dt_sindy_np.reshape(-1,1), dtype=torch.float32, device=device)\n",
    "\n",
    "        # SINDy physics loss (MSE between NN time derivative and SINDy-predicted derivative)\n",
    "        loss_sindy = mse_loss(dy_dt_nn, dy_dt_sindy)\n",
    "\n",
    "        # Total loss and update\n",
    "        loss = loss_data + lambda_sindy * loss_sindy\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # print progress\n",
    "    if epoch % 250 == 0:\n",
    "        print(f\"Epoch {epoch:4d}  loss={loss.item():.6e}  data={loss_data.item():.6e}  sindy={loss_sindy.item():.6e}\")\n",
    "\n",
    "# -------------------------\n",
    "# 7) Predict on train/test and unscale\n",
    "# -------------------------\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pinn_train_scaled = model(torch.tensor(X_train_scaled, dtype=torch.float32, device=device)).cpu().numpy().flatten()\n",
    "    y_pinn_test_scaled  = model(torch.tensor(X_test_scaled, dtype=torch.float32, device=device)).cpu().numpy().flatten()\n",
    "\n",
    "y_pinn_train = scaler_y.inverse_transform(y_pinn_train_scaled.reshape(-1,1)).flatten()\n",
    "y_pinn_test  = scaler_y.inverse_transform(y_pinn_test_scaled.reshape(-1,1)).flatten()\n",
    "\n",
    "train['pinn_pred'] = y_pinn_train\n",
    "train['residual'] = train['y'] - train['pinn_pred']\n",
    "test['pinn_pred']  = y_pinn_test\n",
    "test['residual']   = test['y'] - test['pinn_pred']\n",
    "\n",
    "# -------------------------\n",
    "# 8) Residual modeling with Prophet (same as before)\n",
    "# -------------------------\n",
    "train_res = train[['ds','residual']].rename(columns={'residual':'y'})\n",
    "test_res  = test[['ds','residual']].rename(columns={'residual':'y'})\n",
    "\n",
    "m_res = Prophet(yearly_seasonality=True, weekly_seasonality=False, daily_seasonality=False)\n",
    "m_res.fit(train_res)\n",
    "\n",
    "future_res = m_res.make_future_dataframe(periods=len(test_res), freq='MS')\n",
    "forecast_res = m_res.predict(future_res)\n",
    "res_pred_prophet = forecast_res['yhat'].iloc[len(train_res):].values\n",
    "final_pred_prophet = test['pinn_pred'].values + res_pred_prophet\n",
    "\n",
    "# -------------------------\n",
    "# 9) Metrics\n",
    "# -------------------------\n",
    "def robust_mape(y_true, y_pred):\n",
    "    mask = y_true != 0\n",
    "    if mask.sum() == 0:\n",
    "        return np.nan\n",
    "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    return 100 * np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred)))\n",
    "\n",
    "def compute_metrics(y_true, y_pred, name):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mape_val = robust_mape(y_true, y_pred)\n",
    "    smape_val = smape(y_true, y_pred)\n",
    "    return pd.DataFrame({\n",
    "        'Model': [name],\n",
    "        'MAPE (%)': [mape_val],\n",
    "        'MAE': [mae],\n",
    "        'RMSE': [rmse],\n",
    "        'sMAPE (%)': [smape_val]\n",
    "    })\n",
    "\n",
    "y_true_test = test['y'].values\n",
    "df_metrics = pd.concat([\n",
    "    compute_metrics(y_true_test, y_pinn_test, 'SINDy-PINN'),\n",
    "    compute_metrics(y_true_test, final_pred_prophet, 'SINDy-PINN + Prophet')\n",
    "], ignore_index=True)\n",
    "\n",
    "print(df_metrics)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
